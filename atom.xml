<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>shiyangtao&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://shiyangtao.github.io/"/>
  <updated>2020-06-05T06:00:55.409Z</updated>
  <id>http://shiyangtao.github.io/</id>
  
  <author>
    <name>sytao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>JVM</title>
    <link href="http://shiyangtao.github.io/2020/06/05/JVM/"/>
    <id>http://shiyangtao.github.io/2020/06/05/JVM/</id>
    <published>2020-06-05T00:22:12.000Z</published>
    <updated>2020-06-05T06:00:55.409Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://lovestblog.cn/blog/2015/07/09/final-reference/" target="_blank" rel="noopener">JVM源码分析之FinalReference</a></p><p><a href="https://www.jianshu.com/p/6d37afd1f072" target="_blank" rel="noopener">PhantomReference导致CMS GC耗时严重</a></p><p><a href="https://www.jianshu.com/p/79d4a0516f11" target="_blank" rel="noopener">一次 Young GC 的优化实践（FinalReference 相关）</a></p><p><a href="https://www.jianshu.com/p/28eb3d0d263f" target="_blank" rel="noopener">强/软/弱/虚引用</a></p><p><a href="https://juejin.im/post/5d33be9d5188253a2e1b8fa6" target="_blank" rel="noopener">【译】深入理解G1的GC日志（一）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;http://lovestblog.cn/blog/2015/07/09/final-reference/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;JVM源码分析之FinalReference&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>java基础</title>
    <link href="http://shiyangtao.github.io/2020/05/08/java%E5%9F%BA%E7%A1%80/"/>
    <id>http://shiyangtao.github.io/2020/05/08/java基础/</id>
    <published>2020-05-08T00:39:08.000Z</published>
    <updated>2020-05-08T13:32:25.342Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.zhihu.com/question/49196023" target="_blank" rel="noopener">Java类初始化顺序</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.zhihu.com/question/49196023&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Java类初始化顺序&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>k8s面试题</title>
    <link href="http://shiyangtao.github.io/2020/03/31/k8s%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <id>http://shiyangtao.github.io/2020/03/31/k8s面试题/</id>
    <published>2020-03-31T14:13:19.000Z</published>
    <updated>2020-04-24T01:24:29.015Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes面试题"><a href="#Kubernetes面试题" class="headerlink" title="Kubernetes面试题"></a>Kubernetes面试题</h1><h2 id="理论篇"><a href="#理论篇" class="headerlink" title="理论篇"></a>理论篇</h2><h3 id="简要说下Kubernetes有哪些核心组件以及这些组件负责什么工作？"><a href="#简要说下Kubernetes有哪些核心组件以及这些组件负责什么工作？" class="headerlink" title="简要说下Kubernetes有哪些核心组件以及这些组件负责什么工作？"></a>简要说下Kubernetes有哪些核心组件以及这些组件负责什么工作？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Master节点</span><br><span class="line">etcd：提供数据库服务保存了整个集群的状态</span><br><span class="line">kube-apiserver：提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制,这将公开Kubernetes主节点组件的所有API，并负责在Kubernetes节点和Kubernetes主组件之间建立通信。</span><br><span class="line">kube-controller-manager(控制循环与状态协调机制)：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等。</span><br><span class="line">kub-scheduler：负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上</span><br><span class="line">Node节点</span><br><span class="line">kubelet：负责维护容器的生命周期，同时也负责Volume和网络的管理</span><br><span class="line">kube-proxy：负责为Service提供内部的服务发现和负载均衡，并维护网络规则</span><br><span class="line">container-runtime：是负责管理运行容器的软件，比如docker</span><br></pre></td></tr></table></figure><h3 id="Pod的生命周期"><a href="#Pod的生命周期" class="headerlink" title="Pod的生命周期"></a>Pod的生命周期</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Pending 部署Pod事务已被集群受理，但当前容器镜像还未下载完。</span><br><span class="line">Running 所有容器已被创建，并被部署到k8s节点。</span><br><span class="line">Successed Pod成功退出，并不会被重启。</span><br><span class="line">Failed Pod中有容器被终止。</span><br><span class="line">Unknown 未知原因，如kube-apiserver无法与Pod进行通讯。</span><br><span class="line">详</span><br></pre></td></tr></table></figure><h3 id="k8s的pod内容器之间的关系"><a href="#k8s的pod内容器之间的关系" class="headerlink" title="k8s的pod内容器之间的关系"></a>k8s的pod内容器之间的关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Pod里面的容器共享网络，因此可使用localhost通讯。由于也共享存储，所以可以使用IPC和共享内存进行通讯。</span><br></pre></td></tr></table></figure><h3 id="详述kube-proxy原理"><a href="#详述kube-proxy原理" class="headerlink" title="详述kube-proxy原理"></a>详述kube-proxy原理</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">问题：详述kube-proxy原理，一个请求是如何经过层层转发落到某个pod上的整个过程。请求可能来自pod也可能来自外部。</span><br><span class="line">kube-proxy部署在每个Node节点上，通过监听集群状态变更，并对本机iptables做修改，从而实现网络路由。 </span><br><span class="line">负载均衡的两种实现：1. iptables 2. ipvs + iptables</span><br></pre></td></tr></table></figure><h3 id="问题：deployment-rs有什么区别。其使用方式、使用条件和原理是什么。"><a href="#问题：deployment-rs有什么区别。其使用方式、使用条件和原理是什么。" class="headerlink" title="问题：deployment/rs有什么区别。其使用方式、使用条件和原理是什么。"></a>问题：deployment/rs有什么区别。其使用方式、使用条件和原理是什么。</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deployment是rs的超集，提供更多的部署功能，如：回滚、暂停和重启、 版本记录、事件和状态查看、滚动升级和替换升级。</span><br></pre></td></tr></table></figure><h2 id="命令篇"><a href="#命令篇" class="headerlink" title="命令篇"></a>命令篇</h2><p>查看ops这个命名空间下的所有pod，并显示pod的IP地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -n ops -o wide</span><br></pre></td></tr></table></figure><p>查看tech命名空间下的pod名称为tech-12ddde-fdfde的日志，并显示最后30行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs tech-12ddde-fdfde -n tech|tail -n 30</span><br></pre></td></tr></table></figure><p>怎么查看test的命名空间下pod名称为test-5f7f56bfb7-dw9pw的状态</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe pod test-5f7f56bfb7-dw9pw -n test</span><br></pre></td></tr></table></figure><p>如何查看test命名空间下的所有endpoints</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get ep -n test</span><br></pre></td></tr></table></figure><p>如何查看test命名空间下的所有ingress</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeclt get ingress -n test</span><br></pre></td></tr></table></figure><p>如何删除test命名空间下某个deploymemt，名称为gitlab</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deploy gitlab -n test</span><br></pre></td></tr></table></figure><p>如何缩减test命名空间下deployment名称为gitlab的副本数为1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeclt scale deployment gitlab -n test --replicas=1</span><br></pre></td></tr></table></figure><p>如何在不进入pod内查看命名空间为test,pod名称为test-5f7f56bfb7-dw9pw的hosts</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it test-5f7f56bfb7-dw9pw -n test -- cat /etc/hosts</span><br></pre></td></tr></table></figure><p>如何设置节点test-node-10为不可调度以及如何取消不可调度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes test-node-10 foo=bar:NoSchedule</span><br><span class="line">kubectl taint nodes test-node-10 foo=bar:NoSchedule-</span><br></pre></td></tr></table></figure><p>or</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl cordon test-node-10</span><br><span class="line">kubectl uncordon  test-node-10</span><br></pre></td></tr></table></figure><p><a href="https://stackoverflow.com/questions/60880853/difference-between-cordon-and-taints-and-toleration-taint-effect-noexecute" target="_blank" rel="noopener">taint vs cordon</a></p><h2 id="考察实际生产经验-最重要"><a href="#考察实际生产经验-最重要" class="headerlink" title="考察实际生产经验(最重要)"></a>考察实际生产经验(最重要)</h2><h4 id="Downward-API"><a href="#Downward-API" class="headerlink" title="Downward API"></a>Downward API</h4><p>某个pod启动的时候需要用到pod的名称，请问怎么获取pod的名称，简要写出对应的yaml配置(考察是否对k8s的Downward API有所了解)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">env:</span><br><span class="line">- name: test</span><br><span class="line">  valueFrom:</span><br><span class="line">    fieldRef:</span><br><span class="line">      fieldPath: metadata.name</span><br></pre></td></tr></table></figure><h4 id="hosts"><a href="#hosts" class="headerlink" title="hosts"></a>hosts</h4><p>某个pod需要配置某个内网服务的hosts，比如数据库的host，刑如：192.168.4.124 db.test.com，请问有什么方法可以解决，简要写出对应的yaml配置或其他解决办法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hostAliases:</span><br><span class="line">- ip: &quot;192.168.4.124&quot;</span><br><span class="line">  hostnames:</span><br><span class="line">  - &quot;db.test.com</span><br></pre></td></tr></table></figure><h4 id="写dockerfile"><a href="#写dockerfile" class="headerlink" title="写dockerfile"></a>写dockerfile</h4><p>请用系统镜像为centos:latest制作一个jdk版本为1.8.142的基础镜像，请写出dockerfile（考察dockerfile的编写能力）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM centos:latest</span><br><span class="line">ARG JDK_HOME=/root/jdk1.8.0_142</span><br><span class="line">WORKDIR /root</span><br><span class="line">ADD jdk-8u142-linux-x64.tar.gz /root</span><br><span class="line">ENV JAVA_HOME=/root/jdk1.8.0_142</span><br><span class="line">ENV PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">CMD [&quot;bash&quot;]</span><br></pre></td></tr></table></figure><h4 id="podAntiAffinity"><a href="#podAntiAffinity" class="headerlink" title="podAntiAffinity"></a>podAntiAffinity</h4><p>假如某个pod有多个副本，如何让两个pod分布在不同的node节点上，请简要写出对应的yaml文件（考察是否对pod的亲和性有所了解）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">affinity:</span><br><span class="line">  podAntiAffinity:</span><br><span class="line">    requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">    - topologyKey: &quot;kubernetes.io/hostname&quot;</span><br><span class="line">      labelSelector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: test</span><br></pre></td></tr></table></figure><h4 id="日志收集"><a href="#日志收集" class="headerlink" title="日志收集"></a>日志收集</h4><p>pod的日志如何收集，简要写出方案(考察是否真正有生产经验，日志收集是必须解决的一个难题)</p><p><a href="https://jimmysong.io/kubernetes-handbook/practice/app-log-collection.html" target="_blank" rel="noopener">https://jimmysong.io/kubernetes-handbook/practice/app-log-collection.html</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logstash + elk</span><br></pre></td></tr></table></figure><p><a href="https://blog.51cto.com/14154700/2452179" target="_blank" rel="noopener">https://blog.51cto.com/14154700/2452179</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">classDiagram</span><br><span class="line">class Abstraction &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">class RefinedAbstraction&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Implementation&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class ConcreteImplementation&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Implementation &lt;|.. ConcreteImplementation</span><br><span class="line"></span><br><span class="line">Implementation &lt;-- Abstraction</span><br><span class="line"></span><br><span class="line">Abstraction &lt;|-- RefinedAbstraction</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Kubernetes面试题&quot;&gt;&lt;a href=&quot;#Kubernetes面试题&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes面试题&quot;&gt;&lt;/a&gt;Kubernetes面试题&lt;/h1&gt;&lt;h2 id=&quot;理论篇&quot;&gt;&lt;a href=&quot;#理论篇&quot; c
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>遇到过的问题</title>
    <link href="http://shiyangtao.github.io/2020/03/28/%E9%81%87%E5%88%B0%E8%BF%87%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://shiyangtao.github.io/2020/03/28/遇到过的问题/</id>
    <published>2020-03-28T04:19:09.000Z</published>
    <updated>2020-05-19T06:05:23.265Z</updated>
    
    <content type="html"><![CDATA[<h1 id="工作中遇到的问题"><a href="#工作中遇到的问题" class="headerlink" title="工作中遇到的问题"></a>工作中遇到的问题</h1><h2 id="open-feign"><a href="#open-feign" class="headerlink" title="open feign"></a>open feign</h2><h3 id="not-a-type-supported-by-this-encoder"><a href="#not-a-type-supported-by-this-encoder" class="headerlink" title="not a type supported by this encoder"></a>not a type supported by this encoder</h3><p><a href="https://blog.csdn.net/F_QWERDF/article/details/89713165" target="_blank" rel="noopener">https://blog.csdn.net/F_QWERDF/article/details/89713165</a></p><p><a href="https://blog.csdn.net/gududedabai/article/details/79895893" target="_blank" rel="noopener">https://blog.csdn.net/gududedabai/article/details/79895893</a></p><p><a href="https://github.com/spring-cloud/spring-cloud-netflix/issues/2746" target="_blank" rel="noopener">https://github.com/spring-cloud/spring-cloud-netflix/issues/2746</a></p><p>依赖的项目中有定义全局的Encoder</p><p><img src="/2020/03/28/遇到过的问题/images/image-20200328122126880.png" alt="image-20200328122126880"></p><h2 id="Spring-boot"><a href="#Spring-boot" class="headerlink" title="Spring boot"></a>Spring boot</h2><h3 id="spring外部配置优先级"><a href="#spring外部配置优先级" class="headerlink" title="spring外部配置优先级"></a>spring外部配置优先级</h3><p><a href="https://docs.spring.io/spring-boot/docs/1.5.6.BUILD-SNAPSHOT/reference/html/boot-features-external-config.html" target="_blank" rel="noopener">https://docs.spring.io/spring-boot/docs/1.5.6.BUILD-SNAPSHOT/reference/html/boot-features-external-config.html</a></p><p><img src="/2020/03/28/遇到过的问题/images/image-20200402193603315.png" alt="image-20200402193603315"></p><h2 id="Spring"><a href="#Spring" class="headerlink" title="Spring"></a>Spring</h2><h3 id="Profile踩坑"><a href="#Profile踩坑" class="headerlink" title="@Profile踩坑"></a>@Profile踩坑</h3><p><a href="https://www.jianshu.com/p/75de79fba705" target="_blank" rel="noopener">https://www.jianshu.com/p/75de79fba705</a></p><p>错误用法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProfileDatabaseConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="meta">@Profile</span>(<span class="string">"development"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSource <span class="title">database</span><span class="params">()</span> </span>&#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="meta">@Profile</span>(<span class="string">"production"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSource <span class="title">database</span><span class="params">()</span> </span>&#123; ... &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>正确用法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProfileDatabaseConfig</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(<span class="string">"dataSource"</span>)</span><br><span class="line">    <span class="meta">@Profile</span>(<span class="string">"development"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSource <span class="title">embeddedDatabase</span><span class="params">()</span> </span>&#123; ... &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Bean</span>(<span class="string">"dataSource"</span>)</span><br><span class="line">    <span class="meta">@Profile</span>(<span class="string">"production"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> DataSource <span class="title">productionDatabase</span><span class="params">()</span> </span>&#123; ... &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于相同Java方法名称的重载@Bean方法(类似于构造函数重载), 需要在所有重载方法上一致地声明@Profile条件, 如果条件不一致, 则只有重载方法中第一个声明上的条件才生效。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;工作中遇到的问题&quot;&gt;&lt;a href=&quot;#工作中遇到的问题&quot; class=&quot;headerlink&quot; title=&quot;工作中遇到的问题&quot;&gt;&lt;/a&gt;工作中遇到的问题&lt;/h1&gt;&lt;h2 id=&quot;open-feign&quot;&gt;&lt;a href=&quot;#open-feign&quot; class=&quot;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>容器网络</title>
    <link href="http://shiyangtao.github.io/2020/03/02/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/"/>
    <id>http://shiyangtao.github.io/2020/03/02/容器网络/</id>
    <published>2020-03-02T11:24:45.000Z</published>
    <updated>2020-03-03T14:56:34.148Z</updated>
    
    <content type="html"><![CDATA[<h2 id="docker网络原理"><a href="#docker网络原理" class="headerlink" title="docker网络原理"></a>docker网络原理</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥(docker0)的方式，实现了跟同其他容器的数据交换。</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200302192600220.png" alt="image-20200302192600220" style="zoom:50%;"></p><p>这样在同一个宿主机上的容器之间可以通信，并且容器可以访问外网</p><h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><p>在 Docker 的默认配置下，不同宿主机上的容器通过 IP 地址进行互相访问是根本做不到的。</p><p>在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了。</p><h2 id="跨主机网络"><a href="#跨主机网络" class="headerlink" title="跨主机网络"></a>跨主机网络</h2><h3 id="Flannel"><a href="#Flannel" class="headerlink" title="Flannel"></a>Flannel</h3><h4 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h4><h5 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h5><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200302194933200.png" alt="image-20200302194933200"></p><p><strong>数据包是怎么到了flannel0呢？</strong></p><p>由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥，从而出现在宿主机上。而出现在宿主机上。这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 在Node 1上</span><br><span class="line">$ ip route</span><br><span class="line">default via 10.168.0.1 dev eth0</span><br><span class="line">100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.1.0</span><br><span class="line">100.96.1.0/24 dev docker0  proto kernel  scope link  src 100.96.1.1</span><br><span class="line">10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.2</span><br></pre></td></tr></table></figure><p><strong>如何找到对应node的ip？</strong></p><p>flanneld 进程根据容器IP地址找到对应node的ip，如下所示：</p><p>这里，就用到了 Flannel 项目里一个非常重要的概念：子网（Subnet）。</p><p>事实上，在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ etcdctl ls /coreos.com/network/subnets</span><br><span class="line">/coreos.com/network/subnets/100.96.1.0-24</span><br><span class="line">/coreos.com/network/subnets/100.96.2.0-24</span><br><span class="line">/coreos.com/network/subnets/100.96.3.0-24</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24</span><br><span class="line">&#123;&quot;PublicIP&quot;:&quot;10.168.0.3&quot;&#125;</span><br></pre></td></tr></table></figure><p><strong>node2 flannel0是怎么到的docker0呢</strong></p><p>而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 在Node 2上</span><br><span class="line">$ ip route</span><br><span class="line">default via 10.168.0.1 dev eth0</span><br><span class="line">100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.2.0</span><br><span class="line">100.96.2.0/24 dev docker0  proto kernel  scope link  src 100.96.2.1</span><br><span class="line">10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.3</span><br></pre></td></tr></table></figure><p>由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、<strong>也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。</strong>所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。</p><p><strong>所以需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ FLANNEL_SUBNET=100.96.1.1/24</span><br><span class="line">$ dockerd --bip=$FLANNEL_SUBNET ...</span><br></pre></td></tr></table></figure><h5 id="不足-1"><a href="#不足-1" class="headerlink" title="不足"></a>不足</h5><p>UDP 模式有严重的性能问题，实际上，相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN （三层设备）设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示：</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200302195019977.png" alt="image-20200302195019977" style="zoom:50%;"></p><h4 id="VXLAN"><a href="#VXLAN" class="headerlink" title="VXLAN"></a>VXLAN</h4><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200302200409775.png" alt="image-20200302200409775" style="zoom:50%;"></p><p><strong>数据包是怎么到了flannel.1呢？</strong></p><p>和上边UDP的方式是一样的</p><p>当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ route -n</span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">...</span><br><span class="line">10.1.16.0       10.1.16.0       255.255.255.0   UG    0      0        0 flannel.1</span><br></pre></td></tr></table></figure><p>这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。</p><p><strong>“目的 VTEP 设备”的 MAC 地址是什么？</strong></p><p>此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 在Node 1上</span><br><span class="line"><span class="meta">$</span> ip neigh show dev flannel.1</span><br><span class="line">10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT</span><br></pre></td></tr></table></figure><p>Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200302200535172.png" alt="image-20200302200535172" style="zoom:50%;"></p><p><strong>如何找到对应node的ip？</strong></p><p>在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询</span><br><span class="line">$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37</span><br><span class="line">5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent</span><br></pre></td></tr></table></figure><p><strong>VNI作用</strong></p><p>而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备（二层设备）识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200302201319949.png" alt="image-20200302201319949" style="zoom:50%;"></p><h4 id="host-gw"><a href="#host-gw" class="headerlink" title="host-gw"></a>host-gw</h4><p>参考下文</p><h3 id="k8s-flannel-vxlan的实现"><a href="#k8s-flannel-vxlan的实现" class="headerlink" title="k8s flannel vxlan的实现"></a>k8s flannel vxlan的实现</h3><p>以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已，如下所示：</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200303192651146.png" alt="image-20200303192651146"></p><h3 id="k8s-flannel-host-gw实现"><a href="#k8s-flannel-host-gw实现" class="headerlink" title="k8s flannel host-gw实现"></a>k8s flannel host-gw实现</h3><p>当你设置 Flannel 使用 host-gw 模式之后，flanneld 会在宿主机上创建这样一条规则，以 Node 1 为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ip route</span><br><span class="line">...</span><br><span class="line">10.244.1.0/24 via 10.168.0.3 dev eth0</span><br></pre></td></tr></table></figure><p>这条路由规则的含义是：目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是 10.168.0.3（即：via 10.168.0.3）。所谓下一跳地址就是：如果 IP 包从主机 A 发到主机 B，需要经过路由设备 X 的中转。那么 X 的 IP 地址就应该配置为主机 A 的下一跳地址。</p><p>可以看到，host-gw 模式的工作原理，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。</p><p>也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。</p><p>所以说，Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。</p><p>而在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200303221958976.png" alt="image-20200303221958976"></p><h3 id="k8s-Calico"><a href="#k8s-Calico" class="headerlink" title="k8s Calico"></a>k8s Calico</h3><h4 id="二层互通的情况"><a href="#二层互通的情况" class="headerlink" title="二层互通的情况"></a>二层互通的情况</h4><p>实际上，Calico 项目提供的网络解决方案，与 Flannel 的 host-gw 模式，几乎是完全一样的。</p><p>不过，不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了一个“重型武器”BGP来自动地在整个集群中分发路由信息。</p><p>除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。</p><p>此外，由于 Calico 没有使用 CNI 的网桥模式，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。比如，宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">10.233.2.3 dev cali5863f3 scope link</span><br></pre></td></tr></table></figure><p>即：发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200303222150869.png" alt="image-20200303222150869"></p><h4 id="三层互通（IPIP模式）"><a href="#三层互通（IPIP模式）" class="headerlink" title="三层互通（IPIP模式）"></a>三层互通（IPIP模式）</h4><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200303224919984.png" alt="image-20200303224919984"></p><p>在 Calico 的 IPIP 模式下，Felix 进程在 Node 1 上添加的路由规则，会稍微不同，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10.233.2.0/24 via 192.168.2.2 tunl0</span><br></pre></td></tr></table></figure><p>可以看到，尽管这条规则的下一跳地址仍然是 Node 2 的 IP 地址，但这一次，要负责将 IP 包发出去的设备，变成了 tunl0。注意，是 T-U-N-L-0，而不是 Flannel UDP 模式使用的 T-U-N-0（tun0），这两种设备的功能是完全不一样的。</p><p>由于宿主机之间已经使用路由器配置了三层转发，也就是设置了宿主机之间的“下一跳”。所以这个 IP 包在离开 Node 1 之后，就可以经过路由器，最终“跳”到 Node 2 上。</p><p>Calico 使用的这个 tunl0 设备，是一个 IP 隧道（IP tunnel）设备。在上面的例子中，IP 包进入 IP 隧道设备之后，就会被 Linux 内核的 IPIP 驱动接管。IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中，如下所示：</p><p><img src="/2020/03/02/容器网络/Users/mobike/syt/study/blog-hexo-shiyangtao/source/_posts/容器网络/images/image-20200303225100570.png" alt="image-20200303225100570" style="zoom:50%;"></p><p>不难看到，当 Calico 使用 IPIP 模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。所以，在实际使用时，如非硬性需求，我建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我以网桥类型的 Flannel 插件为例，为你讲解了 Kubernetes 里容器网络和 CNI 插件的主要工作原理。不过，除了这种模式之外，还有一种纯三层（Pure Layer 3）网络方案非常值得你注意。其中的典型例子，莫过于 Flannel 的 host-gw 模式和 Calico 项目了。</p><p>三层（host-gw和calico的二层互通）和隧道模式(IPIP)的异同<br>三层的优点：少了封包和解包的过程，性能肯定是更高的。<br>三层的缺点：需要自己想办法维护路由规则。<br>隧道的优点：简单，原因是大部分工作都是由 Linux 内核的模块实现了，应用层面工作量较少。<br>隧道的缺点：主要的问题就是性能低。</p><p>需要注意的是，在大规模集群里，三层网络方案在宿主机上的路由规则可能会非常多，这会导致错误排查变得困难。此外，在系统故障的时候，路由规则出现重叠冲突的概率也会变大。基于上述原因，如果是在公有云上，由于宿主机网络本身比较“直白”，我一般会推荐更加简单的 Flannel host-gw 模式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;docker网络原理&quot;&gt;&lt;a href=&quot;#docker网络原理&quot; class=&quot;headerlink&quot; title=&quot;docker网络原理&quot;&gt;&lt;/a&gt;docker网络原理&lt;/h2&gt;&lt;h3 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerli
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>学习方法</title>
    <link href="http://shiyangtao.github.io/2020/03/02/%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <id>http://shiyangtao.github.io/2020/03/02/学习方法/</id>
    <published>2020-03-02T03:41:05.000Z</published>
    <updated>2020-03-02T07:01:38.330Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章内容总结自《认知天性》这本书。</p><p>比如读一本书，看完以后就忘了，要怎样才能够记住？</p><h2 id="错误的方式"><a href="#错误的方式" class="headerlink" title="错误的方式"></a>错误的方式</h2><h3 id="重复阅读，划重点"><a href="#重复阅读，划重点" class="headerlink" title="重复阅读，划重点"></a>重复阅读，划重点</h3><p>80%的人都是采取划重点、重复阅读直到记住为止。心理学家告诉你这是白费力气，因为要想运用知识，就必须记忆，而重复阅读并不能带出长久记忆或者说效率很低。当你重复阅读的时候，会产生自己已经记住的错觉（而实际你还差的远）所以读的时候不知道哪里是自己比较弱的部分，因为重复接触并不能强化记忆</p><h3 id="刻意练习"><a href="#刻意练习" class="headerlink" title="刻意练习"></a>刻意练习</h3><p>在一段时间内反复练习同一件事，这种频繁而集中的训练只会产生短期记忆。对于产生长期记忆或者掌握某项技能来说，毫无助益。</p><h2 id="正确的方式"><a href="#正确的方式" class="headerlink" title="正确的方式"></a>正确的方式</h2><p>用纸笔重新回想下书中的内容，写出大致的架构，然后从架构中，抓取自己想要学习的重点或者是自己不清楚的知识，然后用自己的话描述成人人都懂的故事内容。</p><h3 id="检索"><a href="#检索" class="headerlink" title="检索"></a>检索</h3><p>检索就是主动回忆某件事的过程。</p><p>考试就是检索的一种，回忆的时候比较痛苦这时候学习效果反而会加倍，通过一个个的考试，就会不断的回忆做自我检测，就能提高记忆的效果。</p><p>反思也是检索的一种，原理同考试一样。</p><p>检索的过程虽然痛苦，但是在学习上越轻松，效果越不好。</p><p>那些看起来非常勤奋，不停的背书，不停的做笔记的人，只是看起来非常努力，耗费了大量时间。</p><h3 id="间隔练习"><a href="#间隔练习" class="headerlink" title="间隔练习"></a>间隔练习</h3><p>间隔练习就是每隔一段时间，来练习下之前学过的东西。</p><p>间隔练习使知识存储更牢固，可以巩固知识的记忆</p><h3 id="穿插练习"><a href="#穿插练习" class="headerlink" title="穿插练习"></a>穿插练习</h3><p>在练习中穿插两个以上的主题和技能也是一种能胜过集中练习的方法</p><p>如同费曼的理论</p><p>1.选择并教授 2. 发现不能理解的地方 3.重新学习 4.简化</p><h3 id="时间管理"><a href="#时间管理" class="headerlink" title="时间管理"></a>时间管理</h3><p>GTD 时间管理之术</p><p>1.收集</p><p>2.整理 2min能做完的事项、等待事项、特定日程、单步骤事项、多步骤事项</p><p>3.just do it</p><h2 id="自己提炼出的学习方法"><a href="#自己提炼出的学习方法" class="headerlink" title="自己提炼出的学习方法"></a>自己提炼出的学习方法</h2><h3 id="学习一门技术"><a href="#学习一门技术" class="headerlink" title="学习一门技术"></a>学习一门技术</h3><ol><li>先把这个领域最有代表性的并且适合自己这个阶段去看的书籍或者博客学一遍(不要贪多，一两本足以)，<strong>一定要细心阅读，做好笔记，最最最重要的是实践。</strong></li><li>根据记忆或者书籍目录回忆书中的知识点，形成脑图，发现自己比较薄弱的部分</li><li>巩固比较薄弱的地方</li><li>“考试”，找到相关的面试题来检索。偏编程实践要coding验证</li><li>把测试的面试题和答案整理成册，方便自己间隔练习，如果遇到比较薄弱的地方，回到第三步</li><li>间隔练习，看整理的笔记、“考试”</li></ol><h3 id="看一本方法论类的书"><a href="#看一本方法论类的书" class="headerlink" title="看一本方法论类的书"></a>看一本方法论类的书</h3><p>比如《自控力》《时间管理》这一类的</p><ol><li>看完后，找出纸笔回忆出书籍的脉络，或者看着目录回忆。</li><li>整理出读书笔记。</li><li>隔一段时间去翻一下读书笔记。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇文章内容总结自《认知天性》这本书。&lt;/p&gt;
&lt;p&gt;比如读一本书，看完以后就忘了，要怎样才能够记住？&lt;/p&gt;
&lt;h2 id=&quot;错误的方式&quot;&gt;&lt;a href=&quot;#错误的方式&quot; class=&quot;headerlink&quot; title=&quot;错误的方式&quot;&gt;&lt;/a&gt;错误的方式&lt;/h2&gt;&lt;h
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>java程序员的能力要求</title>
    <link href="http://shiyangtao.github.io/2020/02/29/java%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%83%BD%E5%8A%9B%E8%A6%81%E6%B1%82/"/>
    <id>http://shiyangtao.github.io/2020/02/29/java程序员的能力要求/</id>
    <published>2020-02-29T05:35:16.000Z</published>
    <updated>2020-02-29T06:48:22.888Z</updated>
    
    <content type="html"><![CDATA[<p>从boss直聘里翻看了一些热门工作岗位的职位要求</p><h3 id="Java基础"><a href="#Java基础" class="headerlink" title="Java基础"></a>Java基础</h3><ul><li>多线程，并发编程能力</li><li>集合</li><li>JVM</li><li>IO&amp;网络编程</li></ul><h3 id="编程能力"><a href="#编程能力" class="headerlink" title="编程能力"></a>编程能力</h3><ul><li>良好的设计和抽象能力</li><li>设计模式</li></ul><h3 id="掌握主流开发框架"><a href="#掌握主流开发框架" class="headerlink" title="掌握主流开发框架"></a>掌握主流开发框架</h3><ul><li><p>Spring 全家桶</p><ul><li>Spring MVC</li><li>Spring Boot</li><li>Spring Cloud</li></ul></li><li><p>mybatis</p></li></ul><h3 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h3><ul><li>关系型 以Mysql代表</li><li>NoSql<br><strong>按照出现次数排序</strong>，斜体的出现次数比较少<ul><li>K-V型：解决关系型数据库无法存储数据结构的问题，以Redis为代表。</li><li><em>文档数据库：解决关系型数据库强schema约束的问题，以MongoDB为代表。</em></li><li><em>列式数据库：解决关系数据库大数据场景下的I/O问题，以HBase为代表。</em></li><li><em>全文搜索引擎：解决关系数据库的全文搜索性能问题，以Elasticsearch为代表。</em></li></ul></li></ul><h3 id="熟悉分布式系统的设计和应用"><a href="#熟悉分布式系统的设计和应用" class="headerlink" title="熟悉分布式系统的设计和应用"></a>熟悉分布式系统的设计和应用</h3><ul><li>高可用&amp;高性能&amp;可扩展设计</li><li><p>RPC调用 (Dobbo\Grpc\Thrift\OpenFeign)</p></li><li><p>缓存设计</p></li><li>消息中间件</li><li>分布式事务</li></ul><h3 id="加分项"><a href="#加分项" class="headerlink" title="加分项"></a>加分项</h3><p>在大部分公司是加分项，顺序按照出现的次数</p><ul><li><p>数据结构&amp;算法 基础能力</p></li><li><p>Linux</p></li><li>网络</li><li>性能调优</li><li>其他语言 go、python</li><li>容器技术</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;从boss直聘里翻看了一些热门工作岗位的职位要求&lt;/p&gt;
&lt;h3 id=&quot;Java基础&quot;&gt;&lt;a href=&quot;#Java基础&quot; class=&quot;headerlink&quot; title=&quot;Java基础&quot;&gt;&lt;/a&gt;Java基础&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;多线程，并发编程能力&lt;/li&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>k8s_command</title>
    <link href="http://shiyangtao.github.io/2020/02/21/k8s-command/"/>
    <id>http://shiyangtao.github.io/2020/02/21/k8s-command/</id>
    <published>2020-02-21T03:44:32.000Z</published>
    <updated>2020-04-10T13:37:19.895Z</updated>
    
    <content type="html"><![CDATA[<h2 id="minikube-搭建"><a href="#minikube-搭建" class="headerlink" title="minikube 搭建"></a>minikube 搭建</h2><p><a href="https://kubernetes.io/zh/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tasks/tools/install-minikube/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo minikube start --vm-driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --image-repository=&apos;registry.cn-hangzhou.aliyuncs.com/google_containers&apos;</span><br></pre></td></tr></table></figure><h3 id="外网访问dashboard"><a href="#外网访问dashboard" class="headerlink" title="外网访问dashboard"></a>外网访问dashboard</h3><p><a href="https://blog.haohtml.com/archives/19201" target="_blank" rel="noopener">https://blog.haohtml.com/archives/19201</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup kubectl proxy --address=&apos;0.0.0.0&apos; --port=8001 --accept-hosts=&apos;^*$&apos; &amp;</span><br></pre></td></tr></table></figure><p>外网访问dashboard</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://&lt;ip&gt;:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://47.92.136.48:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure><p>headless</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it busybox -- nslookup busybox-1.default-subdomain.default.svc.cluster.local</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it busybox -- nslookup default-subdomain.default.svc.cluster.local</span><br></pre></td></tr></table></figure><p>外网访问内部servcie服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://&lt;ip&gt;:8001/api/v1/namespaces/default/services/SERVICE-NAME:PORT-NAME/proxy/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://47.92.136.48:8001/api/v1/namespaces/default/services/hostnames:default/proxy/</span><br><span class="line">http://47.92.136.48:8001/api/v1/namespaces/default/services/coffee-svc:http/proxy/</span><br><span class="line">http://47.92.136.48:8001/api/v1/namespaces/default/services/tea-svc:http/proxy/</span><br></pre></td></tr></table></figure><p>ingress</p><h2 id="搭建kubeadm"><a href="#搭建kubeadm" class="headerlink" title="搭建kubeadm"></a>搭建kubeadm</h2><h3 id="安装-kubeadm-和-Docker"><a href="#安装-kubeadm-和-Docker" class="headerlink" title="安装 kubeadm 和 Docker"></a>安装 kubeadm 和 Docker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb http://apt.kubernetes.io/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install -y docker.io kubeadm</span><br><span class="line"></span><br><span class="line"># 安装指定版本</span><br><span class="line">sudo apt-get install kubeadm=1.14.3-00 kubectl=1.14.3-00 kubelet=1.14.3-00</span><br></pre></td></tr></table></figure><h3 id="部署-Kubernetes-的-Master-节点"><a href="#部署-Kubernetes-的-Master-节点" class="headerlink" title="部署 Kubernetes 的 Master 节点"></a>部署 Kubernetes 的 Master 节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat kubeadm.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">controllerManager:</span><br><span class="line">    extraArgs:</span><br><span class="line">        horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;</span><br><span class="line">        horizontal-pod-autoscaler-sync-period: &quot;10s&quot;</span><br><span class="line">        node-monitor-grace-period: &quot;10s&quot;</span><br><span class="line">apiServer:</span><br><span class="line">    extraArgs:</span><br><span class="line">        runtime-config: &quot;api/all=true&quot;</span><br><span class="line">kubernetesVersion: &quot;stable-1.14&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm  init --config kubeadm.yaml --ignore-preflight-errors=NumCPU</span><br></pre></td></tr></table></figure><p>就可以完成 Kubernetes Master 的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令：这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 172.26.87.130:6443 --token 29kgba.dytvremded8eicnv \</span><br><span class="line">   --discovery-token-ca-cert-hash sha256:83a982aaefeb3935dcbcce7da005de080a7bd43086ecf26503605bd71b01f6cc</span><br></pre></td></tr></table></figure><h3 id="配置kubectl-授权信息"><a href="#配置kubectl-授权信息" class="headerlink" title="配置kubectl 授权信息"></a>配置kubectl 授权信息</h3><p>而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line"></span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    1d        v1.11.1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node master</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">Conditions:</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Ready   False ... KubeletNotReady  runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods -n kube-system</span><br><span class="line"></span><br><span class="line">NAME               READY   STATUS   RESTARTS  AGE</span><br><span class="line">coredns-78fcdf6894-j9s52     0/1    Pending  0     1h</span><br><span class="line">coredns-78fcdf6894-jm4wf     0/1    Pending  0     1h</span><br><span class="line">etcd-master           1/1    Running  0     2s</span><br><span class="line">kube-apiserver-master      1/1    Running  0     1s</span><br><span class="line">kube-controller-manager-master  0/1    Pending  0     1s</span><br><span class="line">kube-proxy-xbd47         1/1    NodeLost  0     1h</span><br><span class="line">kube-scheduler-master      1/1    Running  0     1s</span><br></pre></td></tr></table></figure><p>可以看到，CoreDNS、kube-controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。</p><h3 id="部署网络插件"><a href="#部署网络插件" class="headerlink" title="部署网络插件"></a>部署网络插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods -n kube-system</span><br><span class="line"></span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE</span><br><span class="line">coredns-78fcdf6894-j9s52         1/1       Running   0          1d</span><br><span class="line">coredns-78fcdf6894-jm4wf         1/1       Running   0          1d</span><br><span class="line">etcd-master                      1/1       Running   0          9s</span><br><span class="line">kube-apiserver-master            1/1       Running   0          9s</span><br><span class="line">kube-controller-manager-master   1/1       Running   0          9s</span><br><span class="line">kube-proxy-xbd47                 1/1       Running   0          1d</span><br><span class="line">kube-scheduler-master            1/1       Running   0          9s</span><br><span class="line">weave-net-cmk27                  2/2       Running   0          19s</span><br></pre></td></tr></table></figure><h3 id="部署-Kubernetes-的-Worker-节点"><a href="#部署-Kubernetes-的-Worker-节点" class="headerlink" title="部署 Kubernetes 的 Worker 节点"></a>部署 Kubernetes 的 Worker 节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 172.26.87.130:6443 --token 29kgba.dytvremded8eicnv \</span><br><span class="line">   --discovery-token-ca-cert-hash sha256:83a982aaefeb3935dcbcce7da005de080a7bd43086ecf26503605bd71b01f6cc</span><br></pre></td></tr></table></figure><h3 id="通过-Taint-Toleration-调整-Master-执行-Pod-的策略"><a href="#通过-Taint-Toleration-调整-Master-执行-Pod-的策略" class="headerlink" title="通过 Taint/Toleration 调整 Master 执行 Pod 的策略"></a>通过 Taint/Toleration 调整 Master 执行 Pod 的策略</h3><p>打污点(taint)指令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl taint nodes node1 foo=bar:NoSchedule</span><br></pre></td></tr></table></figure><p>pod容忍(tolerations)这个污点指令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  tolerations:</span><br><span class="line">  - key: &quot;foo&quot;</span><br><span class="line">    operator: &quot;Equal&quot;</span><br><span class="line">    value: &quot;bar&quot;</span><br><span class="line">    effect: &quot;NoSchedule&quot;</span><br></pre></td></tr></table></figure><p>我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。</p><p>查看master默认污点</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe node master</span><br><span class="line"></span><br><span class="line">Name:               master</span><br><span class="line">Roles:              master</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure><p>删除master默认污点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line">node.kubernetes.io/not-ready</span><br></pre></td></tr></table></figure><p>或者Pod容忍这个污点</p><p>可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  tolerations:</span><br><span class="line">  - key: &quot;foo&quot;</span><br><span class="line">    operator: &quot;Exists&quot;</span><br><span class="line">    effect: &quot;NoSchedule&quot;</span><br></pre></td></tr></table></figure><h3 id="部署-Dashboard-可视化插件"><a href="#部署-Dashboard-可视化插件" class="headerlink" title="部署 Dashboard 可视化插件"></a>部署 Dashboard 可视化插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml</span><br></pre></td></tr></table></figure><h3 id="部署容器存储插件"><a href="#部署容器存储插件" class="headerlink" title="部署容器存储插件"></a>部署容器存储插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml</span><br></pre></td></tr></table></figure><h2 id="创建容器化应用"><a href="#创建容器化应用" class="headerlink" title="创建容器化应用"></a>创建容器化应用</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure><h3 id="替换"><a href="#替换" class="headerlink" title="替换"></a>替换</h3><p>修改yaml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...    </span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.8 #这里被从1.7.9修改为1.8</span><br><span class="line">        ports:</span><br><span class="line">      - containerPort: 80</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl replace -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure><h3 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br><span class="line"></span><br><span class="line"># 修改nginx-deployment.yaml的内容</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure><h2 id="Pod属性"><a href="#Pod属性" class="headerlink" title="Pod属性"></a>Pod属性</h2><h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: nginx-vol</span><br><span class="line">      volumes:</span><br><span class="line">      - name: nginx-vol</span><br><span class="line">        emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure><h4 id="emptyDir"><a href="#emptyDir" class="headerlink" title="emptyDir"></a>emptyDir</h4><p>它其实就等同于我们之前讲过的 Docker 的隐式 Volume 参数，即：不显式声明宿主机目录的 Volume。所以，Kubernetes 也会在宿主机上创建一个临时目录，这个目录将来就会被绑定挂载到容器所声明的 Volume 目录上。<strong><em>备注：不难看到，Kubernetes 的 emptyDir 类型，只是把 Kubernetes 创建的临时目录作为 Volume 的宿主机目录，交给了 Docker。这么做的原因，是 Kubernetes 不想依赖 Docker 自己创建的那个 _data 目录。</em></strong></p><h4 id="hostPath"><a href="#hostPath" class="headerlink" title="hostPath"></a>hostPath</h4><p>当然，Kubernetes 也提供了显式的 Volume 定义，它叫做 hostPath。比如下面的这个 YAML 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...   </span><br><span class="line">   volumes:</span><br><span class="line">     - name: nginx-vol</span><br><span class="line">       hostPath: </span><br><span class="line">         path: /var/data</span><br></pre></td></tr></table></figure><p>进入容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec -it nginx-deployment-5c678cfb6d-lg9lw -- /bin/bash</span><br><span class="line"># ls /usr/share/nginx/html</span><br></pre></td></tr></table></figure><h4 id="Projected-Volume"><a href="#Projected-Volume" class="headerlink" title="Projected Volume"></a>Projected Volume</h4><p>在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。到目前为止，Kubernetes 支持的 Projected Volume </p><p>一共有四种：</p><ul><li>Secret；</li><li>ConfigMap；</li><li>Downward API；</li><li>ServiceAccountToken。</li></ul><h5 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h5><p>它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-projected-volume </span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: test-secret-volume</span><br><span class="line">    image: busybox</span><br><span class="line">    args:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;86400&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: mysql-cred</span><br><span class="line">      mountPath: &quot;/projected-volume&quot;</span><br><span class="line">      readOnly: true</span><br><span class="line">  volumes:</span><br><span class="line">  - name: mysql-cred</span><br><span class="line">    projected:</span><br><span class="line">      sources:</span><br><span class="line">      - secret:</span><br><span class="line">          name: user</span><br><span class="line">      - secret:</span><br><span class="line">          name: pass</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat ./username.txt</span><br><span class="line">admin</span><br><span class="line">$ cat ./password.txt</span><br><span class="line">c1oudc0w!</span><br><span class="line"></span><br><span class="line">$ kubectl create secret generic user --from-file=./username.txt</span><br><span class="line">$ kubectl create secret generic pass --from-file=./password.txt</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get secrets</span><br><span class="line">NAME           TYPE                                DATA      AGE</span><br><span class="line">user          Opaque                                1         51s</span><br><span class="line">pass          Opaque                                1         51s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-projected-volume.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec -it test-projected-volume -- /bin/sh</span><br><span class="line">$ ls /projected-volume/</span><br><span class="line">user</span><br><span class="line">pass</span><br><span class="line">$ cat /projected-volume/user</span><br><span class="line">root</span><br><span class="line">$ cat /projected-volume/pass</span><br><span class="line">1f2d1e2e67df</span><br></pre></td></tr></table></figure><h5 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h5><p>与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># .properties文件的内容</span><br><span class="line">$ cat example/ui.properties</span><br><span class="line">color.good=purple</span><br><span class="line">color.bad=yellow</span><br><span class="line">allow.textmode=true</span><br><span class="line">how.nice.to.look=fairlyNice</span><br><span class="line"></span><br><span class="line"># 从.properties文件创建ConfigMap</span><br><span class="line">$ kubectl create configmap ui-config --from-file=example/ui.properties</span><br><span class="line"></span><br><span class="line"># 查看这个ConfigMap里保存的信息(data)</span><br><span class="line">$ kubectl get configmaps ui-config -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  ui.properties: |</span><br><span class="line">    color.good=purple</span><br><span class="line">    color.bad=yellow</span><br><span class="line">    allow.textmode=true</span><br><span class="line">    how.nice.to.look=fairlyNice</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: ui-config</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><h5 id="Downward-API"><a href="#Downward-API" class="headerlink" title="Downward API"></a>Downward API</h5><p>它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-downwardapi-volume</span><br><span class="line">  labels:</span><br><span class="line">    zone: us-est-coast</span><br><span class="line">    cluster: test-cluster1</span><br><span class="line">    rack: rack-22</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: client-container</span><br><span class="line">      image: k8s.gcr.io/busybox</span><br><span class="line">      command: [&quot;sh&quot;, &quot;-c&quot;]</span><br><span class="line">      args:</span><br><span class="line">      - while true; do</span><br><span class="line">          if [[ -e /etc/podinfo/labels ]]; then</span><br><span class="line">            echo -en &apos;\n\n&apos;; cat /etc/podinfo/labels; fi;</span><br><span class="line">          sleep 5;</span><br><span class="line">        done;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: podinfo</span><br><span class="line">          mountPath: /etc/podinfo</span><br><span class="line">          readOnly: false</span><br><span class="line">  volumes:</span><br><span class="line">    - name: podinfo</span><br><span class="line">      projected:</span><br><span class="line">        sources:</span><br><span class="line">        - downwardAPI:</span><br><span class="line">            items:</span><br><span class="line">              - path: &quot;labels&quot;</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.labels</span><br></pre></td></tr></table></figure><p>在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。</p><p>通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。</p><p>而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f dapi-volume.yaml</span><br><span class="line">$ kubectl logs test-downwardapi-volume</span><br><span class="line">cluster=&quot;test-cluster1&quot;</span><br><span class="line">rack=&quot;rack-22&quot;</span><br><span class="line">zone=&quot;us-est-coast&quot;</span><br></pre></td></tr></table></figure><p>目前Downward API支持的字段更加丰富了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1. 使用fieldRef可以声明使用:</span><br><span class="line">spec.nodeName - 宿主机名字</span><br><span class="line">status.hostIP - 宿主机IP</span><br><span class="line">metadata.name - Pod的名字</span><br><span class="line">metadata.namespace - Pod的Namespace</span><br><span class="line">status.podIP - Pod的IP</span><br><span class="line">spec.serviceAccountName - Pod的Service Account的名字</span><br><span class="line">metadata.uid - Pod的UID</span><br><span class="line">metadata.labels[&apos;&lt;KEY&gt;&apos;] - 指定&lt;KEY&gt;的Label值</span><br><span class="line">metadata.annotations[&apos;&lt;KEY&gt;&apos;] - 指定&lt;KEY&gt;的Annotation值</span><br><span class="line">metadata.labels - Pod的所有Label</span><br><span class="line">metadata.annotations - Pod的所有Annotation</span><br><span class="line"></span><br><span class="line">2. 使用resourceFieldRef可以声明使用:</span><br><span class="line">容器的CPU limit</span><br><span class="line">容器的CPU request</span><br><span class="line">容器的memory limit</span><br><span class="line">容器的memory request</span><br></pre></td></tr></table></figure><h5 id="Service-Account"><a href="#Service-Account" class="headerlink" title="Service Account"></a>Service Account</h5><p>default Service Account</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lw</span><br><span class="line">Containers:</span><br><span class="line">...</span><br><span class="line">  Mounts:</span><br><span class="line">    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)</span><br><span class="line">Volumes:</span><br><span class="line">  default-token-s8rbq:</span><br><span class="line">  Type:       Secret (a volume populated by a Secret)</span><br><span class="line">  SecretName:  default-token-s8rbq</span><br><span class="line">  Optional:    false</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /var/run/secrets/kubernetes.io/serviceaccount </span><br><span class="line">ca.crt namespace  token</span><br></pre></td></tr></table></figure><h3 id="NodeSelector"><a href="#NodeSelector" class="headerlink" title="NodeSelector"></a>NodeSelector</h3><p>是一个供用户将 Pod 与 Node 进行绑定的字段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line"> nodeSelector:</span><br><span class="line">   disktype: ssd</span><br></pre></td></tr></table></figure><h3 id="NodeName"><a href="#NodeName" class="headerlink" title="NodeName"></a>NodeName</h3><p>一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  nodeName: k8s.node1 #指定调度节点为k8s.node1</span><br></pre></td></tr></table></figure><h3 id="HostAliases"><a href="#HostAliases" class="headerlink" title="HostAliases"></a>HostAliases</h3><p>定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  hostAliases:</span><br><span class="line">  - ip: &quot;10.1.2.3&quot;</span><br><span class="line">    hostnames:</span><br><span class="line">    - &quot;foo.remote&quot;</span><br><span class="line">    - &quot;bar.remote&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cat /etc/hosts</span><br><span class="line"># Kubernetes-managed hosts file.</span><br><span class="line">127.0.0.1 localhost</span><br><span class="line">...</span><br><span class="line">10.244.135.10 hostaliases-pod</span><br><span class="line">10.1.2.3 foo.remote</span><br><span class="line">10.1.2.3 bar.remote</span><br></pre></td></tr></table></figure><h3 id="shareProcessNamespace-true"><a href="#shareProcessNamespace-true" class="headerlink" title="shareProcessNamespace=true"></a>shareProcessNamespace=true</h3><p> Pod 里的容器要共享 PID Namespace。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  shareProcessNamespace: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">  - name: shell</span><br><span class="line">    image: busybox</span><br><span class="line">    stdin: true</span><br><span class="line">    tty: true</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f nginx.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl attach -it nginx -c shell</span><br><span class="line">/ # ps ax</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:00 /pause</span><br><span class="line">    8 root      0:00 nginx: master process nginx -g daemon off;</span><br><span class="line">   14 101       0:00 nginx: worker process</span><br><span class="line">   15 root      0:00 sh</span><br><span class="line">   21 root      0:00 ps ax</span><br></pre></td></tr></table></figure><h3 id="共享宿主机的Linux-namespace"><a href="#共享宿主机的Linux-namespace" class="headerlink" title="共享宿主机的Linux namespace"></a>共享宿主机的Linux namespace</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  hostIPC: true</span><br><span class="line">  hostPID: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">  - name: shell</span><br><span class="line">    image: busybox</span><br><span class="line">    stdin: true</span><br><span class="line">    tty: true</span><br></pre></td></tr></table></figure><h3 id="Probe"><a href="#Probe" class="headerlink" title="Probe"></a>Probe</h3><p>这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。</p><p>####livenessProbe </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    test: liveness</span><br><span class="line">  name: test-liveness-exec</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    image: busybox</span><br><span class="line">    args:</span><br><span class="line">    - /bin/sh</span><br><span class="line">    - -c</span><br><span class="line">    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600</span><br><span class="line">    livenessProbe:</span><br><span class="line">      exec:</span><br><span class="line">        command:</span><br><span class="line">        - cat</span><br><span class="line">        - /tmp/healthy</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 5</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-liveness-exec.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod</span><br><span class="line">NAME                READY     STATUS    RESTARTS   AGE</span><br><span class="line">test-liveness-exec   1/1       Running   0          10s</span><br></pre></td></tr></table></figure><p>可以看到，由于已经通过了健康检查，这个 Pod 就进入了 Running 状态。</p><p>而 30 s 之后，我们再查看一下 Pod 的 Events：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod test-liveness-exec</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你会发现，这个 Pod 在 Events 报告了一个异常：</span><br><span class="line">FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message</span><br><span class="line">--------- --------    -----   ----            -------------           --------    ------      -------</span><br><span class="line">2s        2s      1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Warning     Unhealthy   Liveness probe failed: cat: can&apos;t open &apos;/tmp/healthy&apos;: No such file or directory</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod test-liveness-exec</span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE</span><br><span class="line">liveness-exec   1/1       Running   1          1m</span><br></pre></td></tr></table></figure><p>这时我们发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态。这是为什么呢？其实，如果你注意到 RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了。在这个过程中，Pod 保持 Running 状态不变。</p><p>除了在容器中执行命令外，livenessProbe 也可以定义为发起 HTTP 或者 TCP 请求的方式，定义格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">livenessProbe:</span><br><span class="line">     httpGet:</span><br><span class="line">       path: /healthz</span><br><span class="line">       port: 8080</span><br><span class="line">       httpHeaders:</span><br><span class="line">       - name: X-Custom-Header</span><br><span class="line">         value: Awesome</span><br><span class="line">       initialDelaySeconds: 3</span><br><span class="line">       periodSeconds: 3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    livenessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      periodSeconds: 20</span><br></pre></td></tr></table></figure><h4 id="readinessProbe"><a href="#readinessProbe" class="headerlink" title="readinessProbe"></a>readinessProbe</h4><p>虽然它的用法与 livenessProbe 类似，但作用却大不一样。readinessProbe 检查结果的成功与否，决定的这个 Pod 是不是能被通过 Service 的方式访问到，而并不影响 Pod 的生命周期。</p><h3 id="restartPolicy"><a href="#restartPolicy" class="headerlink" title="restartPolicy"></a>restartPolicy</h3><p>它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。</p><p>但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。但是Deployment可以跑到别的节点上去。</p><p>而作为用户，你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况：</p><ul><li>Always：在任何情况下，只要容器不在运行状态，就自动重启容器；</li><li>OnFailure: 只在容器 异常时才自动重启容器；</li><li>Never: 从来不重启容器。</li></ul><p>实际上，你根本不需要死记硬背这些对应关系，只要记住如下两个基本的设计原理即可</p><ol><li>只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。</li><li>对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数，比如</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod test-liveness-exec</span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE</span><br><span class="line">liveness-exec   0/1       Running   1          1m</span><br></pre></td></tr></table></figure><h3 id="PodPreset"><a href="#PodPreset" class="headerlink" title="PodPreset"></a>PodPreset</h3><p>举个例子，现在开发人员编写了如下一个 pod.yaml 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: website</span><br><span class="line">  labels:</span><br><span class="line">    app: website</span><br><span class="line">    role: frontend</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: website</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><p>这个时候，运维人员就可以定义一个 PodPreset 对象。在这个对象中，凡是他想在开发人员编写的 Pod 里追加的字段，都可以预先定义好。比如这个 preset.yaml:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: settings.k8s.io/v1alpha1</span><br><span class="line">kind: PodPreset</span><br><span class="line">metadata:</span><br><span class="line">  name: allow-database</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: frontend</span><br><span class="line">  env:</span><br><span class="line">    - name: DB_PORT</span><br><span class="line">      value: &quot;6379&quot;</span><br><span class="line">  volumeMounts:</span><br><span class="line">    - mountPath: /cache</span><br><span class="line">      name: cache-volume</span><br><span class="line">  volumes:</span><br><span class="line">    - name: cache-volume</span><br><span class="line">      emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f preset.yaml</span><br><span class="line">$ kubectl create -f pod.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod website -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: website</span><br><span class="line">  labels:</span><br><span class="line">    app: website</span><br><span class="line">    role: frontend</span><br><span class="line">  annotations:</span><br><span class="line">    podpreset.admission.kubernetes.io/podpreset-allow-database: &quot;resource version&quot;</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: website</span><br><span class="line">      image: nginx</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: /cache</span><br><span class="line">          name: cache-volume</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      env:</span><br><span class="line">        - name: DB_PORT</span><br><span class="line">          value: &quot;6379&quot;</span><br><span class="line">  volumes:</span><br><span class="line">    - name: cache-volume</span><br><span class="line">      emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="Container-属性"><a href="#Container-属性" class="headerlink" title="Container 属性"></a>Container 属性</h2><h3 id="ImagePullPolicy"><a href="#ImagePullPolicy" class="headerlink" title="ImagePullPolicy"></a>ImagePullPolicy</h3><p>它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。</p><p>ImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。而如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。</p><h3 id="Lifecycle"><a href="#Lifecycle" class="headerlink" title="Lifecycle"></a>Lifecycle</h3><p>它定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: lifecycle-demo</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: lifecycle-demo-container</span><br><span class="line">    image: nginx</span><br><span class="line">    lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;]</span><br><span class="line">      preStop:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]</span><br></pre></td></tr></table></figure><h2 id="作业副本与水平扩展"><a href="#作业副本与水平扩展" class="headerlink" title="作业副本与水平扩展"></a>作业副本与水平扩展</h2><h3 id="ReplicaSet-结构"><a href="#ReplicaSet-结构" class="headerlink" title="ReplicaSet 结构"></a>ReplicaSet 结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: ReplicaSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-set</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br></pre></td></tr></table></figure><h3 id="Deployment结构"><a href="#Deployment结构" class="headerlink" title="Deployment结构"></a>Deployment结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><h3 id="Deployment和ReplicaSet关系"><a href="#Deployment和ReplicaSet关系" class="headerlink" title="Deployment和ReplicaSet关系"></a>Deployment和ReplicaSet关系</h3><p><img src="/2020/02/21/k8s-command/images/image-20200222120521800.png" alt="image-20200222120521800"></p><h3 id="部署deployment"><a href="#部署deployment" class="headerlink" title="部署deployment"></a>部署deployment</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f nginx-deployment.yaml --record</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get deployments</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         0         0            0           1s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout status deployment/nginx-deployment</span><br><span class="line">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">deployment.apps/nginx-deployment successfully rolled out</span><br></pre></td></tr></table></figure><h3 id="查看replicaSet状态"><a href="#查看replicaSet状态" class="headerlink" title="查看replicaSet状态"></a>查看replicaSet状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-3167673210   3         3         3       20s</span><br></pre></td></tr></table></figure><h3 id="edit-deployment"><a href="#edit-deployment" class="headerlink" title="edit deployment"></a>edit deployment</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit deployment/nginx-deployment</span><br><span class="line">... </span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.9.1 # 1.7.9 -&gt; 1.9.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">...</span><br><span class="line">deployment.extensions/nginx-deployment edited</span><br></pre></td></tr></table></figure><p>查看deployment 状态变化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout status deployment/nginx-deployment</span><br><span class="line">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">deployment.extensions/nginx-deployment successfully rolled out</span><br></pre></td></tr></table></figure><p>也可以查看deployment的event</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe deployment nginx-deployment</span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age   From                   Message</span><br><span class="line">  ----    ------             ----  ----                   -------</span><br><span class="line">...</span><br><span class="line">  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 1</span><br><span class="line">  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 2</span><br><span class="line">  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 2</span><br><span class="line">  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 1</span><br><span class="line">  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 3</span><br><span class="line">  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 0</span><br></pre></td></tr></table></figure><p>在这个“滚动更新”过程完成之后，你可以查看一下新、旧两个 ReplicaSet 的最终状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-1764197365   3         3         3       6s</span><br><span class="line">nginx-deployment-3167673210   0         0         0       30s</span><br></pre></td></tr></table></figure><h3 id="scale"><a href="#scale" class="headerlink" title="scale"></a>scale</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale deployment nginx-deployment --replicas=4</span><br><span class="line">deployment.apps/nginx-deployment scaled</span><br></pre></td></tr></table></figure><h3 id="RollingUpdateStrategy"><a href="#RollingUpdateStrategy" class="headerlink" title="RollingUpdateStrategy"></a>RollingUpdateStrategy</h3><p>为了进一步保证服务的连续性，Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">...</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 1</span><br></pre></td></tr></table></figure><p>maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。同时，这两个配置还可以用前面我们介绍的百分比形式来表示。</p><p><img src="/2020/02/21/k8s-command/images/image-20200222121522402.png" alt="image-20200222121522402"></p><p>kubectl set image 的指令，直接修改 nginx-deployment 所使用的镜像。这个命令的好处就是，你可以不用像 kubectl edit 那样需要打开编辑器。不过这一次，我把这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91</span><br><span class="line">deployment.extensions/nginx-deployment image updated</span><br></pre></td></tr></table></figure><p>这时来检查下replicaSet的状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-1764197365   2         2         2       24s</span><br><span class="line">nginx-deployment-3167673210   0         0         0       35s</span><br><span class="line">nginx-deployment-2156724341   2         2         0       7s</span><br></pre></td></tr></table></figure><h3 id="回滚到上一个版本"><a href="#回滚到上一个版本" class="headerlink" title="回滚到上一个版本"></a>回滚到上一个版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout undo deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment</span><br></pre></td></tr></table></figure><h3 id="回滚到指定版本"><a href="#回滚到指定版本" class="headerlink" title="回滚到指定版本"></a>回滚到指定版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout history deployment/nginx-deployment</span><br><span class="line">deployments &quot;nginx-deployment&quot;</span><br><span class="line">REVISION    CHANGE-CAUSE</span><br><span class="line">1           kubectl create -f nginx-deployment.yaml --record</span><br><span class="line">2           kubectl edit deployment/nginx-deployment</span><br><span class="line">3           kubectl set image deployment/nginx-deployment nginx=nginx:1.91</span><br></pre></td></tr></table></figure><p>当然，你还可以通过这个 kubectl rollout history 指令，看到每个版本对应的 Deployment 的 API 对象的细节，具体命令如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout history deployment/nginx-deployment --revision=2</span><br></pre></td></tr></table></figure><p>回滚到指定版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout undo deployment/nginx-deployment --to-revision=2</span><br><span class="line">deployment.extensions/nginx-deployment</span><br></pre></td></tr></table></figure><h3 id="多次修改deployment只生成一个rs"><a href="#多次修改deployment只生成一个rs" class="headerlink" title="多次修改deployment只生成一个rs"></a>多次修改deployment只生成一个rs</h3><p>你可能已经想到了一个问题：我们对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象，是不是有些多余，甚至浪费资源呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout pause deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment paused</span><br></pre></td></tr></table></figure><p>这个 kubectl rollout pause 的作用，是让这个 Deployment 进入了一个“暂停”状态。</p><p>所以接下来，你就可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。</p><p>由于此时 Deployment 正处于“暂停”状态，所以我们对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>而等到我们对 Deployment 修改操作都完成之后，只需要再执行一条 kubectl rollout resume 指令，就可以把这个 Deployment“恢复”回来，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout resume deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment resumed</span><br></pre></td></tr></table></figure><p>查看rs状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME               DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-1764197365   0         0         0         2m</span><br><span class="line">nginx-3196763511   3         3         3         28s</span><br></pre></td></tr></table></figure><h3 id="控制这些“历史”ReplicaSet-的数量呢？"><a href="#控制这些“历史”ReplicaSet-的数量呢？" class="headerlink" title="控制这些“历史”ReplicaSet 的数量呢？"></a>控制这些“历史”ReplicaSet 的数量呢？</h3><p>很简单，Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了。</p><h2 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h2><h3 id="拓扑状态"><a href="#拓扑状态" class="headerlink" title="拓扑状态"></a>拓扑状态</h3><p>svc.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    name: web</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br></pre></td></tr></table></figure><p>当你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;pod-name&gt;.&lt;svc-name&gt;.&lt;namespace&gt;.svc.cluster.local</span><br></pre></td></tr></table></figure><p>statefulset.yaml</p><p>这个 YAML 文件，和我们在前面文章中用到的 nginx-deployment 的唯一区别，就是多了一个 serviceName=nginx 字段。这个字段的作用，就是告诉 StatefulSet 控制器，在执行控制循环（Control Loop）的时候，请使用 nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.9.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br></pre></td></tr></table></figure><p>所以，当你通过 kubectl create 创建了上面这个 Service 和 StatefulSet 之后，就会看到如下两个对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f svc.yaml</span><br><span class="line">$ kubectl get service nginx</span><br><span class="line">NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">nginx     ClusterIP    None         &lt;none&gt;        80/TCP    10s</span><br><span class="line"></span><br><span class="line">$ kubectl create -f statefulset.yaml</span><br><span class="line">$ kubectl get statefulset web</span><br><span class="line">NAME      DESIRED   CURRENT   AGE</span><br><span class="line">web       2         1         19s</span><br></pre></td></tr></table></figure><p>这时候，如果你手比较快的话，还可以通过 kubectl 的 -w 参数，即：Watch 功能，实时查看 StatefulSet 创建两个有状态实例的过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -w -l app=nginx</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE</span><br><span class="line">web-0     0/1       Pending   0          0s</span><br><span class="line">web-0     0/1       Pending   0         0s</span><br><span class="line">web-0     0/1       ContainerCreating   0         0s</span><br><span class="line">web-0     1/1       Running   0         19s</span><br><span class="line">web-1     0/1       Pending   0         0s</span><br><span class="line">web-1     0/1       Pending   0         0s</span><br><span class="line">web-1     0/1       ContainerCreating   0         0s</span><br><span class="line">web-1     1/1       Running   0         20s</span><br></pre></td></tr></table></figure><p>我们使用 kubectl exec 命令进入到容器中查看它们的 hostname：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec web-0 -- sh -c &apos;hostname&apos;</span><br><span class="line">web-0</span><br><span class="line">$ kubectl exec web-1 -- sh -c &apos;hostname&apos;</span><br><span class="line">web-1</span><br></pre></td></tr></table></figure><p>通过这条命令，我们启动了一个一次性的 Pod，因为–rm 意味着 Pod 退出后就会被删除掉。然后，在这个 Pod 的容器里面，我们尝试用 nslookup 命令，解析一下 Pod 对应的 Headless Service：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh</span><br><span class="line">$ nslookup web-0.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-0.nginx</span><br><span class="line">Address 1: 10.244.1.7</span><br><span class="line"></span><br><span class="line">$ nslookup web-1.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-1.nginx</span><br><span class="line">Address 1: 10.244.2.7</span><br></pre></td></tr></table></figure><p>删掉pod看看会出现什么情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete pod -l app=nginx</span><br><span class="line">pod &quot;web-0&quot; deleted</span><br><span class="line">pod &quot;web-1&quot; deleted</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pod -w -l app=nginx</span><br><span class="line">NAME      READY     STATUS              RESTARTS   AGE</span><br><span class="line">web-0     0/1       ContainerCreating   0          0s</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE</span><br><span class="line">web-0     1/1       Running   0          2s</span><br><span class="line">web-1     0/1       Pending   0         0s</span><br><span class="line">web-1     0/1       ContainerCreating   0         0s</span><br><span class="line">web-1     1/1       Running   0         32s</span><br></pre></td></tr></table></figure><p>可以看到，当我们把这两个 Pod 删除之后，Kubernetes 会按照原先编号的顺序，创建出了两个新的 Pod。并且，Kubernetes 依然为它们分配了与原来相同的“网络身份”：web-0.nginx 和 web-1.nginx。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh </span><br><span class="line">$ nslookup web-0.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-0.nginx</span><br><span class="line">Address 1: 10.244.1.8</span><br><span class="line"></span><br><span class="line">$ nslookup web-1.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-1.nginx</span><br><span class="line">Address 1: 10.244.2.8</span><br></pre></td></tr></table></figure><p>不过，相信你也已经注意到了，尽管 web-0.nginx 这条记录本身不会变，但它解析到的 Pod 的 IP 地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式，而绝不应该直接访问这些 Pod 的 IP 地址。</p><h3 id="存储状态"><a href="#存储状态" class="headerlink" title="存储状态"></a>存储状态</h3><h4 id="PVC"><a href="#PVC" class="headerlink" title="PVC"></a>PVC</h4><p>如果你还是不太理解 PVC 的话，可以先记住这样一个结论：PVC 其实就是一种特殊的 Volume。只不过一个 PVC 具体是什么类型的 Volume，要在跟某个 PV 绑定之后才知道。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure><h4 id="在Pod中声明这个PVC"><a href="#在Pod中声明这个PVC" class="headerlink" title="在Pod中声明这个PVC"></a>在Pod中声明这个PVC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: pv-container</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: &quot;http-server&quot;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: pv-storage</span><br><span class="line">  volumes:</span><br><span class="line">    - name: pv-storage</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: pv-claim</span><br></pre></td></tr></table></figure><h4 id="PV"><a href="#PV" class="headerlink" title="PV"></a>PV</h4><p>可以看到，在这个 Pod 的 Volumes 定义中，我们只需要声明它的类型是 persistentVolumeClaim，然后指定 PVC 的名字，而完全不必关心 Volume 本身的定义。</p><p>这时候，只要我们创建这个 PVC 对象，Kubernetes 就会自动为它绑定一个符合条件的 Volume。可是，这些符合条件的 Volume 又是从哪里来的呢？</p><p>答案是，它们来自于由运维人员维护的 PV（Persistent Volume）对象。接下来，我们一起看一个常见的 PV 对象的 YAML 文件。</p><p>pv.yaml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-volume</span><br><span class="line">  labels:</span><br><span class="line">    type: local</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  rbd:</span><br><span class="line">    monitors:</span><br><span class="line">    - &apos;rook-ceph-mon-a-698fc4d55f-q4x8d&apos;</span><br><span class="line">    - &apos;rook-ceph-mon-b-7cf9c7bcb7-wjld6&apos;</span><br><span class="line">    - &apos;rook-ceph-mon-c-5fb4dfb454-gnrll&apos;</span><br><span class="line">    pool: kube</span><br><span class="line">    image: foo</span><br><span class="line">    fsType: ext4</span><br><span class="line">    readOnly: true</span><br><span class="line">    user: admin</span><br><span class="line">    keyring: /etc/ceph/keyring</span><br></pre></td></tr></table></figure><p>–注monitors –修改为 kubectl get pods -n rook-ceph 查看 rook-ceph-mon- 开头的Pod id </p><p>所以，Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。</p><h4 id="StatefulSet-集成PVC"><a href="#StatefulSet-集成PVC" class="headerlink" title="StatefulSet 集成PVC"></a>StatefulSet 集成PVC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.9.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: www</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: www</span><br><span class="line">    spec:</span><br><span class="line">      accessModes:</span><br><span class="line">      - ReadWriteOnce</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 1Gi</span><br></pre></td></tr></table></figure><p>create</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f pv.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f statefulset.yaml</span><br><span class="line">$ kubectl get pvc -l app=nginx</span><br><span class="line">NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE</span><br><span class="line">www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s</span><br><span class="line">www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ for i in 0 1; do kubectl exec web-$i -- sh -c &apos;echo hello $(hostname) &gt; /usr/share/nginx/html/index.html&apos;; done</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done</span><br><span class="line">hello web-0</span><br><span class="line">hello web-1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 删除pod</span><br><span class="line">$ kubectl delete pod -l app=nginx</span><br><span class="line">pod &quot;web-0&quot; deleted</span><br><span class="line">pod &quot;web-1&quot; deleted</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 在被重新创建出来的Pod容器里访问http://localhost</span><br><span class="line">$ kubectl exec -it web-0 -- curl localhost</span><br><span class="line">hello web-0</span><br></pre></td></tr></table></figure><h2 id="daemonSet"><a href="#daemonSet" class="headerlink" title="daemonSet"></a>daemonSet</h2><h3 id="滚动更新"><a href="#滚动更新" class="headerlink" title="滚动更新"></a>滚动更新</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl patch statefulset mysql --type=&apos;json&apos; -p=&apos;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;mysql:5.7.23&quot;&#125;]&apos;</span><br><span class="line">statefulset.apps/mysql patched</span><br></pre></td></tr></table></figure><h3 id="灰度发布"><a href="#灰度发布" class="headerlink" title="灰度发布"></a>灰度发布</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl patch statefulset mysql -p &apos;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:2&#125;&#125;&#125;&#125;&apos;</span><br><span class="line">statefulset.apps/mysql patched</span><br></pre></td></tr></table></figure><p>这样，我就指定了当 Pod 模板发生变化的时候，比如 MySQL 镜像更新到 5.7.23，那么只有序号大于或者等于 2 的 Pod 会被更新到这个版本。并且，如果你删除或者重启了序号小于 2 的 Pod，等它再次启动后，也会保持原先的 5.7.2 版本，绝不会被升级到 5.7.23 版本。</p><h3 id="deamonSet结构"><a href="#deamonSet结构" class="headerlink" title="deamonSet结构"></a>deamonSet结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-elasticsearch</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-logging</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: fluentd-elasticsearch</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: fluentd-elasticsearch</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io/master</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">      - name: fluentd-elasticsearch</span><br><span class="line">        image: k8s.gcr.io/fluentd-elasticsearch:1.20</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 200Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 200Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: varlog</span><br><span class="line">          mountPath: /var/log</span><br><span class="line">        - name: varlibdockercontainers</span><br><span class="line">          mountPath: /var/lib/docker/containers</span><br><span class="line">          readOnly: true</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: varlog</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/log</span><br><span class="line">      - name: varlibdockercontainers</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/lib/docker/containers</span><br></pre></td></tr></table></figure><h3 id="nodeAffinity"><a href="#nodeAffinity" class="headerlink" title="nodeAffinity"></a>nodeAffinity</h3><p>在 Kubernetes 项目里，nodeSelector 其实已经是一个将要被废弃的字段了。因为，现在有了一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-node-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: metadata.name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - node-geektime</span><br></pre></td></tr></table></figure><h3 id="Toleration"><a href="#Toleration" class="headerlink" title="Toleration"></a>Toleration</h3><p>DaemonSet 的“过人之处”，其实就是依靠 Toleration 实现的。假如当前 DaemonSet 管理的，是一个网络插件的 Agent Pod，那么你就必须在这个 DaemonSet 的 YAML 文件里，给它的 Pod 模板加上一个能够“容忍”node.kubernetes.io/network-unavailable“污点”的 Toleration。正如下面这个例子所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">...</span><br><span class="line">template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: network-plugin-agent</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node.kubernetes.io/network-unavailable</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoSchedule</span><br></pre></td></tr></table></figure><p>daemonSet 如何确保能在在mater上部署</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tolerations:</span><br><span class="line">- key: node-role.kubernetes.io/master</span><br><span class="line">  effect: NoSchedule</span><br></pre></td></tr></table></figure><h3 id="创建deamonSet"><a href="#创建deamonSet" class="headerlink" title="创建deamonSet"></a>创建deamonSet</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f fluentd-elasticsearch.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pod -n kube-system -l name=fluentd-elasticsearch</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">fluentd-elasticsearch-dqfv9   1/1       Running   0          53m</span><br><span class="line">fluentd-elasticsearch-pf9z5   1/1       Running   0          53m</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get ds -n kube-system fluentd-elasticsearch</span><br><span class="line">NAME                    DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">fluentd-elasticsearch   2         2         2         2            2           &lt;none&gt;          1h</span><br></pre></td></tr></table></figure><h3 id="回滚daemonSet"><a href="#回滚daemonSet" class="headerlink" title="回滚daemonSet"></a>回滚daemonSet</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system</span><br><span class="line">daemonsets &quot;fluentd-elasticsearch&quot;</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br></pre></td></tr></table></figure><p>滚动更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record -n=kube-system</span><br></pre></td></tr></table></figure><p>查看滚动更新状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system</span><br><span class="line">Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 0 out of 2 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 0 out of 2 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 1 of 2 updated pods are available...</span><br><span class="line">daemon set &quot;fluentd-elasticsearch&quot; successfully rolled out</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system</span><br><span class="line">daemonsets &quot;fluentd-elasticsearch&quot;</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br><span class="line">2         kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --namespace=kube-system --record=true</span><br></pre></td></tr></table></figure><h3 id="ControllerRevision对象"><a href="#ControllerRevision对象" class="headerlink" title="ControllerRevision对象"></a>ControllerRevision对象</h3><p>而我在前面的文章中讲解 Deployment 对象的时候，曾经提到过，Deployment 管理这些版本，靠的是“一个版本对应一个 ReplicaSet 对象”。</p><p>可是，DaemonSet 控制器操作的直接就是 Pod，不可能有 ReplicaSet 这样的对象参与其中。那么，它的这些版本又是如何维护的呢？所谓，一切皆对象！在 Kubernetes 项目中，任何你觉得需要记录下来的状态，都可以被用 API 对象的方式实现。当然，“版本”也不例外。Kubernetes v1.7 之后添加了一个 API 对象，名叫 ControllerRevision</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get controllerrevision -n kube-system -l name=fluentd-elasticsearch</span><br><span class="line">NAME                               CONTROLLER                             REVISION   AGE</span><br><span class="line">fluentd-elasticsearch-64dc6799c9   daemonset.apps/fluentd-elasticsearch   2          1h</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 -n kube-system</span><br><span class="line">Name:         fluentd-elasticsearch-64dc6799c9</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       controller-revision-hash=2087235575</span><br><span class="line">              name=fluentd-elasticsearch</span><br><span class="line">Annotations:  deprecated.daemonset.template.generation=2</span><br><span class="line">              kubernetes.io/change-cause=kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record=true --namespace=kube-system</span><br><span class="line">API Version:  apps/v1</span><br><span class="line">Data:</span><br><span class="line">  Spec:</span><br><span class="line">    Template:</span><br><span class="line">      $ Patch:  replace</span><br><span class="line">      Metadata:</span><br><span class="line">        Creation Timestamp:  &lt;nil&gt;</span><br><span class="line">        Labels:</span><br><span class="line">          Name:  fluentd-elasticsearch</span><br><span class="line">      Spec:</span><br><span class="line">        Containers:</span><br><span class="line">          Image:              k8s.gcr.io/fluentd-elasticsearch:v2.2.0</span><br><span class="line">          Image Pull Policy:  IfNotPresent</span><br><span class="line">          Name:               fluentd-elasticsearch</span><br><span class="line">...</span><br><span class="line">Revision:                  2</span><br><span class="line">Events:                    &lt;none&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision=1 -n kube-system</span><br><span class="line">daemonset.extensions/fluentd-elasticsearch rolled back</span><br></pre></td></tr></table></figure><p>执行完这次回滚完成后，你会发现，DaemonSet 的 Revision 并不会从 Revision=2 退回到 1，而是会增加成 Revision=3。这是因为，一个新的 ControllerRevision 被创建了出来。</p><h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: pi</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: pi</span><br><span class="line">        image: resouer/ubuntu-bc </span><br><span class="line">        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo &apos;scale=10000; 4*a(1)&apos; | bc -l &quot;]</span><br><span class="line">      restartPolicy: Never</span><br><span class="line">  backoffLimit: 4</span><br></pre></td></tr></table></figure><p>job里的restartPolicy只有两种选择：</p><p>restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod</p><p>restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spec:</span><br><span class="line"> backoffLimit: 5 #最多失败几次就不在创建Pod默认是6</span><br><span class="line"> activeDeadlineSeconds: 100 #最长运行时间</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spec: </span><br><span class="line">parallelism: 2 #同一时间最多有几个Pod执行 </span><br><span class="line">completions: 4 #总共执行多少个Pod</span><br></pre></td></tr></table></figure><h2 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h2><h3 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;*/1 * * * *&quot;</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: hello</span><br><span class="line">            image: busybox</span><br><span class="line">            args:</span><br><span class="line">            - /bin/sh</span><br><span class="line">            - -c</span><br><span class="line">            - date; echo Hello from the Kubernetes cluster</span><br><span class="line">          restartPolicy: OnFailure</span><br></pre></td></tr></table></figure><p>这个 Cron 表达式里 <em>/1 中的 </em> 表示从 0 开始，/ 表示“每”，1 表示偏移量。</p><p>所以，它的意思就是：从 0 开始，每 1 个时间单位执行一次。那么，时间单位又是什么呢？Cron 表达式中的五个部分分别代表：</p><p>分钟、小时、日、月、星期。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f ./cronjob.yaml</span><br><span class="line">cronjob &quot;hello&quot; created</span><br><span class="line"></span><br><span class="line"># 一分钟后</span><br><span class="line">$ kubectl get jobs</span><br><span class="line">NAME               DESIRED   SUCCESSFUL   AGE</span><br><span class="line">hello-4111706356   1         1         2s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get cronjob hello</span><br><span class="line">NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE</span><br><span class="line">hello     */1 * * * *   False     0         Thu, 6 Sep 2018 14:34:00 -070</span><br></pre></td></tr></table></figure><h3 id="spec-concurrencyPolicy"><a href="#spec-concurrencyPolicy" class="headerlink" title="spec.concurrencyPolicy"></a>spec.concurrencyPolicy</h3><p>需要注意的是，由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如：</p><p>concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在；</p><p>concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过；</p><p>concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。</p><h3 id="spec-startingDeadlineSeconds"><a href="#spec-startingDeadlineSeconds" class="headerlink" title="spec.startingDeadlineSeconds"></a>spec.startingDeadlineSeconds</h3><p>而如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job。这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。</p><h2 id="Initializer（Dynamic-Admission-Control）"><a href="#Initializer（Dynamic-Admission-Control）" class="headerlink" title="Initializer（Dynamic Admission Control）"></a>Initializer（Dynamic Admission Control）</h2><h3 id="istio架构图"><a href="#istio架构图" class="headerlink" title="istio架构图"></a>istio架构图</h3><p><img src="/2020/02/21/k8s-command/images/image-20200225192309030.png" alt="image-20200225192309030" style="zoom: 33%;"></p><p><strong>Istio 最根本的组件，是运行在每一个应用 Pod 里的 Envoy 容器</strong></p><p>这个 Envoy 项目是 Lyft 公司推出的一个高性能 C++ 网络代理，也是 Lyft 公司对 Istio 项目的唯一贡献。</p><p>而 Istio 项目，则把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。我们知道，Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。</p><p>这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。</p><h3 id="Dynamic-Admission-Control"><a href="#Dynamic-Admission-Control" class="headerlink" title="Dynamic Admission Control"></a>Dynamic Admission Control</h3><p>Istio 项目明明需要在每个 Pod 里安装一个 Envoy 容器，又怎么能做到“无感”的呢？</p><p>实际上，Istio 项目使用的，是 Kubernetes 中的一个非常重要的功能，叫作 Dynamic Admission Control。</p><p>在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，总有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>举个例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp-container</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo Hello Kubernetes! &amp;&amp; sleep 3600&apos;]</span><br></pre></td></tr></table></figure><p>Istio 项目要做的，就是在这个 Pod YAML 被提交给 Kubernetes 之后，在它对应的 API 对象里自动加上 Envoy 容器的配置，使这个对象变成如下所示的样子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp-container</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo Hello Kubernetes! &amp;&amp; sleep 3600&apos;]</span><br><span class="line">  - name: envoy</span><br><span class="line">    image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1</span><br><span class="line">    command: [&quot;/usr/local/bin/envoy&quot;]</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>如何做到的呢</p><p>首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。这个 ConfigMap（名叫：envoy-initializer）的定义如下所示：</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: envoy-initializer</span><br><span class="line">data:</span><br><span class="line">  config: |</span><br><span class="line">    containers:</span><br><span class="line">      - name: envoy</span><br><span class="line">        image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1</span><br><span class="line">        command: [&quot;/usr/local/bin/envoy&quot;]</span><br><span class="line">        args:</span><br><span class="line">          - &quot;--concurrency 4&quot;</span><br><span class="line">          - &quot;--config-path /etc/envoy/envoy.json&quot;</span><br><span class="line">          - &quot;--mode serve&quot;</span><br><span class="line">        ports:</span><br><span class="line">          - containerPort: 80</span><br><span class="line">            protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: &quot;1000m&quot;</span><br><span class="line">            memory: &quot;512Mi&quot;</span><br><span class="line">          requests:</span><br><span class="line">            cpu: &quot;100m&quot;</span><br><span class="line">            memory: &quot;64Mi&quot;</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: envoy-conf</span><br><span class="line">            mountPath: /etc/envoy</span><br><span class="line">    volumes:</span><br><span class="line">      - name: envoy-conf</span><br><span class="line">        configMap:</span><br><span class="line">          name: envoy</span><br></pre></td></tr></table></figure><p>接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。这个 Pod 的定义非常简单，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: envoy-initializer</span><br><span class="line">  name: envoy-initializer</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: envoy-initializer</span><br><span class="line">      image: envoy-initializer:0.0.1</span><br><span class="line">      imagePullPolicy: Always</span><br></pre></td></tr></table></figure><p>这个pod会做什么工作呢</p><ul><li>如果这个 Pod 里面已经添加过 Envoy 容器，那么就“放过”这个 Pod，进入下一个检查周期。</li><li>而如果还没有添加过 Envoy 容器的话，它就要进行 Initialize 操作了，即：修改该 Pod 的 API 对象。<br>Initializer 的代码拿到config-map里的data数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求。<br>这样，一个用户提交的 Pod 对象里，就会被自动加上 Envoy 容器相关的字段。</li></ul><h3 id="InitializerConfiguration"><a href="#InitializerConfiguration" class="headerlink" title="InitializerConfiguration"></a>InitializerConfiguration</h3><p>当然，Kubernetes 还允许你通过配置，来指定要对什么样的资源进行这个 Initialize 操作，比如下面这个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: admissionregistration.k8s.io/v1alpha1</span><br><span class="line">kind: InitializerConfiguration</span><br><span class="line">metadata:</span><br><span class="line">  name: envoy-config</span><br><span class="line">initializers:</span><br><span class="line">  // 这个名字必须至少包括两个 &quot;.&quot;</span><br><span class="line">  - name: envoy.initializer.kubernetes.io</span><br><span class="line">    rules:</span><br><span class="line">      - apiGroups:</span><br><span class="line">          - &quot;&quot; // 前面说过， &quot;&quot;就是core API Group的意思</span><br><span class="line">        apiVersions:</span><br><span class="line">          - v1</span><br><span class="line">        resources:</span><br><span class="line">          - pods</span><br></pre></td></tr></table></figure><p>这个配置，就意味着 Kubernetes 要对所有的 Pod 进行这个 Initialize 操作，并且，我们指定了负责这个操作的 Initializer，名叫：envoy-initializer。</p><p>而一旦这个 InitializerConfiguration 被创建，Kubernetes 就会把这个 Initializer 的名字，加在所有新创建的 Pod 的 Metadata 上，格式如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  initializers:</span><br><span class="line">    pending:</span><br><span class="line">      - name: envoy.initializer.kubernetes.io</span><br><span class="line">  name: myapp-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>此外，除了上面的配置方法，你还可以在具体的 Pod 的 Annotation 里添加一个如下所示的字段，从而声明要使用某个 Initializer：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata</span><br><span class="line">  annotations:</span><br><span class="line">    &quot;initializer.kubernetes.io/envoy&quot;: &quot;true&quot;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>在这个 Pod 里，我们添加了一个 Annotation，写明： initializer.kubernetes.io/envoy=true。这样，就会使用到我们前面所定义的 envoy-initializer 了。</p><h2 id="API对象"><a href="#API对象" class="headerlink" title="API对象"></a>API对象</h2><h3 id="API对象结构"><a href="#API对象结构" class="headerlink" title="API对象结构"></a>API对象结构</h3><p><img src="/2020/02/21/k8s-command/images/image-20200226080846682.png" alt="image-20200226080846682"></p><h3 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h3><p><img src="/2020/02/21/k8s-command/images/image-20200226080929620.png" alt="image-20200226080929620"></p><h3 id="自定义API资源"><a href="#自定义API资源" class="headerlink" title="自定义API资源"></a>自定义API资源</h3><h4 id="CR（Custom-Resource）"><a href="#CR（Custom-Resource）" class="headerlink" title="CR（Custom Resource）"></a>CR（Custom Resource）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: samplecrd.k8s.io/v1</span><br><span class="line">kind: Network</span><br><span class="line">metadata:</span><br><span class="line">  name: example-network</span><br><span class="line">spec:</span><br><span class="line">  cidr: &quot;192.168.0.0/16&quot;</span><br><span class="line">  gateway: &quot;192.168.0.1&quot;</span><br></pre></td></tr></table></figure><h4 id="CRD（Custom-Resource-Definition）"><a href="#CRD（Custom-Resource-Definition）" class="headerlink" title="CRD（Custom Resource Definition）"></a>CRD（Custom Resource Definition）</h4><p>这就是一个 Network API 资源类型的 API 部分的宏观定义了。这就等同于告诉了计算机：“兔子是哺乳动物”。所以这时候，Kubernetes 就能够认识和处理所有声明了 API 类型是“samplecrd.k8s.io/v1/network”的 YAML 文件了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: networks.samplecrd.k8s.io</span><br><span class="line">spec:</span><br><span class="line">  group: samplecrd.k8s.io</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: Network</span><br><span class="line">    plural: networks</span><br><span class="line">  scope: Namespaced</span><br></pre></td></tr></table></figure><p>接下来，我还需要让 Kubernetes“认识”这种 YAML 文件里描述的“网络”部分，比如“cidr”（网段），“gateway”（网关）这些字段的含义。这就相当于我要告诉计算机：“兔子有长耳朵和三瓣嘴”。</p><h4 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h4><h5 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ tree $GOPATH/src/github.com/&lt;your-name&gt;/k8s-controller-custom-resource</span><br><span class="line">.</span><br><span class="line">├── controller.go</span><br><span class="line">├── crd</span><br><span class="line">│   └── network.yaml</span><br><span class="line">├── example</span><br><span class="line">│   └── example-network.yaml</span><br><span class="line">├── main.go</span><br><span class="line">└── pkg</span><br><span class="line">    └── apis</span><br><span class="line">        └── samplecrd</span><br><span class="line">            ├── register.go</span><br><span class="line">            └── v1</span><br><span class="line">                ├── doc.go</span><br><span class="line">                ├── register.go</span><br><span class="line">                └── types.go</span><br></pre></td></tr></table></figure><p>其中，pkg/apis/samplecrd 就是 API 组的名字，v1 是版本，而 v1 下面的 types.go 文件里，则定义了 Network 对象的完整描述。</p><h5 id="pkg-apis-samplecrd-register-go"><a href="#pkg-apis-samplecrd-register-go" class="headerlink" title="pkg/apis/samplecrd/register.go"></a>pkg/apis/samplecrd/register.go</h5><p>register.go,用来放置后面要用到的全局变量。这个文件的内容如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">package samplecrd</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line"> GroupName = &quot;samplecrd.k8s.io&quot;</span><br><span class="line"> Version   = &quot;v1&quot;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h5 id="pkg-apis-samplecrd-v1-doc-go"><a href="#pkg-apis-samplecrd-v1-doc-go" class="headerlink" title="pkg/apis/samplecrd/v1/doc.go"></a>pkg/apis/samplecrd/v1/doc.go</h5><p>doc.go 文件（Golang 的文档源文件）。这个文件里的内容如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// +k8s:deepcopy-gen=package</span><br><span class="line"></span><br><span class="line">// +groupName=samplecrd.k8s.io</span><br><span class="line">package v1</span><br></pre></td></tr></table></figure><p>其中，+k8s:deepcopy-gen=package 意思是，请为整个 v1 包里的所有类型定义自动生成 DeepCopy 方法；而+groupName=samplecrd.k8s.io，则定义了这个包对应的 API 组的名字。可以看到，这些定义在 doc.go 文件的注释，起到的是全局的代码生成控制的作用，所以也被称为 Global Tags。</p><h5 id="pkg-apis-samplecrd-v1-types-go"><a href="#pkg-apis-samplecrd-v1-types-go" class="headerlink" title="pkg/apis/samplecrd/v1/types.go"></a>pkg/apis/samplecrd/v1/types.go</h5><p>接下来，我需要添加 types.go 文件。顾名思义，它的作用就是定义一个 Network 类型到底有哪些字段（比如，spec 字段里的内容）。这个文件的主要内容如下所示：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> v1</span><br><span class="line">...</span><br><span class="line"><span class="comment">// +genclient</span></span><br><span class="line"><span class="comment">// +genclient:noStatus</span></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Network describes a Network resource</span></span><br><span class="line"><span class="keyword">type</span> Network <span class="keyword">struct</span> &#123;</span><br><span class="line"> <span class="comment">// TypeMeta is the metadata for the resource, like kind and apiversion</span></span><br><span class="line"> metav1.TypeMeta <span class="string">`json:",inline"`</span></span><br><span class="line"> <span class="comment">// ObjectMeta contains the metadata for the particular object, including</span></span><br><span class="line"> <span class="comment">// things like...</span></span><br><span class="line"> <span class="comment">//  - name</span></span><br><span class="line"> <span class="comment">//  - namespace</span></span><br><span class="line"> <span class="comment">//  - self link</span></span><br><span class="line"> <span class="comment">//  - labels</span></span><br><span class="line"> <span class="comment">//  - ... etc ...</span></span><br><span class="line"> metav1.ObjectMeta <span class="string">`json:"metadata,omitempty"`</span></span><br><span class="line"> </span><br><span class="line"> Spec networkspec <span class="string">`json:"spec"`</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// networkspec is the spec for a Network resource</span></span><br><span class="line"><span class="keyword">type</span> networkspec <span class="keyword">struct</span> &#123;</span><br><span class="line"> Cidr    <span class="keyword">string</span> <span class="string">`json:"cidr"`</span></span><br><span class="line"> Gateway <span class="keyword">string</span> <span class="string">`json:"gateway"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// NetworkList is a list of Network resources</span></span><br><span class="line"><span class="keyword">type</span> NetworkList <span class="keyword">struct</span> &#123;</span><br><span class="line"> metav1.TypeMeta <span class="string">`json:",inline"`</span></span><br><span class="line"> metav1.ListMeta <span class="string">`json:"metadata"`</span></span><br><span class="line"> </span><br><span class="line"> Items []Network <span class="string">`json:"items"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>+genclient 的意思是：请为下面这个 API 资源类型生成对应的 Client 代码,需要注意的是，+genclient 只需要写在 Network 类型上，而不用写在 NetworkList 上。因为 NetworkList 只是一个返回值类型，Network 才是“主类型”。</p><p>+genclient:noStatus 的意思是：这个 API 资源类型定义里，没有 Status 字段。否则，生成的 Client 就会自动带上 UpdateStatus 方法。</p><p>+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object的注释。它的意思是，请在生成 DeepCopy 的时候，实现 Kubernetes 提供的 runtime.Object 接口。否则，在某些版本的 Kubernetes 里，你的这个类型定义会出现编译错误。这是一个固定的操作，记住即可。</p><h5 id="pkg-apis-samplecrd-v1-register-go"><a href="#pkg-apis-samplecrd-v1-register-go" class="headerlink" title="pkg/apis/samplecrd/v1/register.go"></a>pkg/apis/samplecrd/v1/register.go</h5><p>在前面对 APIServer 工作原理的讲解中，我已经提到，“registry”的作用就是注册一个类型（Type）给 APIServer。其中，Network 资源类型在服务器端的注册的工作，APIServer 会自动帮我们完成。但与之对应的，我们还需要让客户端也能“知道”Network 资源类型的定义。这就需要我们在项目里添加一个 register.go 文件。它最主要的功能，就是定义了如下所示的 addKnownTypes() 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package v1</span><br><span class="line">...</span><br><span class="line">// addKnownTypes adds our types to the API scheme by registering</span><br><span class="line">// Network and NetworkList</span><br><span class="line">func addKnownTypes(scheme *runtime.Scheme) error &#123;</span><br><span class="line"> scheme.AddKnownTypes(</span><br><span class="line">  SchemeGroupVersion,</span><br><span class="line">  &amp;Network&#123;&#125;,</span><br><span class="line">  &amp;NetworkList&#123;&#125;,</span><br><span class="line"> )</span><br><span class="line"> </span><br><span class="line"> // register the type in the scheme</span><br><span class="line"> metav1.AddToGroupVersion(scheme, SchemeGroupVersion)</span><br><span class="line"> return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有了这个方法，Kubernetes 就能够在后面生成客户端的时候，“知道”Network 以及 NetworkList 类型的定义了。</p><h5 id="自动生成代码"><a href="#自动生成代码" class="headerlink" title="自动生成代码"></a>自动生成代码</h5><p>接下来，我就要使用 Kubernetes 提供的代码生成工具，为上面定义的 Network 资源类型自动生成 clientset、informer 和 lister。</p><p>这个代码生成工具名叫k8s.io/code-generator，使用方法如下所示：</p><ol><li>设置环境变量,把以下环境变量写到/etc/environment</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/environment</span><br><span class="line"></span><br><span class="line">GOPATH=/root/go</span><br><span class="line"># 代码生成的工作目录，也就是我们的项目路径</span><br><span class="line">ROOT_PACKAGE=&quot;github.com/shiyangtao/k8s-controller-custom-resource&quot;</span><br><span class="line"># API Group</span><br><span class="line">CUSTOM_RESOURCE_NAME=&quot;samplecrd&quot;</span><br><span class="line"># API Version</span><br><span class="line">CUSTOM_RESOURCE_VERSION=&quot;v1&quot;</span><br><span class="line"></span><br><span class="line">source /etc/environment</span><br></pre></td></tr></table></figure><ol start="2"><li>安装k8s.io/code-generator</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ go get -u k8s.io/code-generator/...</span><br><span class="line"></span><br><span class="line"># 如果go get有问题的话换一种方式</span><br><span class="line">mkdir $GOPATH/src/k8s.io</span><br><span class="line">cd $GOPATH/src/k8s.io</span><br><span class="line">git clone https://github.com/kubernetes/code-generator.git</span><br><span class="line">rm -rf text/.git</span><br></pre></td></tr></table></figure><ol start="3"><li>下载代码 </li></ol><p>代码路径 $GOPATH/src/github.com/shiyangtao/k8s-controller-custom-resource</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $GOPATH/src/github.com/shiyangtao/</span><br><span class="line">cd $GOPATH/src/github.com/shiyangtao/</span><br><span class="line">git clone https://github.com/resouer/k8s-controller-custom-resource.git</span><br></pre></td></tr></table></figure><ol start="4"><li>生成代码</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd $GOPATH/src/k8s.io/code-generator</span><br><span class="line"># 执行代码自动生成，其中pkg/client是生成目标目录，pkg/apis是类型定义目录</span><br><span class="line">$ ./generate-groups.sh all &quot;$ROOT_PACKAGE/pkg/client&quot; &quot;$ROOT_PACKAGE/pkg/apis&quot; &quot;$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION&quot;</span><br></pre></td></tr></table></figure><p><strong><em>如果有报错试着执行下 这段代码go get -u k8s.io/apimachinery </em></strong></p><p>代码生成工作完成之后，我们再查看一下这个项目的目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ tree</span><br><span class="line">.</span><br><span class="line">├── controller.go</span><br><span class="line">├── crd</span><br><span class="line">│   └── network.yaml</span><br><span class="line">├── example</span><br><span class="line">│   └── example-network.yaml</span><br><span class="line">├── main.go</span><br><span class="line">└── pkg</span><br><span class="line">    ├── apis</span><br><span class="line">    │   └── samplecrd</span><br><span class="line">    │       ├── constants.go</span><br><span class="line">    │       └── v1</span><br><span class="line">    │           ├── doc.go</span><br><span class="line">    │           ├── register.go</span><br><span class="line">    │           ├── types.go</span><br><span class="line">    │           └── zz_generated.deepcopy.go</span><br><span class="line">    └── client</span><br><span class="line">        ├── clientset</span><br><span class="line">        ├── informers</span><br><span class="line">        └── listers</span><br></pre></td></tr></table></figure><p>其中，pkg/apis/samplecrd/v1 下面的 zz_generated.deepcopy.go 文件，就是自动生成的 DeepCopy 代码文件。</p><p>而整个 client 目录，以及下面的三个包（clientset、informers、 listers），都是 Kubernetes 为 Network 类型生成的客户端库，这些库会在后面编写自定义控制器的时候用到。</p><p>而有了这些内容，现在你就可以在 Kubernetes 集群里创建一个 Network 类型的 API 对象了。我们不妨一起来实验一下。首先，使用 network.yaml 文件，在 Kubernetes 中创建 Network 对象的 CRD（Custom Resource Definition）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl apply -f crd/network.yaml</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created</span><br></pre></td></tr></table></figure><p>然后，我们就可以创建一个 Network 对象了，这里用到的是 example-network.yaml：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl apply -f example/example-network.yaml </span><br><span class="line">network.samplecrd.k8s.io/example-network created</span><br></pre></td></tr></table></figure><p>通过这个操作，你就在 Kubernetes 集群里创建了一个 Network 对象。它的 API 资源路径是samplecrd.k8s.io/v1/networks。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get network</span><br><span class="line">NAME              AGE</span><br><span class="line">example-network   8s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe network example-network</span><br><span class="line">Name:         example-network</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">...API Version:  samplecrd.k8s.io/v1</span><br><span class="line">Kind:         Network</span><br><span class="line">Metadata:</span><br><span class="line">  ...</span><br><span class="line">  Generation:          1</span><br><span class="line">  Resource Version:    468239</span><br><span class="line">  ...</span><br><span class="line">Spec:</span><br><span class="line">  Cidr:     192.168.0.0/16</span><br><span class="line">  Gateway:  192.168.0.1</span><br></pre></td></tr></table></figure><h3 id="自定义控制器"><a href="#自定义控制器" class="headerlink" title="自定义控制器"></a>自定义控制器</h3><h4 id="编写main函数"><a href="#编写main函数" class="headerlink" title="编写main函数"></a>编写main函数</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)</span><br><span class="line">  ...</span><br><span class="line">  kubeClient, err := kubernetes.NewForConfig(cfg)</span><br><span class="line">  ...</span><br><span class="line">  networkClient, err := clientset.NewForConfig(cfg)</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  networkInformerFactory := informers.NewSharedInformerFactory(networkClient, ...)</span><br><span class="line">  </span><br><span class="line">  controller := NewController(kubeClient, networkClient,</span><br><span class="line">  networkInformerFactory.Samplecrd().V1().Networks())</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">go</span> networkInformerFactory.Start(stopCh)</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">if</span> err = controller.Run(<span class="number">2</span>, stopCh); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    glog.Fatalf(<span class="string">"Error running controller: %s"</span>, err.Error())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>第一步：main 函数根据我提供的 Master 配置（APIServer 的地址端口和 kubeconfig 的路径），创建一个 Kubernetes 的 client（kubeClient）和 Network 对象的 client（networkClient）。但是，如果我没有提供 Master 配置呢？这时，main 函数会直接使用一种名叫 InClusterConfig 的方式来创建这个 client。这个方式，会假设你的自定义控制器是以 Pod 的方式运行在 Kubernetes 集群里的。</p><p>第二步：main 函数为 Network 对象创建一个叫作 InformerFactory（即：networkInformerFactory）的工厂，并使用它生成一个 Network 对象的 Informer，传递给控制器。</p><p>第三步：main 函数启动上述的 Informer，然后执行 controller.Run，启动自定义控制器。至此，main 函数就结束了</p><p><img src="/2020/02/21/k8s-command/images/image-20200226093239685.png" alt="image-20200226093239685"></p><h4 id="编写自定义控制器的定义"><a href="#编写自定义控制器的定义" class="headerlink" title="编写自定义控制器的定义"></a>编写自定义控制器的定义</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewController</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">  kubeclientset kubernetes.Interface,</span></span></span><br><span class="line"><span class="function"><span class="params">  networkclientset clientset.Interface,</span></span></span><br><span class="line"><span class="function"><span class="params">  networkInformer informers.NetworkInformer)</span> *<span class="title">Controller</span></span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  controller := &amp;Controller&#123;</span><br><span class="line">    kubeclientset:    kubeclientset,</span><br><span class="line">    networkclientset: networkclientset,</span><br><span class="line">    networksLister:   networkInformer.Lister(),</span><br><span class="line">    networksSynced:   networkInformer.Informer().HasSynced,</span><br><span class="line">    workqueue:        workqueue.NewNamedRateLimitingQueue(...,  <span class="string">"Networks"</span>),</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">    AddFunc: controller.enqueueNetwork,</span><br><span class="line">    UpdateFunc: <span class="function"><span class="keyword">func</span><span class="params">(old, <span class="built_in">new</span> <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">      oldNetwork := old.(*samplecrdv1.Network)</span><br><span class="line">      newNetwork := <span class="built_in">new</span>.(*samplecrdv1.Network)</span><br><span class="line">      <span class="keyword">if</span> oldNetwork.ResourceVersion == newNetwork.ResourceVersion &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">      controller.enqueueNetwork(<span class="built_in">new</span>)</span><br><span class="line">    &#125;,</span><br><span class="line">    DeleteFunc: controller.enqueueNetworkForDelete,</span><br><span class="line"> <span class="keyword">return</span> controller</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="编写控制器里的业务逻辑"><a href="#编写控制器里的业务逻辑" class="headerlink" title="编写控制器里的业务逻辑"></a>编写控制器里的业务逻辑</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">Run</span><span class="params">(threadiness <span class="keyword">int</span>, stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> ...</span><br><span class="line">  <span class="keyword">if</span> ok := cache.WaitForCacheSync(stopCh, c.networksSynced); !ok &#123;</span><br><span class="line">    <span class="keyword">return</span> fmt.Errorf(<span class="string">"failed to wait for caches to sync"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; threadiness; i++ &#123;</span><br><span class="line">    <span class="keyword">go</span> wait.Until(c.runWorker, time.Second, stopCh)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">runWorker</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">for</span> c.processNextWorkItem() &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">processNextWorkItem</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">  obj, shutdown := c.workqueue.Get()</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  err := <span class="function"><span class="keyword">func</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> err := c.syncHandler(key); err != <span class="literal">nil</span> &#123;</span><br><span class="line">     <span class="keyword">return</span> fmt.Errorf(<span class="string">"error syncing '%s': %s"</span>, key, err.Error())</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    c.workqueue.Forget(obj)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">  &#125;(obj)</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">syncHandler</span><span class="params">(key <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"></span><br><span class="line">  namespace, name, err := cache.SplitMetaNamespaceKey(key)</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  network, err := c.networksLister.Networks(namespace).Get(name)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> errors.IsNotFound(err) &#123;</span><br><span class="line">      glog.Warningf(<span class="string">"Network does not exist in local cache: %s/%s, will delete it from Neutron ..."</span>,</span><br><span class="line">      namespace, name)</span><br><span class="line">      </span><br><span class="line">      glog.Warningf(<span class="string">"Network: %s/%s does not exist in local cache, will delete it from Neutron ..."</span>,</span><br><span class="line">    namespace, name)</span><br><span class="line">    </span><br><span class="line">     <span class="comment">// FIX ME: call Neutron API to delete this network by name.</span></span><br><span class="line">     <span class="comment">//</span></span><br><span class="line">     <span class="comment">// neutron.Delete(namespace, name)</span></span><br><span class="line">     </span><br><span class="line">     <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  glog.Infof(<span class="string">"[Neutron] Try to process network: %#v ..."</span>, network)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// FIX ME: Do diff().</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// actualNetwork, exists := neutron.Get(namespace, name)</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// if !exists &#123;</span></span><br><span class="line">  <span class="comment">//   neutron.Create(namespace, name)</span></span><br><span class="line">  <span class="comment">// &#125; else if !reflect.DeepEqual(actualNetwork, network) &#123;</span></span><br><span class="line">  <span class="comment">//   neutron.Update(namespace, name)</span></span><br><span class="line">  <span class="comment">// &#125;</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="执行控制器"><a href="#执行控制器" class="headerlink" title="执行控制器"></a>执行控制器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ cd $GOPATH/src/github.com/shiyangtao/k8s-controller-custom-resource</span><br><span class="line"></span><br><span class="line">### Skip this part if you don&apos;t want to build</span><br><span class="line"># Install dependency</span><br><span class="line">$ go get github.com/tools/godep</span><br><span class="line">$ godep restore</span><br><span class="line"># Build</span><br><span class="line">$ go build -o samplecrd-controller .</span><br><span class="line"></span><br><span class="line">$ ./samplecrd-controller -kubeconfig=$HOME/.kube/config -alsologtostderr=true</span><br><span class="line">I0915 12:50:29.051349   27159 controller.go:84] Setting up event handlers</span><br><span class="line">I0915 12:50:29.051615   27159 controller.go:113] Starting Network control loop</span><br><span class="line">I0915 12:50:29.051630   27159 controller.go:116] Waiting for informer caches to sync</span><br><span class="line">E0915 12:50:29.066745   27159 reflector.go:134] github.com/resouer/k8s-controller-custom-resource/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接下来可以打开另外一个命令行进行以下操作,观察下以上控制台的变化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">修改网段</span><br><span class="line">example/example-network.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f example/example-network.yaml </span><br><span class="line">network.samplecrd.k8s.io/example-network configured</span><br><span class="line"></span><br><span class="line">删除</span><br><span class="line"></span><br><span class="line">$ kubectl delete -f example/example-network.yaml</span><br></pre></td></tr></table></figure><p>所谓的 Informer，就是一个自带缓存和索引机制，可以触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。</p><p>Informer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。Reflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。</p><p>而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同。</p><p>在实际应用中，除了控制循环之外的所有代码，实际上都是 Kubernetes 为你自动生成的，即：pkg/client/{informers, listers, clientset}里的内容。</p><h2 id="RBAC"><a href="#RBAC" class="headerlink" title="RBAC"></a>RBAC</h2><h3 id="Role"><a href="#Role" class="headerlink" title="Role"></a>Role</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">  name: example-role</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><p>类似的，Role 对象的 rules 字段也可以进一步细化。比如，你可以只针对某一个具体的对象进行权限设置，如下所示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;configmaps&quot;]</span><br><span class="line">  resourceNames: [&quot;my-config&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br></pre></td></tr></table></figure><p>这个例子就表示，这条规则的“被作用者”，只对名叫“my-config”的 ConfigMap 对象，有进行 GET 操作的权限。</p><h3 id="RoleBinding"><a href="#RoleBinding" class="headerlink" title="RoleBinding"></a>RoleBinding</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-rolebinding</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: example-user</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: example-role</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>需要再次提醒的是，Role 和 RoleBinding 对象都是 Namespaced 对象（NamespacedObject），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。</p><h3 id="ClusterRole"><a href="#ClusterRole" class="headerlink" title="ClusterRole"></a>ClusterRole</h3><p>对于非 Namespaced（Non-namespaced）对象（比如：Node），或者，某一个 Role 想要作用于所有的 Namespace 的时候，我们又该如何去做授权呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-clusterrole</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><p>查看k8s内置clusterRole</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get clusterrole</span><br></pre></td></tr></table></figure><h3 id="ClusterRoleBinding"><a href="#ClusterRoleBinding" class="headerlink" title="ClusterRoleBinding"></a>ClusterRoleBinding</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-clusterrolebinding</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: example-user</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: example-clusterrole</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><h3 id="User"><a href="#User" class="headerlink" title="User"></a>User</h3><p>可以看到，这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 User，即 Kubernetes 里的用户。这个用户的名字是 example-user。</p><p>可是，在 Kubernetes 中，其实并没有一个叫作“User”的 API 对象。而且，我们在前面和部署使用 Kubernetes 的流程里，既不需要 User，也没有创建过 User。</p><p>这个 User 到底是从哪里来的呢？</p><p>实际上，Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。或者，你也可以直接给 APIServer 指定一个用户名、密码文件。那么 Kubernetes 的授权系统，就能够从这个文件里找到对应的“用户”了。当然，在大多数私有的使用环境中，我们只要使用 Kubernetes 提供的内置“用户”，就足够了。</p><p>在大多数时候，我们其实都不太使用“用户”这个功能，而是直接使用 Kubernetes 里的“内置用户”。</p><p><strong><em>ServiceAccount其实就是内置用户</em></strong></p><p>定义一个ServiceAccount</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">  name: example-sa</span><br></pre></td></tr></table></figure><p>然后，我们通过编写 RoleBinding 的 YAML 文件，来为这个 ServiceAccount 分配权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-rolebinding</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: example-sa</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: example-role</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f svc-account.yaml</span><br><span class="line">$ kubectl create -f role-binding.yaml</span><br><span class="line">$ kubectl create -f role.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get sa -n mynamespace -o yaml</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  kind: ServiceAccount</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2018-09-08T12:59:17Z</span><br><span class="line">    name: example-sa</span><br><span class="line">    namespace: mynamespace</span><br><span class="line">    resourceVersion: &quot;409327&quot;</span><br><span class="line">    ...</span><br><span class="line">  secrets:</span><br><span class="line">  - name: example-sa-token-vmfg6</span><br></pre></td></tr></table></figure><p>可以看到，Kubernetes 会为一个 ServiceAccount 自动创建并分配一个 Secret 对象，即：上述 ServiceAcount 定义里最下面的 secrets 字段。这个 Secret，就是这个 ServiceAccount 对应的、用来跟 APIServer 进行交互的授权文件，我们一般称它为：Token。Token 文件的内容一般是证书或者密码，它以一个 Secret 对象的方式保存在 Etcd 当中。</p><p>这时候，用户的 Pod，就可以声明使用这个 ServiceAccount 了，比如下面这个例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">  name: sa-token-test</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx:1.7.9</span><br><span class="line">  serviceAccountName: example-sa</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f pod.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl describe pod sa-token-test -n mynamespace</span><br><span class="line">Name:               sa-token-test</span><br><span class="line">Namespace:          mynamespace</span><br><span class="line">...</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    ...</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from example-sa-token-vmfg6 (ro)</span><br></pre></td></tr></table></figure><p>等这个 Pod 运行起来之后，我们就可以看到，该 ServiceAccount 的 token，也就是一个 Secret 对象，被 Kubernetes 自动挂载到了容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录下.</p><p>这时候，我们可以通过 kubectl exec 查看到这个目录里的文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl exec -it sa-token-test -n mynamespace -- /bin/bash</span><br><span class="line">root@sa-token-test:/# ls /var/run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">ca.crt namespace  token</span><br></pre></td></tr></table></figure><h4 id="defaultServiceAccount"><a href="#defaultServiceAccount" class="headerlink" title="defaultServiceAccount"></a>defaultServiceAccount</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$kubectl describe sa default</span><br><span class="line">Name:                default</span><br><span class="line">Namespace:           default</span><br><span class="line">Labels:              &lt;none&gt;</span><br><span class="line">Annotations:         &lt;none&gt;</span><br><span class="line">Image pull secrets:  &lt;none&gt;</span><br><span class="line">Mountable secrets:   default-token-s8rbq</span><br><span class="line">Tokens:              default-token-s8rbq</span><br><span class="line">Events:              &lt;none&gt;</span><br><span class="line"></span><br><span class="line">$ kubectl get secret</span><br><span class="line">NAME                  TYPE                                  DATA      AGE</span><br><span class="line">kubernetes.io/service-account-token   3         82d</span><br><span class="line"></span><br><span class="line">$ kubectl describe secret default-token-s8rbq</span><br><span class="line">Name:         default-token-s8rbq</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name=default</span><br><span class="line">              kubernetes.io/service-account.uid=ffcb12b2-917f-11e8-abde-42010aa80002</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1025 bytes</span><br><span class="line">namespace:  7 bytes</span><br><span class="line">token:      &lt;TOKEN数据&gt;</span><br></pre></td></tr></table></figure><p>可以看到，Kubernetes 会自动为默认 ServiceAccount 创建并绑定一个特殊的 Secret：它的类型是kubernetes.io/service-account-token；它的 Annotation 字段，声明了kubernetes.io/service-account.name=default，即这个 Secret 会跟同一 Namespace 下名叫 default 的 ServiceAccount 进行绑定。</p><h3 id="Group"><a href="#Group" class="headerlink" title="Group"></a>Group</h3><p>如果你为 Kubernetes 配置了外部认证服务的话，这个“用户组”的概念就会由外部认证服务提供。</p><p>而对于 Kubernetes 的内置“用户”ServiceAccount 来说，上述“用户组”的概念也同样适用。</p><p>实际上，一个 ServiceAccount，在 Kubernetes 里对应的“用户”的名字是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">system:serviceaccount:&lt;Namespace名字&gt;:&lt;ServiceAccount名字&gt;</span><br></pre></td></tr></table></figure><p>而它对应的内置“用户组”的名字，就是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">system:serviceaccounts:&lt;Namespace名字&gt;</span><br></pre></td></tr></table></figure><p>比如，现在我们可以在 RoleBinding 里定义如下的 subjects：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: system:serviceaccounts:mynamespace</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>这就意味着这个 Role 的权限规则，作用于 mynamespace 里的所有 ServiceAccount。这就用到了“用户组”的概念。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: system:serviceaccounts</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>就意味着这个 Role 的权限规则，作用于整个系统里的所有 ServiceAccount。</p><h4 id="所有Namespace-下的默认-ServiceAccount，绑定一个只读权限的-Role。"><a href="#所有Namespace-下的默认-ServiceAccount，绑定一个只读权限的-Role。" class="headerlink" title="所有Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。"></a>所有Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">name: readonly-all-default</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">name: system:serviceaccount:default</span><br><span class="line">roleRef:</span><br><span class="line">kind: ClusterRole</span><br><span class="line">name: view</span><br><span class="line">apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>最后，值得一提的是，在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。</p><p>一般来说，这些系统 ClusterRole，是绑定给 Kubernetes 系统组件对应的 ServiceAccount 使用的。比如，其中一个名叫 system:kube-scheduler 的 ClusterRole，定义的权限规则是 kube-scheduler（Kubernetes 的调度器组件）运行所需要的必要权限。你可以通过如下指令查看这些权限的列表：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe clusterrole system:kube-scheduler</span><br><span class="line">Name:         system:kube-scheduler</span><br><span class="line">...</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                    Non-Resource URLs Resource Names    Verbs</span><br><span class="line">  ---------                    -----------------  --------------    -----</span><br><span class="line">...</span><br><span class="line">  services                     []                 []                [get list watch]</span><br><span class="line">  replicasets.apps             []                 []                [get list watch]</span><br><span class="line">  statefulsets.apps            []                 []                [get list watch]</span><br><span class="line">  replicasets.extensions       []                 []                [get list watch]</span><br><span class="line">  poddisruptionbudgets.policy  []                 []                [get list watch]</span><br><span class="line">  pods/status                  []                 []                [patch update]</span><br></pre></td></tr></table></figure><p>这个 system:kube-scheduler 的 ClusterRole，就会被绑定给 kube-system Namesapce 下名叫 kube-scheduler 的 ServiceAccount，它正是 Kubernetes 调度器的 Pod 声明使用的 ServiceAccount。</p><p>除此之外，Kubernetes 还提供了四个预先定义好的 ClusterRole 来供用户直接使用：</p><ul><li>cluster-admin；</li><li>admin；</li><li>edit；</li><li>view。</li></ul><p>通过它们的名字，你应该能大致猜出它们都定义了哪些权限。比如，这个名叫 view 的 ClusterRole，就规定了被作用者只有 Kubernetes API 的只读权限。而我还要提醒你的是，上面这个 cluster-admin 角色，对应的是整个 Kubernetes 项目中的最高权限（verbs=*），如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe clusterrole cluster-admin -n kube-system</span><br><span class="line">Name:         cluster-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=true</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources  Non-Resource URLs Resource Names  Verbs</span><br><span class="line">  ---------  -----------------  --------------  -----</span><br><span class="line">  *.*        []                 []              [*]</span><br><span class="line">             [*]                []              [*]</span><br></pre></td></tr></table></figure><h2 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h2><p>可能你已经感觉到，在 Kubernetes 中，管理“有状态应用”是一个比较复杂的过程，尤其是编写 Pod 模板的时候，总有一种“在 YAML 文件里编程序”的感觉，让人很不舒服。</p><p>而在 Kubernetes 生态中，还有一个相对更加灵活和编程友好的管理“有状态应用”的解决方案，它就是：Operator。</p><p>接下来，我就以 Etcd Operator 为例，来为你讲解一下 Operator 的工作原理和编写方法。Operator的原理就是自定义控制器。</p><h3 id="构建步骤"><a href="#构建步骤" class="headerlink" title="构建步骤"></a>构建步骤</h3><h4 id="第一步，将这个-Operator-的代码-Clone-到本地："><a href="#第一步，将这个-Operator-的代码-Clone-到本地：" class="headerlink" title="第一步，将这个 Operator 的代码 Clone 到本地："></a>第一步，将这个 Operator 的代码 Clone 到本地：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/coreos/etcd-operator</span><br></pre></td></tr></table></figure><h4 id="第二步，将这个-Etcd-Operator-部署在-Kubernetes-集群里。"><a href="#第二步，将这个-Etcd-Operator-部署在-Kubernetes-集群里。" class="headerlink" title="第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。"></a>第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ example/rbac/create_role.sh</span><br></pre></td></tr></table></figure><p>这个脚本的作用，就是为 Etcd Operator 创建 RBAC 规则。这是因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象。</p><h4 id="第三步，创建Etcd-Operator"><a href="#第三步，创建Etcd-Operator" class="headerlink" title="第三步，创建Etcd Operator"></a>第三步，创建Etcd Operator</h4><p>而 Etcd Operator 本身，其实就是一个 Deployment，它的 YAML 文件如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1 高版本改成 apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: etcd-operator</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: etcd-operator</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: etcd-operator</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: etcd-operator</span><br><span class="line">        image: quay.io/coreos/etcd-operator:v0.9.2</span><br><span class="line">        command:</span><br><span class="line">        - etcd-operator</span><br><span class="line">        env:</span><br><span class="line">        - name: MY_POD_NAMESPACE</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line">        - name: MY_POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f example/deployment.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                              READY     STATUS      RESTARTS   AGE</span><br><span class="line">etcd-operator-649dbdb5cb-bzfzp    1/1       Running     0          20s</span><br><span class="line"></span><br><span class="line">$ kubectl get crd</span><br><span class="line">NAME                                    CREATED AT</span><br><span class="line">etcdclusters.etcd.database.coreos.com   2018-09-18T11:42:55Z</span><br></pre></td></tr></table></figure><p>这个 CRD 名叫etcdclusters.etcd.database.coreos.com 。你可以通过 kubectl describe 命令看到它的细节，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe crd  etcdclusters.etcd.database.coreos.com</span><br><span class="line">...</span><br><span class="line">Group:   etcd.database.coreos.com</span><br><span class="line">  Names:</span><br><span class="line">    Kind:       EtcdCluster</span><br><span class="line">    List Kind:  EtcdClusterList</span><br><span class="line">    Plural:     etcdclusters</span><br><span class="line">    Short Names:</span><br><span class="line">      etcd</span><br><span class="line">    Singular:  etcdcluster</span><br><span class="line">  Scope:       Namespaced</span><br><span class="line">  Version:     v1beta2</span><br><span class="line">  </span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>所以说，通过上述两步操作，你实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型。而 Etcd Operator 本身，就是这个自定义资源类型对应的自定义控制器。</p><h4 id="第四步，创建Etcd-集群"><a href="#第四步，创建Etcd-集群" class="headerlink" title="第四步，创建Etcd 集群"></a>第四步，创建Etcd 集群</h4><p>不难想到，这个文件里定义的，正是 EtcdCluster 这个 CRD 的一个具体实例，也就是一个 Custom Resource（CR）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: &quot;etcd.database.coreos.com/v1beta2&quot;</span><br><span class="line">kind: &quot;EtcdCluster&quot;</span><br><span class="line">metadata:</span><br><span class="line">  name: &quot;example-etcd-cluster&quot;</span><br><span class="line">spec:</span><br><span class="line">  size: 3</span><br><span class="line">  version: &quot;3.2.13&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f example/example-etcd-cluster.yaml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                            READY     STATUS    RESTARTS   AGE</span><br><span class="line">example-etcd-cluster-dp8nqtjznc   1/1       Running     0          1m</span><br><span class="line">example-etcd-cluster-mbzlg6sd56   1/1       Running     0          2m</span><br><span class="line">example-etcd-cluster-v6v6s6stxd   1/1       Running     0          2m</span><br></pre></td></tr></table></figure><p>而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作了。看到这里，相信你应该已经对 Operator 有了一个初步的认知：</p><p><strong>Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。</strong></p><h3 id="Etcd-Operator原理分析"><a href="#Etcd-Operator原理分析" class="headerlink" title="Etcd Operator原理分析"></a>Etcd Operator原理分析</h3><h4 id="Etcd静态集群构建"><a href="#Etcd静态集群构建" class="headerlink" title="Etcd静态集群构建"></a>Etcd静态集群构建</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \</span><br><span class="line">  --listen-peer-urls http://10.0.1.10:2380 \</span><br><span class="line">...</span><br><span class="line">  --initial-cluster-token etcd-cluster-1 \</span><br><span class="line">  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \</span><br><span class="line">  --initial-cluster-state new</span><br><span class="line">  </span><br><span class="line">$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \</span><br><span class="line">  --listen-peer-urls http://10.0.1.11:2380 \</span><br><span class="line">...</span><br><span class="line">  --initial-cluster-token etcd-cluster-1 \</span><br><span class="line">  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \</span><br><span class="line">  --initial-cluster-state new</span><br><span class="line">  </span><br><span class="line">$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \</span><br><span class="line">  --listen-peer-urls http://10.0.1.12:2380 \</span><br><span class="line">...</span><br><span class="line">  --initial-cluster-token etcd-cluster-1 \</span><br><span class="line">  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \</span><br><span class="line">  --initial-cluster-state new</span><br></pre></td></tr></table></figure><p>其中，这些节点启动参数里的–initial-cluster 参数，非常值得你关注。它的含义，正是当前节点启动时集群的拓扑结构。说得更详细一点，就是当前这个节点启动时，需要跟哪些节点通信来组成集群。</p><p>此外，一个 Etcd 集群还需要用–initial-cluster-token 字段，来声明一个该集群独一无二的 Token 名字。</p><p>像上述这样为每一个 Ectd 节点配置好它对应的启动参数之后把它们启动起来，一个 Etcd 集群就可以自动组建起来了。</p><h4 id="EtcdCluster-CRD"><a href="#EtcdCluster-CRD" class="headerlink" title="EtcdCluster CRD"></a>EtcdCluster CRD</h4><p>types.go的定义</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// +genclient</span><br><span class="line">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span><br><span class="line"></span><br><span class="line">type EtcdCluster struct &#123;</span><br><span class="line">  metav1.TypeMeta   `json:&quot;,inline&quot;`</span><br><span class="line">  metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">  Spec              ClusterSpec   `json:&quot;spec&quot;`</span><br><span class="line">  Status            ClusterStatus `json:&quot;status&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type ClusterSpec struct &#123;</span><br><span class="line"> // Size is the expected size of the etcd cluster.</span><br><span class="line"> // The etcd-operator will eventually make the size of the running</span><br><span class="line"> // cluster equal to the expected size.</span><br><span class="line"> // The vaild range of the size is from 1 to 7.</span><br><span class="line"> Size int `json:&quot;size&quot;`</span><br><span class="line"> ... </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，EtcdCluster 是一个有 Status 字段的 CRD。在这里，我们可以不必关心 ClusterSpec 里的其他字段，只关注 Size（即：Etcd 集群的大小）字段即可。Size 字段的存在，就意味着将来如果我们想要调整集群大小的话，应该直接修改 YAML 文件里 size 的值，并执行 kubectl apply -f。</p><p><strong>这样，Operator 就会帮我们完成 Etcd 节点的增删操作。这种“scale”能力，也是 Etcd Operator 自动化运维 Etcd 集群需要实现的主要功能。</strong></p><p>而为了能够支持这个功能，我们就不再像前面那样在–initial-cluster 参数里把拓扑结构固定死</p><h4 id="Etcd-Operator-scale原理"><a href="#Etcd-Operator-scale原理" class="headerlink" title="Etcd Operator scale原理"></a>Etcd Operator scale原理</h4><p>首先，Etcd Operator 会创建一个“种子节点”；</p><p>然后，Etcd Operator 会不断创建新的 Etcd 节点，然后将它们逐一加入到这个集群当中，直到集群的节点数等于 size。</p><p>而这两种节点的不同之处，就在于一个名叫–initial-cluster-state 的启动参数：</p><ul><li><p>当这个参数值设为 new 时，就代表了该节点是种子节点。而我们前面提到过，种子节点还必须通过–initial-cluster-token 声明一个独一无二的 Token。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> etcd</span><br><span class="line">  --data-dir=/var/etcd/data</span><br><span class="line">  --name=infra0</span><br><span class="line">  --initial-advertise-peer-urls=http://10.0.1.10:2380</span><br><span class="line">  --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">  --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">  --advertise-client-urls=http://10.0.1.10:2379</span><br><span class="line">  --initial-cluster=infra0=http://10.0.1.10:2380</span><br><span class="line">  --initial-cluster-state=new</span><br><span class="line">  --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>而如果这个参数值设为 existing，那就是说明这个节点是一个普通节点，Etcd Operator 需要把它加入到已有集群里。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 第一步：通过 Etcd 命令行添加一个新成员：</span><br><span class="line"><span class="meta">$</span> etcdctl member add infra1 http://10.0.1.11:2380</span><br><span class="line"><span class="meta">#</span> 第二步：为这个成员节点生成对应的启动参数，并启动它：</span><br><span class="line"><span class="meta">$</span> etcd</span><br><span class="line">    --data-dir=/var/etcd/data</span><br><span class="line">    --name=infra1</span><br><span class="line">    --initial-advertise-peer-urls=http://10.0.1.11:2380</span><br><span class="line">    --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">    --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">    --advertise-client-urls=http://10.0.1.11:2379</span><br><span class="line">    --initial-cluster=infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380</span><br><span class="line">    --initial-cluster-state=existing</span><br></pre></td></tr></table></figure></li></ul><h4 id="Etcd-Operator-Informer"><a href="#Etcd-Operator-Informer" class="headerlink" title="Etcd Operator Informer"></a>Etcd Operator Informer</h4><p>跟所有的自定义控制器一样，Etcd Operator 的启动流程也是围绕着 Informer 展开的，如下所示：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">Start</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> <span class="keyword">for</span> &#123;</span><br><span class="line">  err := c.initResource()</span><br><span class="line">  ...</span><br><span class="line">  time.Sleep(initRetryWaitTime)</span><br><span class="line"> &#125;</span><br><span class="line"> c.run()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">run</span><span class="params">()</span></span> &#123;</span><br><span class="line"> ...</span><br><span class="line"> </span><br><span class="line"> _, informer := cache.NewIndexerInformer(source, &amp;api.EtcdCluster&#123;&#125;, <span class="number">0</span>, cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">  AddFunc:    c.onAddEtcdClus,</span><br><span class="line">  UpdateFunc: c.onUpdateEtcdClus,</span><br><span class="line">  DeleteFunc: c.onDeleteEtcdClus,</span><br><span class="line"> &#125;, cache.Indexers&#123;&#125;)</span><br><span class="line"> </span><br><span class="line"> ctx := context.TODO()</span><br><span class="line"> <span class="comment">// <span class="doctag">TODO:</span> use workqueue to avoid blocking</span></span><br><span class="line"> informer.Run(ctx.Done())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，Etcd Operator 启动要做的第一件事（ c.initResource），是创建 EtcdCluster 对象所需要的 CRD，即：前面提到的etcdclusters.etcd.database.coreos.com。这样 Kubernetes 就能够“认识”EtcdCluster 这个自定义 API 资源了。</p><p>而接下来，Etcd Operator 会定义一个 EtcdCluster 对象的 Informer。不过，需要注意的是，由于 Etcd Operator 的完成时间相对较早，所以它里面有些代码的编写方式会跟我们之前讲解的最新的编写方式不太一样。在具体实践的时候，你还是应该以我讲解的模板为主。</p><p>比如，在上面的代码最后，你会看到有这样一句注释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// TODO: use workqueue to avoid blocking</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>具体来讲，我们在控制循环里执行的业务逻辑，往往是比较耗时间的。比如，创建一个真实的 Etcd 集群。而 Informer 的 WATCH 机制对 API 对象变化的响应，则非常迅速。所以，控制器里的业务逻辑就很可能会拖慢 Informer 的执行周期，甚至可能 Block 它。而要协调这样两个快、慢任务的一个典型解决方法，就是引入一个工作队列。</p><p>由于 Etcd Operator 里没有工作队列，那么在它的 EventHandler 部分，就不会有什么入队操作，而直接就是每种事件对应的具体的业务逻辑了。</p><p><img src="/2020/02/21/k8s-command/images/image-20200229152908546.png" alt="image-20200229152908546"></p><p>可以看到，Etcd Operator 的特殊之处在于，它为每一个 EtcdCluster 对象，都启动了一个控制循环，“并发”地响应这些对象的变化。显然，这种做法不仅可以简化 Etcd Operator 的代码实现，还有助于提高它的响应速度。</p><h4 id="启动Bootstrap"><a href="#启动Bootstrap" class="headerlink" title="启动Bootstrap"></a>启动Bootstrap</h4><p>其中，第一个工作只在该 Cluster 对象第一次被创建的时候才会执行。这个工作，就是我们前面提到过的 Bootstrap，即：创建一个单节点的种子集群。</p><p>由于种子集群只有一个节点，所以这一步直接就会生成一个 Etcd 的 Pod 对象。这个 Pod 里有一个 InitContainer，负责检查 Pod 的 DNS 记录是否正常。如果检查通过，用户容器也就是 Etcd 容器就会启动起来。</p><p>启动命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/usr/local/bin/etcd</span><br><span class="line">  --data-dir=/var/etcd/data</span><br><span class="line">  --name=example-etcd-cluster-mbzlg6sd56</span><br><span class="line">  --initial-advertise-peer-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">  --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">  --advertise-client-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2379</span><br><span class="line">  --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --initial-cluster-state=new</span><br><span class="line">  --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8</span><br></pre></td></tr></table></figure><p>可以看到，在这些启动参数（比如：initial-cluster）里，Etcd Operator 只会使用 Pod 的 DNS 记录，而不是它的 IP 地址。</p><p>这当然是因为，在 Operator 生成上述启动命令的时候，Etcd 的 Pod 还没有被创建出来，它的 IP 地址自然也无从谈起。</p><p>这也就意味着，每个 Cluster 对象，都会事先创建一个与该 EtcdCluster 同名的 Headless Service。这样，Etcd Operator 在接下来的所有创建 Pod 的步骤里，就都可以使用 Pod 的 DNS 记录来代替它的 IP 地址了。</p><h4 id="启动该集群所对应的控制循环"><a href="#启动该集群所对应的控制循环" class="headerlink" title="启动该集群所对应的控制循环"></a>启动该集群所对应的控制循环</h4><p>Cluster 对象的第二个工作，则是启动该集群所对应的控制循环。</p><p>这个控制循环每隔一定时间，就会执行一次下面的 Diff 流程。</p><p>首先，控制循环要获取到所有正在运行的、属于这个 Cluster 的 Pod 数量，也就是该 Etcd 集群的“实际状态”。</p><p>而这个 Etcd 集群的“期望状态”，正是用户在 EtcdCluster 对象里定义的 size。</p><p>如果实际的 Pod 数量不够，那么控制循环就会执行一个添加成员节点的操作（即：上述流程图中的 addOneMember 方法）；反之，就执行删除成员节点的操作（即：上述流程图中的 removeOneMember 方法）</p><p>以 addOneMember 方法为例，它执行的流程如下所示：</p><ol><li>生成一个新节点的 Pod 的名字，比如：example-etcd-cluster-v6v6s6stxd；</li><li>调用 Etcd Client，执行前面提到过的 etcdctl member add example-etcd-cluster-v6v6s6stxd 命令；</li><li>使用这个 Pod 名字，和已经存在的所有节点列表，组合成一个新的 initial-cluster 字段的值；</li><li>使用这个 initial-cluster 的值，生成这个 Pod 里 Etcd 容器的启动命令。如下所示：</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/usr/local/bin/etcd</span><br><span class="line">  --data-dir=/var/etcd/data</span><br><span class="line">  --name=example-etcd-cluster-v6v6s6stxd</span><br><span class="line">  --initial-advertise-peer-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">  --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">  --advertise-client-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2379</span><br><span class="line">  --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380,example-etcd-cluster-v6v6s6stxd=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --initial-cluster-state=existing</span><br></pre></td></tr></table></figure><h3 id="对比StatefulSet-可能有的两个疑问"><a href="#对比StatefulSet-可能有的两个疑问" class="headerlink" title="对比StatefulSet  可能有的两个疑问"></a>对比StatefulSet  可能有的两个疑问</h3><h4 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h4><p>在 StatefulSet 里，它为 Pod 创建的名字是带编号的，这样就把整个集群的拓扑状态固定了下来（比如：一个三节点的集群一定是由名叫 web-0、web-1 和 web-2 的三个 Pod 组成）。可是，在 Etcd Operator 里，为什么我们使用随机名字就可以了呢？</p><p>答：这是因为，Etcd Operator 在每次添加 Etcd 节点的时候，都会先执行 etcdctl member add ；每次删除节点的时候，则会执行 etcdctl member remove 。这些操作，其实就会更新 Etcd 内部维护的拓扑信息，所以 Etcd Operator 无需在集群外部通过编号来固定这个拓扑关系。</p><h4 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h4><p>为什么我没有在 EtcdCluster 对象里声明 Persistent Volume？难道，我们不担心节点宕机之后 Etcd 的数据会丢失吗？</p><p>答：在有了 Operator 机制之后，上述 Etcd 的备份操作，是由一个单独的 Etcd Backup Operator 负责完成的。</p><p>创建和使用这个 Operator 的流程，如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span> 首先，创建etcd-backup-operator</span><br><span class="line"><span class="meta">$</span> kubectl create -f example/etcd-backup-operator/deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 确认etcd-backup-operator已经在正常运行</span><br><span class="line"><span class="meta">$</span> kubectl get pod</span><br><span class="line">NAME                                    READY     STATUS    RESTARTS   AGE</span><br><span class="line">etcd-backup-operator-1102130733-hhgt7   1/1       Running   0          3s</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 可以看到，Backup Operator会创建一个叫etcdbackups的CRD</span><br><span class="line"><span class="meta">$</span> kubectl get crd</span><br><span class="line">NAME                                    KIND</span><br><span class="line">etcdbackups.etcd.database.coreos.com    CustomResourceDefinition.v1beta1.apiextensions.k8s.io</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 我们这里要使用AWS S3来存储备份，需要将S3的授权信息配置在文件里</span><br><span class="line"><span class="meta">$</span> cat $AWS_DIR/credentials</span><br><span class="line">[default]</span><br><span class="line">aws_access_key_id = XXX</span><br><span class="line">aws_secret_access_key = XXX</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> cat $AWS_DIR/config</span><br><span class="line">[default]</span><br><span class="line">region = &lt;region&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 然后，将上述授权信息制作成一个Secret</span><br><span class="line"><span class="meta">$</span> kubectl create secret generic aws --from-file=$AWS_DIR/credentials --from-file=$AWS_DIR/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 使用上述S3的访问信息，创建一个EtcdBackup对象</span><br><span class="line"><span class="meta">$</span> sed -e 's|&lt;full-s3-path&gt;|mybucket/etcd.backup|g' \</span><br><span class="line">    -e 's|&lt;aws-secret&gt;|aws|g' \</span><br><span class="line">    -e 's|&lt;etcd-cluster-endpoints&gt;|"http://example-etcd-cluster-client:2379"|g' \</span><br><span class="line">    example/etcd-backup-operator/backup_cr.yaml \</span><br><span class="line">    | kubectl create -f -</span><br></pre></td></tr></table></figure><p>需要注意的是，每当你创建一个 EtcdBackup 对象（backup_cr.yaml），就相当于为它所指定的 Etcd 集群做了一次备份。EtcdBackup 对象的 etcdEndpoints 字段，会指定它要备份的 Etcd 集群的访问地址。所以，在实际的环境里，我建议你把最后这个备份操作，编写成一个 Kubernetes 的 CronJob 以便定时运行。</p><p>而当 Etcd 集群发生了故障之后，你就可以通过创建一个 EtcdRestore 对象来完成恢复操作。当然，这就意味着你也需要事先启动 Etcd Restore Operator。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建etcd-restore-operator</span><br><span class="line"><span class="meta">$</span> kubectl create -f example/etcd-restore-operator/deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 确认它已经正常运行</span><br><span class="line"><span class="meta">$</span> kubectl get pods</span><br><span class="line">NAME                                     READY     STATUS    RESTARTS   AGE</span><br><span class="line">etcd-restore-operator-4203122180-npn3g   1/1       Running   0          7s</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建一个EtcdRestore对象，来帮助Etcd Operator恢复数据，记得替换模板里的S3的访问信息</span><br><span class="line"><span class="meta">$</span> sed -e 's|&lt;full-s3-path&gt;|mybucket/etcd.backup|g' \</span><br><span class="line">    -e 's|&lt;aws-secret&gt;|aws|g' \</span><br><span class="line">    example/etcd-restore-operator/restore_cr.yaml \</span><br><span class="line">    | kubectl create -f -</span><br></pre></td></tr></table></figure><p>上面例子里的 EtcdRestore 对象（restore_cr.yaml），会指定它要恢复的 Etcd 集群的名字和备份数据所在的 S3 存储的访问信息。而当一个 EtcdRestore 对象成功创建后，Etcd Restore Operator 就会通过上述信息，恢复出一个全新的 Etcd 集群。然后，Etcd Operator 会把这个新集群直接接管过来，从而重新进入可用的状态。EtcdBackup 和 EtcdRestore 这两个 Operator 的工作原理，与 Etcd Operator 的实现方式非常类似。</p><h2 id="再谈PVC、PV、StorageClass"><a href="#再谈PVC、PV、StorageClass" class="headerlink" title="再谈PVC、PV、StorageClass"></a>再谈PVC、PV、StorageClass</h2><h3 id="PV-1"><a href="#PV-1" class="headerlink" title="PV"></a>PV</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 10.244.1.4</span><br><span class="line">    path: &quot;/&quot;</span><br></pre></td></tr></table></figure><h3 id="PVC-1"><a href="#PVC-1" class="headerlink" title="PVC"></a>PVC</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure><h3 id="在Pod中使用PVC"><a href="#在Pod中使用PVC" class="headerlink" title="在Pod中使用PVC"></a>在Pod中使用PVC</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    role: web-frontend</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: web</span><br><span class="line">    image: nginx</span><br><span class="line">    ports:</span><br><span class="line">      - name: web</span><br><span class="line">        containerPort: 80</span><br><span class="line">    volumeMounts:</span><br><span class="line">        - name: nfs</span><br><span class="line">          mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">  volumes:</span><br><span class="line">  - name: nfs</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: nfs</span><br></pre></td></tr></table></figure><p>可以看到，Pod 需要做的，就是在 volumes 字段里声明自己要使用的 PVC 名字。接下来，等这个 Pod 创建之后，kubelet 就会把这个 PVC 所对应的 PV，也就是一个 NFS 类型的 Volume，挂载在这个 Pod 容器内的目录上。</p><h3 id="Volume-Controller"><a href="#Volume-Controller" class="headerlink" title="Volume Controller"></a>Volume Controller</h3><p>在 Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器,这个 Volume Controller 维护着多个控制循环。</p><h4 id="PersistentVolumeController"><a href="#PersistentVolumeController" class="headerlink" title="PersistentVolumeController"></a>PersistentVolumeController</h4><p>其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。它的名字叫作 PersistentVolumeController。</p><p>PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态，从而结束“单身”之旅。</p><p>而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。</p><h4 id="AttachDetachController"><a href="#AttachDetachController" class="headerlink" title="AttachDetachController"></a>AttachDetachController</h4><p>其中，“第一阶段”的 Attach（以及 Dettach）操作，是由 Volume Controller 负责维护的，这个控制循环的名字叫作：AttachDetachController。而它的作用，就是不断地检查每一个 Pod 对应的 PV，和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作。</p><p>需要注意，作为一个 Kubernetes 内置的控制器，Volume Controller 自然是 kube-controller-manager 的一部分。所以，AttachDetachController 也一定是运行在 Master 节点上的。当然，Attach 操作只需要调用公有云或者具体存储项目的 API，并不需要在具体的宿主机上执行操作，所以这个设计没有任何问题。</p><h3 id="VolumeManagerReconciler"><a href="#VolumeManagerReconciler" class="headerlink" title="VolumeManagerReconciler"></a>VolumeManagerReconciler</h3><p>而“第二阶段”的 Mount（以及 Unmount）操作，必须发生在 Pod 对应的宿主机上，所以它必须是 kubelet 组件的一部分。这个控制循环的名字，叫作：VolumeManagerReconciler，它运行起来之后，是一个独立于 kubelet 主循环的 Goroutine。通过这样将 Volume 的处理同 kubelet 的主循环解耦，Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环，进而导致 Pod 的创建效率大幅下降的问题。实际上，kubelet 的一个主要设计原则，就是它的主控制循环绝对不可以被 block。这个思想，我在后续的讲述容器运行时的时候还会提到。</p><h3 id="PV对象是如何变成容器里的一个持久化存储的呢？"><a href="#PV对象是如何变成容器里的一个持久化存储的呢？" class="headerlink" title="PV对象是如何变成容器里的一个持久化存储的呢？"></a>PV对象是如何变成容器里的一个持久化存储的呢？</h3><p>我在前面讲解容器基础的时候，已经为你详细剖析了容器 Volume 的挂载机制。</p><p><strong>用一句话总结，所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。</strong></p><p><strong>而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”。</strong>即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。</p><p><strong>显然，我们前面使用的 hostPath 和 emptyDir 类型的 Volume 并不具备这个特征</strong>：它们既有可能被 kubelet 清理掉，也不能被“迁移”到其他节点上。</p><p><strong>所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务</strong>，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。</p><p><strong>而 Kubernetes 需要做的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载时使用。而所谓“持久化”，指的是容器在这个目录里写入的文件，都会保存在远程存储中，从而使得这个目录具备了“持久性”。</strong></p><p>当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;</span><br></pre></td></tr></table></figure><p>这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。</p><h4 id="“第一阶段”（Attach）"><a href="#“第一阶段”（Attach）" class="headerlink" title="“第一阶段”（Attach）"></a>“第一阶段”（Attach）</h4><p>接下来，kubelet 要做的操作就取决于你的 Volume 类型了。</p><ul><li><p>如果你的 Volume 类型是远程块存储，即提供远程磁盘服务。</p><p>比如 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么 kubelet 就需要先调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。<br>这相当于执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> gcloud compute instances attach-disk &lt;虚拟机名字&gt; --disk &lt;远程磁盘名字&gt;</span><br></pre></td></tr></table></figure><p>这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach。</p></li><li><p>而如果你的 Volume 类型是远程文件存储（比如 NFS）的话，kubelet 的处理过程就会更简单一些。因为在这种情况下，kubelet 可以跳过“第一阶段”（Attach）的操作，这是因为一般来说，远程文件存储并没有一个“存储设备”需要挂载在宿主机上。</p></li></ul><h4 id="“第二阶段”（Mount）"><a href="#“第二阶段”（Mount）" class="headerlink" title="“第二阶段”（Mount）"></a>“第二阶段”（Mount）</h4><ul><li><p>如果你的 Volume 类型是远程块存储，即提供远程磁盘服务。</p><p>Attach 阶段完成后，为了能够使用这个远程磁盘，kubelet 还要进行第二个操作，即：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上。不难理解，这个挂载点，正是我在前面反复提到的 Volume 的宿主机目录。所以，这一步相当于执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 通过lsblk命令获取磁盘设备ID</span><br><span class="line">$ sudo lsblk</span><br><span class="line"># 格式化成ext4格式</span><br><span class="line">$ sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/&lt;磁盘设备ID&gt;</span><br><span class="line"># 挂载到挂载点</span><br><span class="line">$ sudo mkdir -p /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;</span><br></pre></td></tr></table></figure><p>这个<strong>将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。</strong></p><p>Mount 阶段完成后，这个 Volume 的宿主机目录就是一个“持久化”的目录了，容器在它里面写入的内容，会保存在 Google Cloud 的远程磁盘中。</p></li><li><p>而如果你的 Volume 类型是远程文件存储（比如 NFS）的话。</p><p>kubelet 会直接从“第二阶段”（Mount）开始准备宿主机上的 Volume 目录。</p><p>kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mount -t nfs &lt;NFS服务器地址&gt;:/ /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;</span><br></pre></td></tr></table></figure></li></ul><p>而经过了“两阶段处理”，我们就得到了一个“持久化”的 Volume 宿主机目录。所以，接下来，kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -v /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;:/&lt;容器内的目标目录&gt; 我的镜像 ...</span><br></pre></td></tr></table></figure><h3 id="Dynamic-Provisioning"><a href="#Dynamic-Provisioning" class="headerlink" title="Dynamic Provisioning"></a>Dynamic Provisioning</h3><p>前面人工管理 PV 的方式就叫作 Static Provisioning。</p><p>Kubernetes 为我们提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。</p><h4 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h4><p>具体地说，StorageClass 对象会定义如下两个部分内容：</p><p>第一，PV 的属性。比如，存储类型、Volume 的大小等等。</p><p>第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。</p><p>有了这样两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 StorageClass 声明的存储插件，创建出需要的 PV。</p><p>举个例子,假如我们的 Volume 的类型是 GCE 的 Persistent Disk 的话</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: block-service</span><br><span class="line">provisioner: kubernetes.io/gce-pd</span><br><span class="line">parameters:</span><br><span class="line">  type: pd-ssd</span><br></pre></td></tr></table></figure><p>这个 StorageClass 的 provisioner 字段的值是：kubernetes.io/gce-pd，这正是 Kubernetes 内置的 GCE PD 存储插件的名字。</p><p>在举个例子，使用Rook存储服务的话</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: ceph.rook.io/v1beta1</span><br><span class="line">kind: Pool</span><br><span class="line">metadata:</span><br><span class="line">  name: replicapool</span><br><span class="line">  namespace: rook-ceph</span><br><span class="line">spec:</span><br><span class="line">  replicated:</span><br><span class="line">    size: 3</span><br><span class="line">---</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: block-service</span><br><span class="line">provisioner: ceph.rook.io/block</span><br><span class="line">parameters:</span><br><span class="line">  pool: replicapool</span><br><span class="line">  #The value of &quot;clusterNamespace&quot; MUST be the same as the one in which your rook cluster exist</span><br><span class="line">  clusterNamespace: rook-ceph</span><br></pre></td></tr></table></figure><p>有了 StorageClass 的 YAML 文件之后，运维人员就可以在 Kubernetes 里创建这个 StorageClass 了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f sc.yaml</span><br></pre></td></tr></table></figure><p><strong>StorageClass 并不是专门为了 Dynamic Provisioning 而设计的。</strong></p><p>比如，在本篇一开始的例子里，我在 PV 和 PVC 里都声明了 storageClassName=manual。而我的集群里，实际上并没有一个名叫 manual 的 StorageClass 对象。这完全没有问题，这个时候 Kubernetes 进行的是 Static Provisioning，但在做绑定决策的时候，它依然会考虑 PV 和 PVC 的 StorageClass 定义。</p><h4 id="在Pvc中使用"><a href="#在Pvc中使用" class="headerlink" title="在Pvc中使用"></a>在Pvc中使用</h4><p>这时候，作为应用开发者，我们只需要在 PVC 里指定要使用的 StorageClass 名字即可，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: claim1</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: block-service</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 30Gi</span><br></pre></td></tr></table></figure><p>这里，你可能会有疑问，我在之前讲解 StatefulSet 存储状态的例子时，好像并没有声明 StorageClass 啊？<strong>实际上，如果你的集群已经开启了名叫 DefaultStorageClass 的 Admission Plugin，它就会为 PVC 和 PV 自动添加一个默认的 StorageClass；否则，PVC 的 storageClassName 的值就是“”，这也意味着它只能够跟 storageClassName 也是“”的 PV 进行绑定。</strong></p><h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><p><img src="/2020/02/21/k8s-command/images/image-20200229195512419.png" alt="image-20200229195512419" style="zoom:50%;"></p><p>从图中我们可以看到，在这个体系中：</p><ul><li><p>PVC 描述的，是 Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。</p></li><li><p>PV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。</p></li><li><p>而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。</p></li></ul><p>当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>容器持久化存储涉及的概念比较多，试着总结一下整体流程。</p><p>当用户创建了一个 PVC 之后，为你创建出对应的 PV。</p><p>PersistentVolumeController帮PVC和PV配对。</p><p>用户提交请求创建pod，Kubernetes发现这个pod声明使用了PVC，找到对应的PV，新创建的PV，还只是一个API 对象，需要经过“两阶段处理”变成宿主机上的“持久化 Volume”才真正有用：<br>第一阶段由运行在master上的AttachDetachController负责，为这个PV完成 Attach 操作，为宿主机挂载远程磁盘；<br>第二阶段是运行在每个节点上kubelet组件的内部，把第一步attach的远程磁盘 mount 到宿主机目录。这个控制循环叫VolumeManagerReconciler，运行在独立的Goroutine，不会阻塞kubelet主循环。</p><p>完成这两步，PV对应的“持久化 Volume”就准备好了，POD可以正常启动，将“持久化 Volume”挂载在容器内指定的路径。</p><h3 id="Local-Persistent-Volume"><a href="#Local-Persistent-Volume" class="headerlink" title="Local Persistent Volume"></a>Local Persistent Volume</h3><h4 id="出现的意义"><a href="#出现的意义" class="headerlink" title="出现的意义"></a>出现的意义</h4><p>在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。也就是说，用户希望 Kubernetes 能够直接使用宿主机上的本地磁盘目录，而不依赖于远程存储服务，来提供“持久化”的容器 Volume。这样做的好处很明显，由于这个 Volume 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多。这个需求对本地物理服务器部署的私有 Kubernetes 集群来说，非常常见。所以，Kubernetes 在 v1.10 之后，就逐渐依靠 PV、PVC 体系实现了这个特性。这个特性的名字叫作：Local Persistent Volume。</p><h4 id="LPV，不就等同于-hostPath-加-NodeAffinity-吗？"><a href="#LPV，不就等同于-hostPath-加-NodeAffinity-吗？" class="headerlink" title="LPV，不就等同于 hostPath 加 NodeAffinity 吗？"></a>LPV，不就等同于 hostPath 加 NodeAffinity 吗？</h4><p>比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了。那么，我只需要再给这个 Pod 加上一个 nodeAffinity=nodeA，不就可以使用这个 Volume 了吗？</p><p>事实上，<strong>你绝不应该把一个宿主机上的目录当作 PV 使用。</strong>这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。</p><p>所以，一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。这个原则，我们可以称为<strong>“一个 PV 一块盘”</strong>。</p><h4 id="适用哪些应用"><a href="#适用哪些应用" class="headerlink" title="适用哪些应用"></a>适用哪些应用</h4><p>它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。</p><h4 id="实现难点一"><a href="#实现难点一" class="headerlink" title="实现难点一"></a>实现难点一</h4><p>如何把本地磁盘抽象成 PV</p><ul><li><p>在公有云上，这个操作等同于给虚拟机额外挂载一个磁盘，比如 GCE 的 Local SSD 类型的磁盘就是一个典型例子。</p></li><li><p>而在我们部署的私有环境中，你有两种办法来完成这个步骤。</p><ul><li>第一种，当然就是给你的宿主机挂载并格式化一个可用的本地磁盘，这也是最常规的操作；</li><li>第二种，对于实验环境，你其实可以在宿主机上挂载几个 RAM Disk（内存盘）来模拟本地磁盘。</li></ul></li></ul><p>我们使用第二种模拟</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span> 在node-1上执行</span><br><span class="line"><span class="meta">$</span> mkdir /mnt/disks</span><br><span class="line"><span class="meta">$</span> for vol in vol1 vol2 vol3; do</span><br><span class="line">    mkdir /mnt/disks/$vol</span><br><span class="line">    mount -t tmpfs $vol /mnt/disks/$vol</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>需要注意的是，如果你希望其他节点也能支持 Local Persistent Volume 的话，那就需要为它们也执行上述操作，并且确保这些磁盘的名字（vol1、vol2 等）都不重复。</p><h4 id="实现难点二"><a href="#实现难点二" class="headerlink" title="实现难点二"></a>实现难点二</h4><p>调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？</p><p>造成这个问题的原因在于，对于常规的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后，再通过“两阶段处理”来“持久化”这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。</p><p>可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。</p><p>这个原则，我们可以称为<strong>“在调度的时候考虑 Volume 分布”</strong>。在 Kubernetes 的调度器里，有一个叫作 <strong>VolumeBindingChecker</strong> 的过滤条件专门负责这个事情。在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。</p><h4 id="模拟Local-PV"><a href="#模拟Local-PV" class="headerlink" title="模拟Local PV"></a>模拟Local PV</h4><h5 id="PV-2"><a href="#PV-2" class="headerlink" title="PV"></a>PV</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">example-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">5</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">local-storage</span></span><br><span class="line"><span class="attr">  local:</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/mnt/disks/vol1</span></span><br><span class="line"><span class="attr">  nodeAffinity:</span></span><br><span class="line"><span class="attr">    required:</span></span><br><span class="line"><span class="attr">      nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">      - matchExpressions:</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line"><span class="attr">          operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">          values:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">node-1</span></span><br></pre></td></tr></table></figure><p>可以看到，这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。</p><p>当然了，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 node-1 上。</p><p>所以，在这个 PV 的定义里，需要有一个 nodeAffinity 字段指定 node-1 这个节点的名字。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。这正是 Kubernetes 实现<strong>“在调度的时候就考虑 Volume 分布”</strong>的主要方法。</p><p>接下来，我们就可以使用 kubect create 来创建这个 PV，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f local-pv.yaml </span><br><span class="line">persistentvolume/example-pv created</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY  STATUS      CLAIM             STORAGECLASS    REASON    AGE</span><br><span class="line">example-pv   5Gi        RWO            Delete           Available                     local-storage             16s</span><br></pre></td></tr></table></figure><p>而正如我在上一篇文章里所建议的那样，使用 PV 和 PVC 的最佳实践，是你要创建一个 StorageClass 来描述这个 PV，如下所示：</p><h5 id="StorageClass-1"><a href="#StorageClass-1" class="headerlink" title="StorageClass"></a>StorageClass</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: local-storage</span><br><span class="line">provisioner: kubernetes.io/no-provisioner</span><br><span class="line">volumeBindingMode: WaitForFirstConsumer</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f local-sc.yaml </span><br><span class="line">storageclass.storage.k8s.io/local-storage created</span><br></pre></td></tr></table></figure><p>这个 StorageClass 的名字，叫作 local-storage。需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。也就是说，我们前面创建 PV 的操作，是不可以省略的。</p><p>与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：<strong>延迟绑定</strong>[^1]。</p><h5 id="PVC-2"><a href="#PVC-2" class="headerlink" title="PVC"></a>PVC</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-local-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br><span class="line">  storageClassName: local-storage</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f local-pvc.yaml </span><br><span class="line">persistentvolumeclaim/example-local-claim created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME                  STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">example-local-claim   Pending                                       local-storage   7s</span><br></pre></td></tr></table></figure><p>可以看到，尽管这个时候，Kubernetes 里已经存在了一个可以与 PVC 匹配的 PV，但这个 PVC 依然处于 Pending 状态，也就是等待绑定的状态。</p><h5 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-pv-pod</span><br><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">    - name: example-pv-storage</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">       claimName: example-local-claim</span><br><span class="line">  containers:</span><br><span class="line">    - name: example-pv-container</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: &quot;http-server&quot;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: example-pv-storage</span><br></pre></td></tr></table></figure><p>而我们一旦使用 kubectl create 创建这个 Pod，就会发现，我们前面定义的 PVC，会立刻变成 Bound 状态，与前面定义的 PV 绑定在了一起，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f local-pod.yaml </span><br><span class="line">pod/example-pv-pod created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME                  STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">example-local-claim   Bound     example-pv   5Gi        RWO            local-storage   6h</span><br></pre></td></tr></table></figure><p>也就是说，在我们创建的 Pod 进入调度器之后，“绑定”操作才开始进行。这时候，我们可以尝试在这个 Pod 的 Volume 目录里，创建一个测试文件，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl exec -it example-pv-pod -- /bin/sh</span><br><span class="line"># cd /usr/share/nginx/html</span><br><span class="line"># touch test.txt</span><br></pre></td></tr></table></figure><p>然后，登录到 node-1 这台机器上，查看一下它的 /mnt/disks/vol1 目录下的内容，你就可以看到刚刚创建的这个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 在node-1上</span><br><span class="line">$ ls /mnt/disks/vol1</span><br><span class="line">test.txt</span><br></pre></td></tr></table></figure><p>而如果你重新创建这个 Pod 的话，就会发现，我们之前创建的测试文件，依然被保存在这个持久化 Volume 当中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f local-pod.yaml </span><br><span class="line"></span><br><span class="line">$ kubectl create -f local-pod.yaml </span><br><span class="line"></span><br><span class="line">$ kubectl exec -it example-pv-pod -- /bin/sh</span><br><span class="line"># ls /usr/share/nginx/html</span><br><span class="line"># touch test.txt</span><br></pre></td></tr></table></figure><h5 id="为什么需要延迟绑定"><a href="#为什么需要延迟绑定" class="headerlink" title="为什么需要延迟绑定"></a>为什么需要延迟绑定</h5><p>我们知道，当你提交了 PV 和 PVC 的 YAML 文件之后，Kubernetes 就会根据它们俩的属性，以及它们指定的 StorageClass 来进行绑定。只有绑定成功后，Pod 才能通过声明这个 PVC 来使用对应的 PV。可是，如果你使用的是 Local Persistent Volume 的话，就会发现，这个流程根本行不通。比如，现在你有一个 Pod，它声明使用的 PVC 叫作 pvc-1。并且，我们规定，这个 Pod 只能运行在 node-2 上。而在 Kubernetes 集群中，有两个属性（比如：大小、读写权限）相同的 Local 类型的 PV。其中，第一个 PV 的名字叫作 pv-1，它对应的磁盘所在的节点是 node-1。而第二个 PV 的名字叫作 pv-2，它对应的磁盘所在的节点是 node-2。假设现在，Kubernetes 的 Volume 控制循环里，首先检查到了 pvc-1 和 pv-1 的属性是匹配的，于是就将它们俩绑定在一起。然后，你用 kubectl create 创建了这个 Pod。这时候，问题就出现了。调度器看到，这个 Pod 所声明的 pvc-1 已经绑定了 pv-1，而 pv-1 所在的节点是 node-1，根据“调度器必须在调度的时候考虑 Volume 分布”的原则，这个 Pod 自然会被调度到 node-1 上。可是，我们前面已经规定过，这个 Pod 根本不允许运行在 node-1 上。所以。最后的结果就是，这个 Pod 的调度必然会失败。这就是为什么，在使用 Local Persistent Volume 的时候，我们必须想办法推迟这个“绑定”操作。</p><h5 id="延迟到什么时候"><a href="#延迟到什么时候" class="headerlink" title="延迟到什么时候"></a>延迟到什么时候</h5><p>答案是：推迟到调度的时候。</p><p>所以说，StorageClass 里的 volumeBindingMode=WaitForFirstConsumer 的含义，就是告诉 Kubernetes 里的 Volume 控制循环（“红娘”）：虽然你已经发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但请不要现在就执行绑定操作（即：设置 PVC 的 VolumeName 字段）。</p><p>而要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV 所在的节点位置，来统一决定，这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。</p><p>这样，在上面的例子里，由于这个 Pod 不允许运行在 pv-1 所在的节点 node-1，所以它的 PVC 最后会跟 pv-2 绑定，并且 Pod 也会被调度到 node-2 上。所以，通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。</p><h3 id="编写自己的存储插件"><a href="#编写自己的存储插件" class="headerlink" title="编写自己的存储插件"></a>编写自己的存储插件</h3><h4 id="FlexVolume"><a href="#FlexVolume" class="headerlink" title="FlexVolume"></a>FlexVolume</h4><p>flexVolume插件只负责attach和mount，使用简单</p><p>所以，如果场景简单，不需要Dynamic Provisioning，则可以使用flexVolume；</p><p><img src="/2020/02/21/k8s-command/images/image-20200229233523745.png" alt="image-20200229233523745" style="zoom:50%;"></p><p>可以看到，在上述体系下，无论是 FlexVolume，还是 Kubernetes 内置的其他存储插件，它们实际上担任的角色，仅仅是 Volume 管理中的“Attach 阶段”和“Mount 阶段”的具体执行者。而像 Dynamic Provisioning 这样的功能，就不是存储插件的责任，而是 Kubernetes 本身存储管理功能的一部分。</p><h4 id="CSI"><a href="#CSI" class="headerlink" title="CSI"></a>CSI</h4><p>而CSI插件包括了一部分原来kubernetes中存储管理的功能，实现、部署起来比较复杂。</p><p>如果场景复杂，需要支持Dynamic Provisioning，则用CSI插件。</p><p><img src="/2020/02/21/k8s-command/images/image-20200229233458219.png" alt="image-20200229233458219" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;minikube-搭建&quot;&gt;&lt;a href=&quot;#minikube-搭建&quot; class=&quot;headerlink&quot; title=&quot;minikube 搭建&quot;&gt;&lt;/a&gt;minikube 搭建&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://kubernetes.io/zh/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>note</title>
    <link href="http://shiyangtao.github.io/2020/02/08/linux_note/"/>
    <id>http://shiyangtao.github.io/2020/02/08/linux_note/</id>
    <published>2020-02-08T13:29:33.000Z</published>
    <updated>2020-02-23T04:36:46.049Z</updated>
    
    <content type="html"><![CDATA[<h3 id="获取linux是ubuntu-or-centos"><a href="#获取linux是ubuntu-or-centos" class="headerlink" title="获取linux是ubuntu or centos"></a>获取linux是ubuntu or centos</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk -F= &apos;/^NAME/&#123;print $2&#125;&apos; /etc/os-release</span><br></pre></td></tr></table></figure><h3 id="获取ubuntu-version"><a href="#获取ubuntu-version" class="headerlink" title="获取ubuntu version"></a>获取ubuntu version</h3><figure class="highlight plain"><figcaption><span>-a```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```output</span><br><span class="line">OUTPUT</span><br><span class="line">No LSB modules are available.</span><br><span class="line">Distributor ID: Ubuntu</span><br><span class="line">Description:    Ubuntu 18.04 LTS</span><br><span class="line">Release:    18.04</span><br><span class="line">Codename:   bionic</span><br></pre></td></tr></table></figure><h3 id="获取docker容器的mount-id和镜像层"><a href="#获取docker容器的mount-id和镜像层" class="headerlink" title="获取docker容器的mount id和镜像层"></a>获取docker容器的mount id和镜像层</h3><figure class="highlight plain"><figcaption><span>/var/lib/docker/image/aufs/layerdb/mounts/$(docker inspect --format &#123;&#123;.Id&#125;&#125; <your_container_name>)/mount-id)```</your_container_name></span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![image-20200210125625341](/images/image-20200210125625341.png)</span><br><span class="line"></span><br><span class="line">有两种方式</span><br><span class="line"></span><br><span class="line">1. cat /var/lib/docker/aufs/layers/&lt;mount-id&gt;</span><br><span class="line">   &lt;img src=&quot;/images/image-20200210125530118.png&quot; alt=&quot;image-20200210125530118&quot; style=&quot;zoom:150%;&quot; /&gt;</span><br><span class="line"></span><br><span class="line">2. Cat /proc/mounts |grep &lt;mount-id&gt;</span><br><span class="line"></span><br><span class="line">   获取si=xxxxxxxxxxx</span><br><span class="line"></span><br><span class="line">   cat /sys/fs/aufs/si_xxxxxxxxxxxx/br[0-9]*</span><br><span class="line"></span><br><span class="line">   ![image-20200210125431975](/images/image-20200210125431975.png)</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">kubeadm join 202.182.103.180:6443 --token wqk2zv.v873dv5se20c0z0o \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:39c09a5a814b67e0a02d7c1656e714487c8a780e046feef300b61c66789b4f4f</span><br><span class="line"></span><br><span class="line">### ssh</span><br><span class="line"></span><br><span class="line">如果遇到</span><br></pre></td></tr></table></figure><p>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@<br>@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @<br>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@<br>IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!<br>Someone could be eavesdropping on you right now (man-in-the-middle attack)!<br>It is also possible that a host key has just been changed.<br>The fingerprint for the ECDSA key sent by the remote host is<br>SHA256:oE6RlmdY9tbK5nwlSoHSb3MsmezZNxC7FatOOoiuXow.<br>Please contact your system administrator.<br>Add correct host key in /Users/mobike/.ssh/known_hosts to get rid of this message.<br>Offending ECDSA key in /Users/mobike/.ssh/known_hosts:18<br>ECDSA host key for 202.182.103.180 has changed and you have requested strict checking.<br>Host key verification failed.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">解决方法</span><br></pre></td></tr></table></figure></p><p>ssh-keygen -R 202.182.103.180<br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;获取linux是ubuntu-or-centos&quot;&gt;&lt;a href=&quot;#获取linux是ubuntu-or-centos&quot; class=&quot;headerlink&quot; title=&quot;获取linux是ubuntu or centos&quot;&gt;&lt;/a&gt;获取linux是ubuntu
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2019年度总结·调整心态</title>
    <link href="http://shiyangtao.github.io/2020/01/10/2019%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%C2%B7%E8%B0%83%E6%95%B4%E5%BF%83%E6%80%81/"/>
    <id>http://shiyangtao.github.io/2020/01/10/2019年度总结·调整心态/</id>
    <published>2020-01-10T08:30:41.000Z</published>
    <updated>2020-01-15T13:36:48.742Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>学习不是努力读更多的书，盲目追求阅读的速度和数量，这会让人产生低层次的勤奋和成长的感觉，这只是在使蛮力。要思辨，要践行，要总结和归纳，否则，你只是在机械地重复某件事，而不会有质的成长的。</p></blockquote><p>从今年开始，我会写年度总结，对比去年的变化，本年的收获和不足，并制定下一年的的计划。</p><p>2019年感觉对我是比较关键的一年，工作方面晋升到下一等级，个人成长方面一直在不断努力，虽然收效甚微，但是万事开头难，做总是比不做强，我相信坚持下去总会有收获。</p><h2 id="2019年总结"><a href="#2019年总结" class="headerlink" title="2019年总结"></a>2019年总结</h2><p>主要是心态的转变，下面是心态转变的几个阶段。</p><h3 id="盲目追新名词"><a href="#盲目追新名词" class="headerlink" title="盲目追新名词"></a>盲目追新名词</h3><p>2017-2019，好多新名词层出不穷，人工智能、区块链、k8s容器化，然后自己的内心也跟着着急起来。想着与时俱进，自己想着赶紧学点啥，然后购买了极客时间的《区块链课程》，看的时候感觉太难了，各种没听过的算法，中途放弃，进入自我怀疑的心态。然后受某个大佬的博客启发，不要盲目追新技术，新技术万变不离其宗，都是基础知识加上新的理念构成。文中有这么一句话，基础扎实的人可能很快掌握这种新技术，但是为了学新名词的菜鸟可能得花上几个月的时间，所以菜鸟要追新技术太难了，成本太高了。不如回归基础。</p><h3 id="莫以浮沙筑高楼"><a href="#莫以浮沙筑高楼" class="headerlink" title="莫以浮沙筑高楼"></a>莫以浮沙筑高楼</h3><p>受到上述启发后，制定了一些针对基础知识的专项训练。包括计算机网络、算法、linux操作系统、网络编程等方面的计划。2019年基本完成了计算机网络的学习，对于TCP协议有了清晰的认识，由当时只了解为了应付面试的三次握手四次挥手，到了解了重传策略、流控制、拥塞控制、keepalive，并用wireshark分析报文。</p><p><strong>这个过程的不足</strong></p><h4 id="贪多"><a href="#贪多" class="headerlink" title="贪多"></a>贪多</h4><p>这个阶段效率确实比较低，因为我学新知识点有个毛病，总是想把关于这个知识点的所有书籍和专栏都看一遍。所以关于计算机网络，我看了极客时间的专栏《趣谈网络协议》和正在看的专栏《Web协议详解与抓包实战》，看了书籍《网络是怎样连接的》、《图解TCP/IP》。所以这个基础知识大概进行了5个月，目前还未完结。</p><h4 id="缺少好的学习习惯"><a href="#缺少好的学习习惯" class="headerlink" title="缺少好的学习习惯"></a>缺少好的学习习惯</h4><p>学的过程不爱记笔记总结，所以基本看完一个专栏或书籍，就忘了，然后就有个重新在读一遍的念头闪现出来，然后整个人就开始浮躁，感觉进入了一怪圈里边，心里想着放弃吧，又不甘心。所以学习过程比较痛苦，这也是效率不高的一个原因。</p><h4 id="浮躁"><a href="#浮躁" class="headerlink" title="浮躁"></a>浮躁</h4><p>这一年自己一直以一种特别浮躁的心态来学东西，总是感觉要学的东西太多了，所以比较着急。举个例子，因为技术好多层次都是相关的，比如今天看到一个知识点，里边有另一个领域的一个知识点，我就会顺藤摸瓜的去学另外一个知识点，然后又发现另外一个知识点…，这样层层追溯下去，自己把自己逼得放弃了，会很苦恼。时间花了不少，因为学习过程是一个不断自我否定的过程，最后自己放弃了。</p><p>以上不足归结到一点还是浮躁。虽然每次都想到放弃，但是值得欣慰的一点是自己2019年没有放弃。因为听到过一句话，大概是这样「平庸的人和优秀的人都会遇到类似问题，但是优秀的人会去总结，然后调整策略继续投入进去，平庸的人到这里就结束了。」</p><h3 id="摆正心态"><a href="#摆正心态" class="headerlink" title="摆正心态"></a>摆正心态</h3><p>根据上述不足，自己复盘反思了下，觉得自己并不是学习意念不够强烈，而是心态不对。</p><ul><li><p>不浮躁：学习是一个循序渐进的过程，有一个好心态，能事半功倍</p></li><li><p>学习方法论：养成好的学习习惯，学完一个知识点，要整理学习笔记，不要怕花时间，经常反思，要借鉴别人的学习方法。</p></li><li><p>不急功近利：从2017年到2019年经历了立志-放弃-立志-放弃的n次循环后，往往都是不断的自我怀疑和自我否定占据上风，这种想法一直在提醒我，你差的太多了，补起来无从下手，在这种思想压力下自己索性就放弃了。2019年我坚持下来了，因为当你前面敌人太多的时候，你总要挑一个下手，收拾完一个，就会有信心收拾另外一个，而不是因为敌人多而停止脚步。</p></li></ul><h2 id="2020年展望"><a href="#2020年展望" class="headerlink" title="2020年展望"></a>2020年展望</h2><p>作为一个码农，技术是安身立命之根本，所以优先从技术层面开始。因为自己达不到程序员的标准，所以姑且认为自己就是个码农吧。</p><h3 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h3><h4 id="编程能力"><a href="#编程能力" class="headerlink" title="编程能力"></a>编程能力</h4><p>由于上级人事变动，新来的leader更关注写代码的能力，而自己之前的观点是「能完成功能是主要的，代码可以适当抽象，不必过分较真」。但是这个观点是错误的，质量好的代码，能避免好多问题。</p><p>编程能力是软件工程师最重要的能力，很多工程师在讨论分布式、高可用系统时都能把别人说的一愣一愣的，张嘴闭嘴就是五个九的可用性，异地多活，但是一旦落到编程能力上，实现一个小功能都能漏洞百出，更不用说写出真正高可用的系统了。</p><p>今年要提升代码抽象能力，计划阅读相关书籍《Clean Code》和《Head First 设计模式》，多看成熟框架的源码。工作中的编码任务，要仔细推敲，好好抽象，不能为了完成任务而糊弄。</p><h4 id="架构能力"><a href="#架构能力" class="headerlink" title="架构能力"></a>架构能力</h4><p>2019年阅读了极客时间的专栏《从0开始学架构》、《从0开始学微服务》、《微服务架构核心20讲》、《微服务架构实战160讲》和书籍《大型网站技术架构》，因为没有实际参与过架构设计，但是看了这些专栏和书籍，对架构和微服务有了一些浅显的认知。2020年希望能结合公司目前的架构设计，加深对架构的理解。</p><h4 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h4><p>2019年基本没有接触其他技术栈。2020年还是以巩固基础能力为主，其他技术栈也可以适当了解。</p><p>2020年会看《Java核心技术 卷一》等Java基础书籍，关于新技术栈目前对容齐比较感兴趣，希望系统的了解下Kubernetes领域。</p><h3 id="软能力"><a href="#软能力" class="headerlink" title="软能力"></a>软能力</h3><h4 id="沟通"><a href="#沟通" class="headerlink" title="沟通"></a>沟通</h4><p>一个好的程序员，需要有好的学习能力，这样你才能为成为技术专家，但是，你还要有好的沟通能力，不然你的技术能力发挥不出来。就像一棵大树一样，学习能力能让你根越来越深，无论遇到什么狂风暴雨，你都可以屹立不倒，而沟通能力则是树干和树叶，它们能让你延伸到更远的天空。</p><p>自己沟通能力欠佳，逻辑有时会混乱，容易激动，语速加快，不能很准确的表达出心里的想法。</p><p>2020年会看下《简单的逻辑学》和《金字塔理论》，并在沟通的时候有意的注意下以上问题。</p><h4 id="写作能力"><a href="#写作能力" class="headerlink" title="写作能力"></a>写作能力</h4><p>今年感觉自己基本没有写作能力和画技术流程图、架构图的能力，把技术方案输出为文档能力很差。</p><p>2020年会着重注意这个问题。</p><h4 id="英语"><a href="#英语" class="headerlink" title="英语"></a>英语</h4><p>搜索尽量google，经常看下medium里的文章。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>2020年，自己会调整好心态，一步一个脚印，积少成多。浮躁的时候，要多想想这句话「学习是一辈子的事，没有速成」。带着浮躁的心态学习，过程很痛苦，而且最后收获很少。所以2020会从以下几个方面入手，在实践中总结出适合自己的学习方法</p><ul><li><p>端正学习态度，摆正心态</p></li><li><p>坚持学习</p></li><li><p>一定要动手实践</p></li><li><p>不要乱买书，一定要买某个技术中最经典的，评价最高的</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;学习不是努力读更多的书，盲目追求阅读的速度和数量，这会让人产生低层次的勤奋和成长的感觉，这只是在使蛮力。要思辨，要践行，要总结和归纳，否则，你只是在机械地重复某件事，而不会有质的成长的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从今年开始，我会写
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>TCP协议详解</title>
    <link href="http://shiyangtao.github.io/2020/01/10/TCP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3/"/>
    <id>http://shiyangtao.github.io/2020/01/10/TCP协议详解/</id>
    <published>2020-01-10T00:03:55.000Z</published>
    <updated>2020-07-07T02:06:59.497Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TCP首部格式"><a href="#TCP首部格式" class="headerlink" title="TCP首部格式"></a>TCP首部格式</h2><p><img src="/2020/01/10/TCP协议详解/images/image-20200114085728302.png" alt="image-20200114085728302" style="zoom:50%;"></p><p>数据偏移：表示TCP所传输的数据部分应该从TCP包的哪个位开始计算，当然也可以看成TCP首部的长度。该字段长4位，单位4字节</p><p>如果没有Options，TCP头部占20字节，Options最多40字节(数据偏移量占4个bit位代表4个字节，即头部最多15*4=60个字节)</p><h3 id="TCP选项"><a href="#TCP选项" class="headerlink" title="TCP选项"></a>TCP选项</h3><p><img src="/2020/01/10/TCP协议详解/images/image-20200114085821927.png" alt="image-20200114085821927" style="zoom: 33%;"></p><h2 id="TCP的特点与其目的"><a href="#TCP的特点与其目的" class="headerlink" title="TCP的特点与其目的"></a>TCP的特点与其目的</h2><p>TCP通过校验和、序列号、确认应答、重发控制、连接管理、流控制以及拥塞控制等机制实现<strong>可靠</strong>传输。</p><h2 id="通过序列号与确认应答提高可靠性"><a href="#通过序列号与确认应答提高可靠性" class="headerlink" title="通过序列号与确认应答提高可靠性"></a>通过序列号与确认应答提高可靠性</h2><h3 id="丢包的情况"><a href="#丢包的情况" class="headerlink" title="丢包的情况"></a>丢包的情况</h3><p><img src="/2020/01/10/TCP协议详解/images/image-20200114075335553.png" alt="image-20200114075335553" style="zoom:50%;"></p><h3 id="确认应答丢失的情况"><a href="#确认应答丢失的情况" class="headerlink" title="确认应答丢失的情况"></a>确认应答丢失的情况</h3><p><img src="/2020/01/10/TCP协议详解/images/image-20200114080214224.png" alt="image-20200114080214224" style="zoom:50%;"></p><h3 id="序列号"><a href="#序列号" class="headerlink" title="序列号"></a>序列号</h3><p>上述这些确认应答处理、重发控制以及重复控制等功能都可以通过序列号实现。</p><p><strong>序列号</strong>是按照顺序给发送数据的每一个字节（8位字节）都标上号码的编码。接收端查询接受数据TCP首部中的序列号和数据长度，将自己下一步应该接受的序列号作为确认应该返送回去。这样通过序列号和确认应答号，TCP可以实现可靠传输。</p><p>序号不能都从1开始，因为A发1/2/3，三个包给B，中途3丢了，这时A掉线了重新连上B，如果序号又从1开始，发送1/2，这时之前丢的3绕回来了，B就认为这个3是下一个包，于是发生了错误。</p><p><strong>因此每个连接都有不同的序号</strong>这个起始序号是随着时间变化的可以看成一个32位的计数器</p><h3 id="序列号回绕"><a href="#序列号回绕" class="headerlink" title="序列号回绕"></a>序列号回绕</h3><p>序列号的范围是0-2^32 也就是0-4G，如果一个连接累计发送超过4G的流量，那么序列号就开始复用了。比如你发送0G-4G流量的时候丢了一个包，当序列号开始复用的时候这个丢的包恰好到了接收方，那么就有问题了。如何解决这个问题呢？</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200114081814275.png" alt="image-20200114081814275" style="zoom:33%;"></p><h3 id="解决序列号回绕"><a href="#解决序列号回绕" class="headerlink" title="解决序列号回绕"></a>解决序列号回绕</h3><p>使用TCP options中的时间戳。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200114082340319.png" alt="image-20200114082340319" style="zoom:33%;"></p><h2 id="重发时间如何确定"><a href="#重发时间如何确定" class="headerlink" title="重发时间如何确定"></a>重发时间如何确定</h2><p>RTO(Retransmission TimeOut )，重传超时时间。</p><p><em>RTT</em>(Round-Trip Time)，报文往返时间。</p><p>重发超时时间(RTO)是指在重发数据之前，等待确认应答到来的那个特定时间间隔(RTT)，如果超过这个时间仍然未收到确认应答，发送端将进行数据重发。</p><h3 id="RTT如何计算"><a href="#RTT如何计算" class="headerlink" title="RTT如何计算"></a>RTT如何计算</h3><p>TCP不论在何种网络环境下都要提供高性能通信，并且无论网络拥堵情况发生了何种变化，都必须保持这一特性。为此，<strong>它在每次发包时都会计算往返时间及其偏差</strong>。将这个时间和偏差相加，重发超时的时间就是比这个总和要稍微大一点的值。</p><p>重发的计算既要考虑往返时间又要考虑偏差是有原因的。如图所示，根据网络环境的不同往返时间可能会产生大幅度的摇摆，之所以发生这种情况是因为数据包的分段是经过不同线路到达的。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200114084826115.png" alt="image-20200114084826115" style="zoom:50%;"></p><h3 id="重发策略"><a href="#重发策略" class="headerlink" title="重发策略"></a>重发策略</h3><p>数据被重发之后若还是收不到确认应答，则进行再次发送。此时，等待确认应答的时间将会以2倍、4倍的指数函数延长。此外数据也不会被无限的、反复地重发。达到一定重发次数后，如果仍没有任何确认应答返回，就会判断为网络或对端主机发生了异常，<strong>强行关闭连接，并且通知应用通信异常强行终止。</strong></p><p>linux设置RTO间隔和次数，<a href="https://pracucci.com/linux-tcp-rto-min-max-and-tcp-retries2.html" target="_blank" rel="noopener">Linux TCP_RTO_MIN, TCP_RTO_MAX and the tcp_retries2 sysctl</a></p><h2 id="连接管理"><a href="#连接管理" class="headerlink" title="连接管理"></a>连接管理</h2><h3 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h3><h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><ul><li><p>建立连接</p><p>TCP是面向连接的，所以在数据通信之前，需要先建立连接。保证双方的包有去有回连接就建立成功了。</p></li><li><p>同步 Sequence 序列号</p><p>初始序列号 ISN(Initial Sequence Number)</p></li><li><p>交换 TCP 通讯参数<br>如 MSS、窗口比例因子、选择性确认、指定校验和算法</p></li></ul><h4 id="状态变迁"><a href="#状态变迁" class="headerlink" title="状态变迁"></a>状态变迁</h4><p><img src="/2020/01/10/TCP协议详解/images/image-20200115071157777.png" alt="image-20200115071157777" style="zoom: 33%;"></p><h4 id="存在的性能和安全问题"><a href="#存在的性能和安全问题" class="headerlink" title="存在的性能和安全问题"></a>存在的性能和安全问题</h4><p>三次握手示例流程<img src="/2020/01/10/TCP协议详解/images/image-20200115073146709.png" alt="image-20200115073146709" style="zoom:50%;"></p><h5 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h5><ul><li><p>应用层 connect 超时时间调整</p></li><li><p>操作系统内核限制调整</p><ul><li>服务器端 SYN_RCV 状态<br> net.ipv4.tcp_max_syn_backlog:SYN_RCVD 状态连接的最大个数<br> net.ipv4.tcp_synack_retries:被动建立连接时，发SYN/ACK的重试次数<br> net.ipv4.tcp_abort_on_overflow:超出处理能力时，对新来的 SYN 直接回包 RST，丢弃连接</li><li>客户端 SYN_SENT 状态<br> net.ipv4.tcp_syn_retries = 6 主动建立连接时，发 SYN 的重试次数<br> net.ipv4.ip_local_port_range = 32768 60999 建立连接时的本地端口可用范围</li></ul></li><li><p>ACCEPT/backlog队列设置<br>当应用创建一个sock的时候，需要制定这个参数。<a href="http://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html" target="_blank" rel="noopener">How TCP backlog works in Linux</a></p></li><li><p>Fast Open降低时延<br><img src="/2020/01/10/TCP协议详解/images/image-20200115074803843.png" alt="image-20200115074803843" style="zoom: 50%;"><br>net.ipv4.tcp_fastopen:系统开启 TFO 功能 </p><ul><li>0:关闭</li></ul><ul><li>1:作为客户端时可以使用 TFO</li><li>2:作为服务器时可以使用 TFO</li><li>3:无论作为客户端还是服务器，都可以使用 TFO</li></ul></li><li><p>TCP_DEFER_ACCEPT<br>设置到server端收到ack后不会进入established，而是丢掉ack，等到接收到client发送的数据才会进入established状态。<br>既然server端忽略ACK，那么client可不可以不发送ACK，而直接发送数据呢，这样就节省了一次传输？答案是可以的：TCP_DEFER_ACCEPT设置在client端的socket fd上，就能达到这个效果。</p></li></ul><h5 id="安全，如何应对SYN攻击"><a href="#安全，如何应对SYN攻击" class="headerlink" title="安全，如何应对SYN攻击"></a>安全，如何应对SYN攻击</h5><p>SYN攻击属于DOS攻击的一种，它利用TCP协议缺陷，伪造IP通过发送大量的半连接请求，服务器回复确认包（进入SYN_RECV），并等待客户的确认，由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列(syn_backlog)，正常的SYN请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。使服务器不能为正常用户服务。</p><ul><li>tcp_syncookies<br><img src="/2020/01/10/TCP协议详解/images/image-20200115080543777.png" alt="image-20200115080543777"><img src="/2020/01/10/TCP协议详解/images/image-20200115080605982.png" alt="image-20200115080605982" style="zoom:50%;"><br>   net.ipv4.tcp_syncookies = 1<br>  当 SYN 队列满后，新的 SYN 不进入队列，计算出 cookie 再 以 SYN+ACK 中的序列号返回客户端，正常客户端发报文时， 服务器根据报文中携带的 cookie 重新恢复连接。<br>  <strong><em>由于 cookie 占用序列号空间，导致此时所有 TCP 可选 功能失效，例如扩充窗口、时间戳等</em></strong><h3 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h3></li></ul><p>为什么建立连接是三次握手，关闭连接确是四次挥手呢？</p><p>建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。<br>而关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。<br>举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。</p><p><a href="https://blog.csdn.net/qzcsu/article/details/72861891" target="_blank" rel="noopener">TCP的三次握手与四次挥手（详解+动图）</a></p><p><img src="/2020/01/10/TCP协议详解/images/image-20200115082654003.png" alt="image-20200115082654003" style="zoom: 33%;"></p><h4 id="优化关闭连接时的TIME-WAIT状态"><a href="#优化关闭连接时的TIME-WAIT状态" class="headerlink" title="优化关闭连接时的TIME-WAIT状态"></a>优化关闭连接时的TIME-WAIT状态</h4><ul><li><p>MSL(Maximum Segment Lifetime)<br>报文最大生存时间</p></li><li><p>维持 2MSL 时长的 TIME-WAIT 状态<br> 保证至少一次报文的往返时间内</p><p>端口是不可复用</p></li></ul><h5 id="TIME-WAIT的作用"><a href="#TIME-WAIT的作用" class="headerlink" title="TIME-WAIT的作用?"></a>TIME-WAIT的作用?</h5><p>这要从两个方面来说。</p><p>首先，这样做是为了确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。</p><blockquote><p>TCP 在设计的时候，做了充分的容错性设计，比如，TCP 假设报文会出错，需要重传。在这里，如果图中主机 1 的 ACK 报文没有传输成功，那么主机 2 就会重新发送 FIN 报文。如果主机 1 没有维护 TIME_WAIT 状态或者维时间太短，而直接进入 CLOSED 状态，它就失去了当前状态的上下文，只能回复一个 RST 操作，从而导致被动关闭方出现错误。现在主机 1 知道自己处于 TIME_WAIT 的状态，就可以在接收到 FIN 报文之后，重新发出一个 ACK 报文，使得主机 2 可以进入正常的 CLOSED 状态。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116110623047.png" alt="image-20200116110623047" style="zoom:50%;"></p></blockquote><p>第二个理由和连接“化身”和报文迷走有关系，为了让旧连接的重复分节在网络中自然消失。</p><blockquote><p>我们知道，在网络中，经常会发生报文经过一段时间才能到达目的地的情况，产生的原因是多种多样的，如路由器重启，链路突然出现故障等。如果迷走报文到达时，发现 TCP 连接四元组（源 IP，源端口，目的 IP，目的端口）所代表的连接不复存在，那么很简单，这个报文自然丢弃。我们考虑这样一个场景，在原连接中断后，又重新创建了一个原连接的“化身”，说是化身其实是因为这个连接和原先的连接四元组完全相同，如果迷失报文经过一段时间也到达，那么这个报文会被误认为是连接“化身”的一个 TCP 分节，这样就会对 TCP 通信产生影响。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116110429174.png" alt="image-20200116110429174" style="zoom:50%;"></p></blockquote><p>所以，TCP 就设计出了这么一个机制，经过 2MSL 这个时间，足以让两个方向上的分组都被丢弃，使得原来连接的分组在网络中都自然消失，再出现的分组一定都是新化身所产生的。</p><h5 id="TIME-WAIT-的危害"><a href="#TIME-WAIT-的危害" class="headerlink" title="TIME_WAIT 的危害"></a>TIME_WAIT 的危害</h5><p>过多的 TIME_WAIT 的主要危害有两种。</p><p>第一是内存资源占用，这个目前看来不是太严重，基本可以忽略。</p><p>第二是对端口资源的占用，一个 TCP 连接至少消耗一个本地端口。要知道，端口资源也是有限的，一般可以开启的端口为 32768～61000 ，也可以通过net.ipv4.ip_local_port_range指定，如果 TIME_WAIT 状态过多，会导致无法创建新连接。这个也是我们在一开始讲到的那个例子。</p><h5 id="如何优化TIME-WAIT"><a href="#如何优化TIME-WAIT" class="headerlink" title="如何优化TIME_WAIT"></a>如何优化TIME_WAIT</h5><h6 id="net-ipv4-tcp-max-tw-buckets"><a href="#net-ipv4-tcp-max-tw-buckets" class="headerlink" title="net.ipv4.tcp_max_tw_buckets"></a>net.ipv4.tcp_max_tw_buckets</h6><p>一个暴力的方法是通过 sysctl 命令，将系统值调小。这个值默认为 18000，当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将所有的 TIME_WAIT 连接状态重置，并且只打印出警告信息。这个方法过于暴力，而且治标不治本，带来的问题远比解决的问题多，不推荐使用。</p><h6 id="net-ipv4-tcp-tw-reuse"><a href="#net-ipv4-tcp-tw-reuse" class="headerlink" title="net.ipv4.tcp_tw_reuse"></a>net.ipv4.tcp_tw_reuse</h6><p>那么 Linux 有没有提供更安全的选择呢？当然有。这就是net.ipv4.tcp_tw_reuse选项。</p><p>Linux 系统对于net.ipv4.tcp_tw_reuse的解释如下:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Allow to reuse TIME-WAIT sockets <span class="keyword">for</span> <span class="keyword">new</span> connections when it is safe from protocol viewpoint. Default value is <span class="number">0</span>.It should not be changed without advice/request of technical experts.</span><br></pre></td></tr></table></figure><p>这段话的大意是从协议角度理解如果是安全可控的，可以复用处于 TIME_WAIT 的套接字为新的连接所用。</p><p>那么什么是协议角度理解的安全可控呢？主要有两点：</p><ol><li><p>只适用于连接发起方（C/S 模型中的客户端）；</p></li><li><p>对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用。</p></li></ol><p>使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即net.ipv4.tcp_timestamps=1（默认即为 1）。</p><p>要知道，TCP 协议也在与时俱进，RFC 1323 中实现了 TCP 拓展规范，以便保证 TCP 的高可用，并引入了新的 TCP 选项，两个 4 字节的时间戳字段，用于记录 TCP 发送方的当前时间戳和从对端接收到的最新时间戳。由于引入了时间戳，我们在前面提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。</p><h6 id="net-ipv4-tcp-tw-recycle"><a href="#net-ipv4-tcp-tw-recycle" class="headerlink" title="net.ipv4.tcp_tw_recycle"></a>net.ipv4.tcp_tw_recycle</h6><p> 开启后，同时作为客户端和服务器都可以使用 TIME-WAIT 状态的端口，不安全，容易造成端口接收数据混乱，无法避免报文延迟、重复等给新连接造成混乱。</p><h2 id="TCP以段为单位发送数据"><a href="#TCP以段为单位发送数据" class="headerlink" title="TCP以段为单位发送数据"></a>TCP以段为单位发送数据</h2><ul><li><p>定义:仅指 TCP 承载数据，不包含 TCP 头部的大小，参见 RFC879</p></li><li><p>MSS 选择目的</p><ul><li>尽量每个 Segment 报文段携带更多的数据，以减少头部空间占用比率</li><li>防止 Segment 被某个设备的 IP 层基于 MTU 拆分</li></ul></li><li><p>默认 MSS:536 字节(默认 MTU576 字节，20 字节 IP 头部，20 字节 TCP 头部)</p></li><li><p>握手阶段协商 MSS<br>两端主机在发出建立连接的请求时，会在TCP首部中写入MSS选项，然后会在两者之间选择一个较小的值使用</p></li><li><p>MSS 分类</p><ul><li>发送方最大报文段 SMSS:SENDER MAXIMUM SEGMENT SIZE</li><li>接收方最大报文段 RMSS:RECEIVER MAXIMUM SEGMENT SIZE</li></ul></li></ul><h2 id="窗口控制"><a href="#窗口控制" class="headerlink" title="窗口控制"></a>窗口控制</h2><h3 id="利用窗口控制提高速度"><a href="#利用窗口控制提高速度" class="headerlink" title="利用窗口控制提高速度"></a>利用窗口控制提高速度</h3><p>TCP以一个段为单位，每发一个段进行一次确认应答的处理。这样的传输方式有个缺点。那就是包的往返时间越长通信性能就越低。</p><p>为了解决这个问题，TCP引入了窗口这个概念。即使在往返时间较长的情况下，他也能控制网络性能的下降。</p><h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><h4 id="发送端的缓存"><a href="#发送端的缓存" class="headerlink" title="发送端的缓存"></a>发送端的缓存</h4><p>第一部分：发送并已经确认的</p><p>第二部分：发送还并且尚未确认</p><p>第三部分：没有发送，但是已经等待发送</p><p>第四部分：没有发送，并且暂时不会发送</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200115091557612.png" alt="image-20200115091557612"></p><p>TCP里接收端会给发送端报一个窗口大小，叫做<strong>AdvertisedWindow</strong>,窗口大小=第二部分+第三部分</p><h4 id="接收端的缓存"><a href="#接收端的缓存" class="headerlink" title="接收端的缓存"></a>接收端的缓存</h4><p>第一部分：接收已确认,之后是已经接收了，但是还没被应用层读取的</p><p>第二部分：等待接收，也就是能承受的最大工作量。也就是<strong>AdvertisedWindow</strong></p><p>第三部分：还没接收，无法接受，因为超过了最大承受量</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200115085639648.png" alt="image-20200115085639648"></p><p>其中第二部分里面，由于收到的包可能不是顺序的，会出现空挡，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。</p><h3 id="窗口控制与重发策略"><a href="#窗口控制与重发策略" class="headerlink" title="窗口控制与重发策略"></a>窗口控制与重发策略</h3><h4 id="丢包怎么办"><a href="#丢包怎么办" class="headerlink" title="丢包怎么办"></a>丢包怎么办</h4><h5 id="某些ack未能返回"><a href="#某些ack未能返回" class="headerlink" title="某些ack未能返回"></a>某些ack未能返回</h5><p><img src="/2020/01/10/TCP协议详解/images/image-20200115084958907.png" alt="image-20200115084958907" style="zoom:50%;"></p><h5 id="某个报文段丢失-快速重传"><a href="#某个报文段丢失-快速重传" class="headerlink" title="某个报文段丢失(快速重传)"></a>某个报文段丢失(快速重传)</h5><p><img src="/2020/01/10/TCP协议详解/images/image-20200115085057723.png" alt="image-20200115085057723" style="zoom:50%;"></p><h2 id="流控制"><a href="#流控制" class="headerlink" title="流控制"></a>流控制</h2><p>发送端根据自己的实际情况发送数据。但是接收端可能收到的是一个毫无关系的数据包又可能会在处理问题其他问题上花费一些时间。因此在为这个数据包做其他处理时会耗费一些时间，甚至在高负荷的情况下无法接受任何数据。如此一来，如果接收端将本应该接收的数据丢弃的话，就又会触发重发机制，从而导致网络流量的无端浪费。<strong>为了防止这种现象的发生，TCP提供一种机制可以让发送端根据接收端的实际接收能力控制发送的数据量。这就是所谓的流控</strong></p><h3 id="窗口滑动与流量控制"><a href="#窗口滑动与流量控制" class="headerlink" title="窗口滑动与流量控制"></a>窗口滑动与流量控制</h3><p>对于包的确认中，同时会携带一个窗口大小。我们假设窗口不变，始终为9，4的ACK来的时候会右移一个，这个时候第13个包也可以发送了</p><p><img src="/2020/01/10/TCP协议详解/images/image-20190821085658843.png" alt="image-20190821085658843"></p><p>这个时候如果发送端发送过猛，会将10、11、12、13都发送了，之后停止发送，因为未发送可发送部分为0。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20190821090108801.png" alt="image-20190821090108801"></p><p>这时候又收到了5的ACK，窗口又会往右滑动一格，这时才可以有更多包可以发送</p><p><img src="/2020/01/10/TCP协议详解/images/image-20190821090331167.png" alt="image-20190821090331167"></p><p>如果接收方处理的太慢，导致缓存中没有空间，可以通过确认信息修改窗口大小，甚至可以设置为0，让发送方定制发送。</p><p>我们假设一个极端情况，接收端的应用一直不读取缓存里的数据，当数据包6确认后，窗口大小不能在是9了，就要缩小一个变成8。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20190821090711406.png" alt="image-20190821090711406"></p><p>这个新窗口8通过6的确认消息，到达发送端的时候，你会发现发送端窗口没有右移，而只是左边的移动了，窗口大小由9变成8</p><p><img src="/2020/01/10/TCP协议详解/images/image-20190821091011217.png" alt="image-20190821091011217"></p><p>如果接收端的应用还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，甚至变为0<img src="/2020/01/10/TCP协议详解/images/image-20190821091137217.png" alt="image-20190821091137217"></p><p>当窗口大小通过14包的ACK到达发送端时，发送端的窗口也变为0。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20190821091254307.png" alt="image-20190821091254307"></p><p><strong>如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。</strong>当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。</p><h3 id="操作系统缓冲区和滑动窗口的关系"><a href="#操作系统缓冲区和滑动窗口的关系" class="headerlink" title="操作系统缓冲区和滑动窗口的关系"></a>操作系统缓冲区和滑动窗口的关系</h3><ul><li>应用层没有及时读取缓存<br><img src="/2020/01/10/TCP协议详解/images/image-20200116073728379.png" alt="image-20200116073728379" style="zoom: 150%;"></li><li>收紧缓冲区导致的丢包<br><img src="/2020/01/10/TCP协议详解/images/image-20200116074115288.png" alt="image-20200116074115288" style="zoom:50%;"><br>解决办法：先收缩窗口在减少缓冲区</li></ul><h2 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h2><p>有了TCP的窗口控制，收发主机之间即使不再以一个数据段为单位发送确认应答，也能够连续发送大量的数据包。然后，如果在通信刚开始时就发送大量的数据，也可能会引发其他问题。</p><p>那发送方怎么判断网络是不是满呢？这其实是个挺难的事情，因为对于 TCP 协议来讲，他压根不知道整个网络路径都会经历什么，对他来讲就是一个黑盒。</p><p>假设tcp发送的速度超过了带宽，中间设备处理不了的包就会<strong>被丢弃</strong>， 为了解决这个问题中间经过的设备会会加上缓存，处理不过来的在队列里排着，虽然避免了丢包，但是造成了<strong>超时重传</strong></p><p>于是TCP的拥塞控制住要来解决这两种现象，<strong>包丢失和超时重传</strong>，一旦出现了这些现象就说明太快了，要慢一点。<strong>TCP 的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽</strong></p><p>拥塞窗口cwnd(congestion window)</p><p>通告窗口rwnd(receiver‘s advertised window) </p><p>发送窗口swnd = min(cwnd，rwnd)</p><h3 id="拥塞控制历史"><a href="#拥塞控制历史" class="headerlink" title="拥塞控制历史"></a>拥塞控制历史</h3><ul><li><p>以丢包作为依据<br> • New Reno:RFC6582<br> • BIC:Linux2.6.8 – 2.6.18<br> • CUBIC(RFC8312):Linux2.6.19</p></li><li><p>以探测带宽作为依据<br>• BBR:Linux4.9</p></li></ul><h3 id="以丢包为依据的CUBIC"><a href="#以丢包为依据的CUBIC" class="headerlink" title="以丢包为依据的CUBIC"></a>以丢包为依据的CUBIC</h3><h4 id="慢启动"><a href="#慢启动" class="headerlink" title="慢启动"></a>慢启动</h4><p>每收到一个ACK，cwnd扩充一倍，呈指数级增长。<br><img src="/2020/01/10/TCP协议详解/images/image-20200116080718738.png" alt="image-20200116080718738" style="zoom:50%;"></p><p>有了上述这些机制，就可以有效地减少通信开始时的连续发包导致的网络拥堵，还可以避免网络拥塞情况的发生。</p><h4 id="拥塞避免"><a href="#拥塞避免" class="headerlink" title="拥塞避免"></a>拥塞避免</h4><p>不过随着每次往返，拥塞窗口也会以指数函数的增长，拥堵状况激增甚至导致网络拥塞的发生。为了防止这些，引入慢启动阀值 ssthresh(slow start threshold)的概念。只要拥塞窗口超过这个阀值，在没收到一次确认应答时，只允许以下面这种比例放大拥塞窗口。达到 ssthresh 后，以<strong>线性方式增加</strong> </p><p> cwnd += SMSS*SMSS/cwnd<img src="/2020/01/10/TCP协议详解/images/image-20200116081538341.png" alt="image-20200116081538341" style="zoom: 50%;"></p><h4 id="超时重传"><a href="#超时重传" class="headerlink" title="超时重传"></a>超时重传</h4><p>但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。</p><p>拥塞的一种表现形式是丢包，需要超时重传，这个时候，将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。</p><h4 id="快速重传与快速恢复"><a href="#快速重传与快速恢复" class="headerlink" title="快速重传与快速恢复"></a>快速重传与快速恢复</h4><p>前面讲过<strong>快速重传算法</strong>。当接收端发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速的重传，不必等待超时再重传。TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd 减半为 cwnd/2，然后 sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116081937646.png" alt="image-20200116081937646" style="zoom: 33%;"></p><h4 id="TCP窗口变化图"><a href="#TCP窗口变化图" class="headerlink" title="TCP窗口变化图"></a>TCP窗口变化图</h4><p><img src="/2020/01/10/TCP协议详解/images/image-20200116082156643.png" alt="image-20200116082156643"></p><h3 id="从丢包到测量驱动的拥塞控制算法"><a href="#从丢包到测量驱动的拥塞控制算法" class="headerlink" title="从丢包到测量驱动的拥塞控制算法"></a>从丢包到测量驱动的拥塞控制算法</h3><p>上述拥塞策略有什么问题</p><p><strong>第一个问题</strong>是是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。</p><p><strong>第二个问题</strong>是 TCP 的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实 TCP 只要填满管道就可以了，不应该接着填，直到连缓存也填满。</p><p>为了优化这两个问题出来了<strong>TCP拥塞 BBR算法</strong>。它企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116083329085.png" alt="image-20200116083329085" style="zoom: 25%;"></p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116083413192.png" alt="image-20200116083413192" style="zoom: 33%;"></p><h2 id="提高网络利用率"><a href="#提高网络利用率" class="headerlink" title="提高网络利用率"></a>提高网络利用率</h2><h3 id="SACK-与选择性重传算法"><a href="#SACK-与选择性重传算法" class="headerlink" title="SACK 与选择性重传算法"></a>SACK 与选择性重传算法</h3><h4 id="仅重传丢失段-保守乐观"><a href="#仅重传丢失段-保守乐观" class="headerlink" title="仅重传丢失段 保守乐观"></a>仅重传丢失段 保守乐观</h4><p>累积确认 Sequence 序号的问题</p><p>如果part3没有收到，ack只会通知server端它没有收到part3，但是不会告知他收到了part4.</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116083744636.png" alt="image-20200116083744636" style="zoom:50%;"></p><p>在这张图中这个算法工作的特别好，因为只重传了丢失的part3。但是如果part4页丢失了呢，part3经过重发已经收到了了，那么client又开始通知server端part4也没有收到，效率比较低。</p><p>仅重传丢失段:保守乐观 ，大量丢包时效率低下</p><h4 id="重传所有段-–积极悲观"><a href="#重传所有段-–积极悲观" class="headerlink" title="重传所有段 –积极悲观"></a>重传所有段 –积极悲观</h4><p><img src="/2020/01/10/TCP协议详解/images/image-20200116083828977.png" alt="image-20200116083828977" style="zoom:50%;"></p><p>重传所有段:积极悲观 ，可能浪费带宽</p><h3 id="引入SACK-Selective-Acknowledgment"><a href="#引入SACK-Selective-Acknowledgment" class="headerlink" title="引入SACK( Selective Acknowledgment)"></a>引入SACK( Selective Acknowledgment)</h3><p><img src="/2020/01/10/TCP协议详解/images/image-20200116084735278.png" alt="image-20200116084735278" style="zoom:50%;"></p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116084850439.png" alt="image-20200116084850439" style="zoom:50%;"></p><p>wireshark抓包</p><p><img src="/2020/01/10/TCP协议详解/images/image-20200116084921705.png" alt="image-20200116084921705"></p><h3 id="如何减少小报文提高网络效率"><a href="#如何减少小报文提高网络效率" class="headerlink" title="如何减少小报文提高网络效率?"></a>如何减少小报文提高网络效率?</h3><h4 id="糊涂窗口综合症SWS"><a href="#糊涂窗口综合症SWS" class="headerlink" title="糊涂窗口综合症SWS"></a>糊涂窗口综合症SWS</h4><p><img src="/2020/01/10/TCP协议详解/images/image-20200116085214448.png" alt="image-20200116085214448" style="zoom:50%;"></p><p>糊涂窗口综合症（Silly window syndrome），亦称愚蠢窗口综合症、愚笨窗口综合症，是TCP流量控制实现不良导致的一种计算机网络问题。当发送程序缓慢地创建数据，接收程序缓慢地消耗数据，或者两者同时存在时，滑动窗口运作会出现严重问题。如果一个服务器无法处理所有传入的数据而存在此问题，它会要求客户端减少一次发送的数据量（TCP网络数据包的滑动窗口）。如果服务器仍然无法处理所有传入的数据，窗口会随此问题越来越小，有时甚至将使传输数据小于数据包头，使数据传输变得极为低效。这个问题的名字取自窗口缩小到“愚蠢”的值。</p><h4 id="SWS-避免算法"><a href="#SWS-避免算法" class="headerlink" title="SWS 避免算法"></a>SWS 避免算法</h4><h5 id="发送方"><a href="#发送方" class="headerlink" title="发送方"></a>发送方</h5><p>• Nagle 算法:TCP_NODELAY 用于关闭 Nagle 算法</p><p>Nagle解决方案要求发送方发送第一个段，即使它是小的，然后就等待直至收到一个ACK，或者累积到最大大小段（MSS）。</p><h5 id="接收方"><a href="#接收方" class="headerlink" title="接收方"></a>接收方</h5><p> • David D Clark 算法:Clark的解决方案是关闭窗口，直到接收到最大段大小（MSS）的另一个段，或者缓冲区为半空。</p><h4 id="TCP-delayed-acknowledgment-延迟确认"><a href="#TCP-delayed-acknowledgment-延迟确认" class="headerlink" title="TCP delayed acknowledgment 延迟确认"></a>TCP delayed acknowledgment 延迟确认</h4><p>接收数据的主机如果每次都立即回复确认的话，可能会返回一个较小的窗口。那是因为刚接受完数据，应用还没有读取。当某个发送端收到这个小窗口后，会以它为上限发送数据，从而降低网络利用率。因此引入一个方法，那就是收到数据不立即返回确认，而是延时一段时间。</p><ul><li>在没有收到2✖️最大长度的数据为止不做确认应答(根据操作系统的不同，有时也有不论数据大小，只要收到两个包就即刻返回确认应答的情况。)</li><li>其他情况下，最大延迟0.5秒发送确认应答( 很多操作系统设置为0.2秒左右)</li></ul><h4 id="PiggyBack-Acknowleggement-捎带应答"><a href="#PiggyBack-Acknowleggement-捎带应答" class="headerlink" title="(PiggyBack Acknowleggement)捎带应答"></a>(PiggyBack Acknowleggement)捎带应答</h4><p>TCP的确认应答和回执数据可以通过一个包发送。这种方式叫做捎带应答。通过这种机制，可以使收发的数据量减少。</p><p>另外，接收数据以后如果立刻返回确认应答，就无法实现捎带应答。而是将所接收的数据传给应用处理生成返回数据以后再进行发送请求为止，必须一直等待确认应答的发送。也就是说，如果没有启用延迟确认应答就无法实现捎带应答。</p><h4 id="Linux-上更为激进的“Nagle”：TCP-CORK"><a href="#Linux-上更为激进的“Nagle”：TCP-CORK" class="headerlink" title="Linux 上更为激进的“Nagle”：TCP_CORK"></a>Linux 上更为激进的“Nagle”：TCP_CORK</h4><p><a href="http://senlinzhan.github.io/2017/02/10/Linux的TCP-CORK/" target="_blank" rel="noopener">Nagle 算法与 TCP socket 选项 TCP_CORK</a></p><h4 id="Nagle-VS-delayed-ACK"><a href="#Nagle-VS-delayed-ACK" class="headerlink" title="Nagle VS delayed ACK"></a>Nagle VS delayed ACK</h4><p>Nagle和delayed ACK不能同时打开</p><p>• 关闭 delayed ACK:TCP_QUICKACK </p><p>• 关闭 Nagle:TCP_NODELAY</p><h2 id="TCP的keep-alive功能"><a href="#TCP的keep-alive功能" class="headerlink" title="TCP的keep-alive功能"></a>TCP的keep-alive功能</h2><p> Linux 的 tcp keepalive </p><p>• 发送心跳周期</p><p>• Linux: net.ipv4.tcp_keepalive_time = 7200 • 探测包发送间隔</p><p>• net.ipv4.tcp_keepalive_intvl = 75 • 探测包重试次数</p><p>• net.ipv4.tcp_keepalive_probes = 9</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;TCP首部格式&quot;&gt;&lt;a href=&quot;#TCP首部格式&quot; class=&quot;headerlink&quot; title=&quot;TCP首部格式&quot;&gt;&lt;/a&gt;TCP首部格式&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/2020/01/10/TCP协议详解/images/image-2020011
      
    
    </summary>
    
    
      <category term="TCP" scheme="http://shiyangtao.github.io/tags/TCP/"/>
    
  </entry>
  
  <entry>
    <title>《趣谈网络协议》云计算中的网络24-28</title>
    <link href="http://shiyangtao.github.io/2019/11/01/%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C24-28/"/>
    <id>http://shiyangtao.github.io/2019/11/01/《趣谈网络协议》云计算中的网络24-28/</id>
    <published>2019-11-01T03:34:15.000Z</published>
    <updated>2019-11-01T06:28:40.333Z</updated>
    
    <content type="html"><![CDATA[<h3 id="虚拟网卡的原理"><a href="#虚拟网卡的原理" class="headerlink" title="虚拟网卡的原理"></a>虚拟网卡的原理</h3><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101113541971.png" alt="image-20191101113541971" style="zoom: 33%;"></p><h3 id="虚拟机互联"><a href="#虚拟机互联" class="headerlink" title="虚拟机互联"></a>虚拟机互联</h3><p>在物理机上，应该有一个虚拟的交换机，在 Linux 上有一个命令叫作 brctl，可以创建虚拟的网桥 brctl addbr br0。创建出来以后，将两个虚拟机的虚拟网卡，都连接到虚拟网桥 brctl addif br0 tap0 上，这样将两个虚拟机配置相同的子网网段，两台虚拟机就能够相互通信了。</p><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101113710817.png" alt="image-20191101113710817" style="zoom:33%;"><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101113844901.png" alt="image-20191101113844901" style="zoom:50%;"></p><p>这里面，host-only 的网络对应的，其实就是上面两个虚拟机连到一个 br0 虚拟网桥上，而且不考虑访问外部的场景，只要虚拟机之间能够相互访问就可以了。</p><h3 id="虚拟机连接外网"><a href="#虚拟机连接外网" class="headerlink" title="虚拟机连接外网"></a>虚拟机连接外网</h3><ul><li><p>桥接</p><p>虚拟交换机就是br0</p></li></ul><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101114020201.png" alt="image-20191101114020201" style="zoom:30%;"></p><p>如果使用桥接网络，当你登录虚拟机里看 IP 地址的时候会发现，你的虚拟机的地址和你的笔记本电脑的，以及你旁边的同事的电脑的网段是一个网段。这是为什么呢？这其实相当于将物理机和虚拟机放在同一个网桥上，相当于这个网桥上有三台机器，是一个网段的，全部打平了。我将图画成下面的样子你就好理解了。</p><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101114043236.png" alt="image-20191101114043236" style="zoom:30%;"></p><p>在数据中心里面，采取的也是类似的技术，只不过都是 Linux，在每台机器上都创建网桥 br0，虚拟机的网卡都连到 br0 上，物理网卡也连到 br0 上，所有的 br0 都通过物理网卡出来连接到物理交换机上。</p><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101140610267.png" alt="image-20191101140610267" style="zoom:33%;"></p><p>同样我们换一个角度看待这个拓扑图。同样是将网络打平，虚拟机会和你的物理网络具有相同的网段。</p><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101140637789.png" alt="image-20191101140637789" style="zoom:33%;"></p><p>在这种方式下，不但解决了同一台机器的互通问题，也解决了跨物理机的互通问题，因为都在一个二层网络里面，彼此用相同的网段访问就可以了。但是当规模很大的时候，会存在问题。你还记得吗？在一个二层网络里面，最大的问题是广播。</p><ul><li><p>NAT</p><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101140844198.png" alt="image-20191101140844198" style="zoom:33%;"></p></li></ul><p>在这种方式下，你登录到虚拟机里面查看 IP 地址，会发现虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。虚拟机要想访问物理机的时候，需要将地址 NAT 成为物理机的地址。</p><p>除此之外，它还会在你的笔记本电脑里内置一个 DHCP 服务器，为笔记本电脑上的虚拟机动态分配 IP 地址。因为虚拟机的网络自成体系，需要进行 IP 管理。为什么桥接方式不需要呢？因为桥接将网络打平了，虚拟机的 IP 地址应该由物理网络的 DHCP 服务器分配。</p><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101141026746.png" alt="image-20191101141026746" style="zoom:33%;"></p><p>如果是你自己登录到物理机上做个简单配置，你可以简化一下。例如将虚拟机所在网络的网关的地址直接配置到 br0 上，不用 DHCP Server，手动配置每台虚拟机的 IP 地址，通过命令 iptables -t nat -A POSTROUTING -o ethX -j MASQUERADE，直接在物理网卡 ethX 上进行 NAT，所有从这个网卡出去的包都 NAT 成这个网卡的地址。通过设置 net.ipv4.ip_forward = 1，开启物理机的转发功能，直接做路由器，而不用单独的路由器，这样虚拟机就能直接上网了</p><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101141235230.png" alt="image-20191101141235230" style="zoom:33%;"></p><h3 id="隔离问题"><a href="#隔离问题" class="headerlink" title="隔离问题"></a>隔离问题</h3><p>如果一台机器上的两个虚拟机不属于同一个用户，怎么办呢？</p><ul><li><p>好在 brctl 创建的网桥也是支持 VLAN 功能的，可以设置两个虚拟机的 tag，这样在这个虚拟网桥上，两个虚拟机是不互通的。</p></li><li><p>但是如何跨物理机互通，并且实现 VLAN 的隔离呢？由于 brctl 创建的网桥上面的 tag 是没办法在网桥之外的范围内起作用的，于是我们需要寻找其他的方式。有一个命令<strong>vconfig</strong>可以基于物理网卡 eth0 创建带 VLAN 的虚拟网卡，所有从这个虚拟网卡出去的包，都带这个 VLAN，如果这样，跨物理机的互通和隔离就可以通过这个网卡来实现。</p></li></ul><p><img src="/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/image-20191101142002588.png" alt="image-20191101142002588" style="zoom: 33%;"></p><ol><li>首先为每个用户分配不同的 VLAN，例如有一个用户 VLAN 10，一个用户 VLAN 20。</li><li>在一台物理机上，基于物理网卡，为每个用户用 vconfig 创建一个带 VLAN 的网卡。</li><li>不同的用户使用不同的虚拟网桥，带 VLAN 的虚拟网卡也连接到虚拟网桥上。</li></ol><p>不同的用户由于网桥不通，不能相互通信，一旦出了网桥，由于 VLAN 不同，也不会将包转发到另一个网桥上。另外，出了物理机，也是带着 VLAN ID 的。只要物理交换机也是支持 VLAN 的，到达另一台物理机的时候，VLAN ID 依然在，它只会将包转发给相同 VLAN 的网卡和网桥，所以跨物理机，不同的 VLAN 也不会相互通信。</p><p><strong>缺点：</strong> </p><p>使用 brctl 创建出来的网桥功能是简单的，基于 VLAN 的虚拟网卡也能实现简单的隔离。但是这都不是大规模云平台能够满足的，一个是 VLAN 的隔离，数目太少。前面我们学过，VLAN ID 只有 4096 个，明显不够用。另外一点是这个配置不够灵活。谁和谁通，谁和谁不通，流量的隔离也没有实现，还有大量改进的空间。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;虚拟网卡的原理&quot;&gt;&lt;a href=&quot;#虚拟网卡的原理&quot; class=&quot;headerlink&quot; title=&quot;虚拟网卡的原理&quot;&gt;&lt;/a&gt;虚拟网卡的原理&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/2019/11/01/《趣谈网络协议》云计算中的网络24-28/images/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《趣谈网络协议》第14-23讲笔记</title>
    <link href="http://shiyangtao.github.io/2019/10/25/%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E7%AC%AC14-23%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    <id>http://shiyangtao.github.io/2019/10/25/《趣谈网络协议》第14-23讲笔记/</id>
    <published>2019-10-25T02:20:06.000Z</published>
    <updated>2019-11-01T03:32:58.396Z</updated>
    
    <content type="html"><![CDATA[<p>HTTP 协议虽然很常用，也很复杂，重点记住 GET、POST、 PUT、DELETE 这几个方法，以及重要的首部字段；</p><h3 id="HTTP2-0"><a href="#HTTP2-0" class="headerlink" title="HTTP2.0"></a>HTTP2.0</h3><p>HTTP 2.0 通过头压缩、分帧、二进制编码、多路复用等技术提升性能；</p><h4 id="相比HTTP1-1的优点"><a href="#相比HTTP1-1的优点" class="headerlink" title="相比HTTP1.1的优点"></a>相比HTTP1.1的优点</h4><p>HTTP 2.0 会对 HTTP 的头进行一定的压缩，将原来每次都要携带的大量 key  value 在两端建立一个索引表，对相同的头只发送索引表中的索引</p><p>HTTP 2.0 协议将一个 TCP 的连接中，切分成多个流，每个流都有自己的 ID，而且流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。流是有优先级的</p><p>HTTP 2.0 还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。常见的帧有Header 帧，用于传输 Header 内容，并且会开启一个新的流。再就是Data 帧，用来传输正文实体。多个 Data 帧属于同一个流。</p><p>通过以上两种机制，HTTP 2.0 的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。</p><p>我们来举一个例子。假设我们的一个页面要发送三个独立的请求，一个获取 css，一个获取 js，一个获取图片 jpg。如果使用 HTTP 1.1 就是<strong>串行</strong>的，但是如果使用 HTTP 2.0，就可以在一个连接里，客户端和服务端都可以同时发送多个请求或回应，而且不用按照顺序一对一对应。</p><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191025104036399.png" alt="image-20191025104036399" style="zoom:33%;"></p><p>HTTP 2.0 其实是将三个请求变成三个流，将数据分成帧，乱序发送到一个 TCP 连接中。<img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191025104109000.png" alt="image-20191025104109000" style="zoom:33%;"></p><p><strong>HTTP 2.0 成功解决了 HTTP 1.1 的队首阻塞问题，同时，也不需要通过 HTTP 1.x 的 pipeline 机制用多条 TCP 连接来实现并行请求与响应，减少了 TCP 连接数对服务器性能的影响。同时将页面的多个数据 css、js、 jpg 等通过一个数据链接进行传输，能够加快页面组件的传输速度。</strong></p><h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><p>HTTP 2.0 虽然大大增加了并发性，但还是有问题的。因为 HTTP 2.0 也是基于 TCP 协议的，TCP 协议在处理包时是有严格顺序的。当其中一个数据包遇到问题，TCP 连接需要等待这个包完成重传之后才能继续进行。虽然 HTTP 2.0 通过多个 stream，使得逻辑上一个 TCP 连接上的并行内容，进行多路数据的传输，并且这些数据没有关联。一前一后，前面 stream 2 的帧没有收到，后面 stream 1 的帧也会因此阻塞。</p><h3 id="QUIC"><a href="#QUIC" class="headerlink" title="QUIC"></a>QUIC</h3><p>于是，就又到了从 TCP 切换到 UDP。这就是 Google 的 QUIC 协议。</p><p>QUIC 协议通过基于 UDP 自定义的类似 TCP 的连接、重试、多路复用、流量控制技术，进一步提升性能。</p><h3 id="HTTPS"><a href="#HTTPS" class="headerlink" title="HTTPS"></a>HTTPS</h3><p>加密分对称加密和非对称加密。对称加密效率高，但是解决不了密钥传输问题；非对称加密可以解决这个问题，但是效率不高。</p><p> 非对称加密需要通过证书和权威机构来验证公钥的合法性。</p><p>HTTPS 是综合了对称加密和非对称加密算法的 HTTP 协议。既保证传输安全，也保证传输效率。</p><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191029083836932.png" alt="image-20191029083836932" style="zoom:67%;"></p><h3 id="FTP下载"><a href="#FTP下载" class="headerlink" title="FTP下载"></a>FTP下载</h3><p>FTP 采用两个 TCP 连接来传输一个文件。</p><ul><li>控制连接：服务器以被动的方式，打开众所周知用于 FTP 的端口 21，客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有：list——获取文件目录；reter——取一个文件；store——存一个文件。</li><li>数据连接：每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。</li></ul><p>FTP两种工作模式</p><ul><li>主动模式(PORT) 主动模式是FTP客户端向FTP服务器发送数据传输需要的端口，FTP服务端去连接FTP客户端的端口。<img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191029215256257.png" alt="image-20191029215256257"></li><li>被动模式(PASV) 被动模式是FTP服务器返回数据传输需要的端口，FTP客户端去连接FTP服务端。<br>绝大部分的互联网应用(比如Web/Http)，都是客户端向服务端发起连接。换句话说，绝大部分互联网应用都是被动模式。<img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191029215401395.png" alt="image-20191029215401395"></li></ul><p><strong>需要注意的是，被动模式和主动模式的登录过程，都是FTP客户端去连接FTP服务器。</strong></p><h4 id="为什么绝大部分互联网应用都是被动模式"><a href="#为什么绝大部分互联网应用都是被动模式" class="headerlink" title="为什么绝大部分互联网应用都是被动模式"></a>为什么绝大部分互联网应用都是被动模式</h4><p>因为大部分客户端都是在路由器后面，没有独立的公网IP地址，服务器想要主动连接客户端，难度太大，在现在真实的互联网环境里面几乎是不可能完成的任务。</p><h3 id="P2P"><a href="#P2P" class="headerlink" title="P2P"></a>P2P</h3><p>种子文件(.torrent)由两部分组成，分别是：announce（tracker URL）和文件信息。</p><p>文件信息里面有这些内容。</p><ul><li>info 区：这里指定的是该种子有几个文件、文件有多长、目录结构，以及目录和文件的名字。</li><li>Name 字段：指定顶层目录名字。</li><li>每个段的大小：BitTorrent（简称 BT）协议把一个文件分成很多个小段，然后分段下载。</li><li>段哈希值：将整个种子中，每个段的 SHA-1 哈希值拼在一起</li></ul><p>下载时，BT 客户端首先解析.torrent 文件，得到 tracker 地址，然后连接 tracker 服务器。tracker 服务器回应下载者的请求，将其他下载者（包括发布者）的 IP 提供给下载者。</p><p><strong>虽然下载的过程是非中心化的，但是加入这个 P2P 网络的时候，都需要借助 tracker 中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。所以，这种工作方式有一个弊端，一旦 tracker 服务器出现故障或者线路遭到屏蔽，BT 工具就无法正常工作了。能不能彻底非中心化呢？请看DHT</strong></p><h4 id="去中心化网络-DHT"><a href="#去中心化网络-DHT" class="headerlink" title="去中心化网络(DHT)"></a>去中心化网络(DHT)</h4><p><strong>DHT(Distributed Hash Table)</strong></p><p>有一种著名的 DHT 协议，叫 Kademlia 协议，接下来要介绍的就是这个协议。</p><p>任何一个BitTorrent启动后，都有两个角色</p><ul><li><strong>peer</strong>角色  监听TCP端口，用来上传下载文件。</li><li><strong>DHT node</strong> 监听一个UDP端口，通过这个角色加入DHT网络</li></ul><h4 id="存储和定位"><a href="#存储和定位" class="headerlink" title="存储和定位"></a>存储和定位</h4><p>一个DHT网络里，每一个DHT Node，都有一个id，每个DHT都保存有一些索引，哪些文件保存在哪个节点上。</p><p>算法规定，如果有文件的哈希值和DHT node的id一致，则这个DHT node知道从哪里下载，除了一模一样的DHT node知道，和哈希值接近的N个DHT node也知道。什么叫和哈希值接近呢？例如只修改了最后一位，就很接近；修改了倒数 2 位，也不远；修改了倒数 3 位，也可以接受。总之，凑齐了规定的 N 这个数就行。</p><h4 id="如何找到对应的NODE呢"><a href="#如何找到对应的NODE呢" class="headerlink" title="如何找到对应的NODE呢"></a>如何找到对应的NODE呢</h4><h5 id="分层"><a href="#分层" class="headerlink" title="分层"></a>分层</h5><p> 如果一个节点的 ID，前面所有位数相同，<strong>从倒数第 i 位开始不同</strong>，这样的节点只有 2^(i-1) 个，与基础节点的距离范围为 [2^(i-1), 2^i)；对于 01010 而言，<strong>这样的节点归为“k-bucket i”</strong>。</p><h5 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h5><p><strong><em>判断目标节点C与自己从倒数第i几位开始不同，然后从“k-bucket i”中去找，这一层所有NODE的第i位肯定和A的第i位不一样</em></strong></p><ul><li>如果有，那就找到了；</li><li>如果没有，在k-bucket i里随便找一个B节点（注意任意B节点，它的第i位肯定与C相同，即它与C的距离小于 2^(i-1)，距离缩短了一半，Kademlia 的这种查询机制，是通过折半查找的方式来收缩范围，对于总的节点数目为 N，最多只需要查询 log2(N) 次，就能够找到。）</li></ul><h4 id="Kademlia-算法中，每个节点只有-4-个指令。"><a href="#Kademlia-算法中，每个节点只有-4-个指令。" class="headerlink" title="Kademlia 算法中，每个节点只有 4 个指令。"></a>Kademlia 算法中，每个节点只有 4 个指令。</h4><ul><li>PING：测试一个节点是否在线，还活着没，相当于打个电话，看还能打通不。</li><li>STORE：要求一个节点存储一份数据，既然加入了组织，有义务保存一份数据。</li><li>FIND_NODE：根据节点 ID 查找一个节点，就是给一个 160 位的 ID，通过上面朋友圈的方式找到那个节点。</li><li>FIND_VALUE：根据 KEY 查找一个数据，实则上跟 FIND_NODE 非常类似。KEY 就是文件对应的 160 位的 ID，就是要找到保存了文件的节点。</li></ul><h4 id="k-bucket的维护及更新机制"><a href="#k-bucket的维护及更新机制" class="headerlink" title="k-bucket的维护及更新机制"></a>k-bucket的维护及更新机制</h4><ul><li><p>每个bucket里的节点都按照最后一次接触的时间倒序排列(按最后一次接触时间从末尾往开头排序)</p></li><li><p>每次执行四个指令的任何一个都会触发更新</p></li><li><p>当一个节点与自己接触时，检查它是否在K-bucket中</p><p>–如果在，那么将它挪到k-bucket列表最底层（最新）</p><p>–如果不在，PING一下列表最上面（最旧）的一个节点</p><p>–a）如果PING通，将旧节点挪到列表最底，并丢弃新节点</p><p>–b）如果PING不通，删除旧节点，并将新节点加入列表</p></li></ul><p>该机制保证了任意节点加入和离开都不影响整体网络</p><p><a href="https://www.jianshu.com/p/f2c31e632f1d" target="_blank" rel="noopener">易懂分布式 | Kademlia算法</a></p><h3 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h3><ul><li>传统的 DNS 有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT 问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。</li><li>HTTPDNS 通过客户端 SDK 和服务端，通过 HTTP 直接调用解析 DNS 的方式，绕过了传统 DNS 的这些缺点，实现了智能的调度</li></ul><h3 id="CDN"><a href="#CDN" class="headerlink" title="CDN"></a>CDN</h3><ul><li>CDN 和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。</li><li>CDN 最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。</li></ul><h3 id="VPN"><a href="#VPN" class="headerlink" title="VPN"></a>VPN</h3><h4 id="连接多个数据中心的方式："><a href="#连接多个数据中心的方式：" class="headerlink" title="连接多个数据中心的方式："></a>连接多个数据中心的方式：</h4><ul><li><p>走公网，最简单但不安全</p></li><li><p>专线连接，成本高昂，效率高</p></li><li><p>VPN 连接，简单便宜，保证私密性，性能稍差</p></li></ul><h4 id="IPSec-VPN「Virtual-Private-Network」工作原理："><a href="#IPSec-VPN「Virtual-Private-Network」工作原理：" class="headerlink" title="IPSec VPN「Virtual Private Network」工作原理："></a>IPSec VPN「Virtual Private Network」工作原理：</h4><p>将要发送的 IP 包「乘客协议」加密后加上 IPSec 包头「隧道协议」后再放入另一个 IP 包「承载协议」中发送</p><p><img src="https://mubu.com/document_image/9a172e1e-5204-4c06-8b21-2baeb957a4e2-963664.jpg" alt="img"></p><h4 id="IPSec-VPN-是基于-IP-协议的安全隧道协议，采用一些机制保证安全性"><a href="#IPSec-VPN-是基于-IP-协议的安全隧道协议，采用一些机制保证安全性" class="headerlink" title="IPSec VPN 是基于 IP 协议的安全隧道协议，采用一些机制保证安全性"></a>IPSec VPN 是基于 IP 协议的安全隧道协议，采用一些机制保证安全性</h4><ul><li><p>私密性：加密数据</p></li><li><p>完整性：对数据进行 hash 运算产生数据摘要</p></li><li><p>真实性：通过身份认证保证对端身份的真实性</p></li></ul><h4 id="IPSec-VPN-协议簇包括："><a href="#IPSec-VPN-协议簇包括：" class="headerlink" title="IPSec VPN 协议簇包括："></a>IPSec VPN 协议簇包括：</h4><ul><li><p>两种协议：</p><ul><li><p>AH「Authentication Header」：只能进行数据摘要，不能实现数据加密</p></li><li><p>ESP「Encapsulating Security Payload」：能够进行数据加密和数据摘要</p></li></ul></li><li><p>两种算法：</p><ul><li><p>加密算法</p></li><li><p>摘要算法</p></li></ul></li><li><p>两大组件：</p><ul><li><p>IKE「Internet Key Exchange Key Management」：用于交换对称秘钥</p></li><li><p>SA「Security Association」：进行连接维护</p></li></ul></li></ul><h4 id="IPsec-VPN-的建立过程："><a href="#IPsec-VPN-的建立过程：" class="headerlink" title="IPsec VPN 的建立过程："></a>IPsec VPN 的建立过程：</h4><ul><li><p>建立 IKE 自己的 SA，算出对称秘钥 K</p></li><li><p>使用对称秘钥 K 建立 IPSec SA，在 SA 中生成随机对称秘钥 M，使用 M 进行双方接下来的通信</p></li></ul><h4 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h4><h5 id="IP-协议："><a href="#IP-协议：" class="headerlink" title="IP 协议："></a>IP 协议：</h5><p>不是面向连接的，是尽力而为的协议，每个 IP 包自由选择路径，依赖于上一层 TCP 的重发来保证可靠性</p><p>优点：一条道路崩溃时，可以自动换其他路</p><p>缺点：不断的路由查找，效率低下</p><h5 id="IPSec-VPN-的缺点："><a href="#IPSec-VPN-的缺点：" class="headerlink" title="IPSec VPN 的缺点："></a>IPSec VPN 的缺点：</h5><p>由于 IPSec VPN 是基于 IP 协议的，所以速度慢</p><h5 id="ATM-协议："><a href="#ATM-协议：" class="headerlink" title="ATM 协议："></a>ATM 协议：</h5><p>这种协议是面向连接的，并且和 IP 是同一个层次，ATM 是在传输之前先建立一个连接，形成一个虚拟的通路</p><p>优点：速度快，因为按照指定路径传输</p><p>缺点：当某个节点故障，连接就会中断，无法传输数据</p><h5 id="多协议标签交换「MPLS，Multi-Protocol-Label-Switching」"><a href="#多协议标签交换「MPLS，Multi-Protocol-Label-Switching」" class="headerlink" title="多协议标签交换「MPLS，Multi-Protocol Label Switching」"></a>多协议标签交换「MPLS，Multi-Protocol Label Switching」</h5><p>结合了 IP 和 ATM 协议的优点</p><p>需要标签交换路由器「LSR，Label Switching Router」的支持</p><p>如何动态生成标签 LDP「Label Distribution Protocol」</p><h5 id="将-MPLS-和-VPN-结合起来可以提高-VPN-的效率"><a href="#将-MPLS-和-VPN-结合起来可以提高-VPN-的效率" class="headerlink" title="将 MPLS 和 VPN 结合起来可以提高 VPN 的效率"></a>将 MPLS 和 VPN 结合起来可以提高 VPN 的效率</h5><p>需要解决的问题有：</p><p>BGP 协议如何处理地址空间重叠的 VPN 的路由</p><p>路由表怎么区分重复的网段</p><h3 id="移动网络"><a href="#移动网络" class="headerlink" title="移动网络"></a>移动网络</h3><p>手机是通过收发无线信号来通信的，专业名称是 Mobile Station，简称 MS，需要嵌入 SIM。手机是客户端，而无线信号的服务端，就是<strong>基站子系统（BSS，Base Station SubsystemBSS）</strong></p><h4 id="2G"><a href="#2G" class="headerlink" title="2G"></a>2G</h4><p>2G 时代，上网使用的不是 IP 网络，而是电话网络，走模拟信号，专业名称为公共交换电话网（PSTN，Public Switched Telephone Network）。</p><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031084314819.png" alt="image-20191031084314819"></p><p>基站子系统分两部分，一部分对外提供无线通信，叫作<strong>基站收发信台（BTS，Base Transceiver Station）</strong>，另一部分对内连接有线网络，叫作<strong>基站控制器（BSC，Base Station Controller）</strong>。基站收发信台通过无线收到数据后，转发给基站控制器。这部分属于无线的部分，统称为<strong>无线接入网（RAN，Radio Access Network）。</strong></p><p>基站控制器通过有线网络，连接到提供手机业务的运营商的数据中心，这部分称为<strong>核心网（CN，Core Network）</strong>。核心网还没有真的进入互联网，这部分还是主要提供手机业务，是手机业务的有线部分。</p><p>首先接待基站来的数据的是<strong>移动业务交换中心（MSC，Mobile Service Switching Center）</strong>，它是进入核心网的入口。</p><p><strong>鉴权中心（AUC，Authentication Center）</strong>和<strong>设备识别寄存器（EIR，Equipment Identity Register）</strong>主要是负责安全性的</p><p>另外，需要看你是本地的号，还是外地的号，这个牵扯到计费的问题，异地收费还是很贵的。<strong>访问位置寄存器（VLR，Visit Location Register）</strong>是看你目前在的地方，<strong>归属位置寄存器（HLR，Home Location Register）</strong>是看你的号码归属地。</p><p><strong>网关移动交换中心（GMSC ，Gateway Mobile Switching Center）</strong>是一个网关，连接核心网和真正的互联网。</p><p>数据中心里面的这些模块统称为<strong>网络子系统（NSS，Network and Switching Subsystem）</strong></p><p><strong>因而 2G 时代的上网如图所示，我们总结一下，有这几个核心点</strong>：</p><ul><li>手机通过无线信号连接基站；</li><li>基站一面朝前接无线，一面朝后接核心网；</li><li>核心网一面朝前接到基站请求，一是判断你是否合法，二是判断你是不是本地号，还有没有钱，一面通过网关连接电话网络。</li></ul><h4 id="2-5G"><a href="#2-5G" class="headerlink" title="2.5G"></a>2.5G</h4><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031085342609.png" alt="image-20191031085342609"></p><p>相对2G网络的变化：</p><ul><li><p>原来电路交换的基础上，加入了分组交换业务，支持 Packet 的转发，从而支持 IP 网络。**</p></li><li><p>在上述网络的基础上，基站一面朝前接无线，一面朝后接核心网。在朝后的组件中，多了一个分组控制单元（PCU，Packet Control Unit），用以提供分组交换通道。</p></li><li>在核心网里面，有个朝前的接待员（SGSN，Service GPRS Supported Node）和朝后连接 IP 网络的网关型 GPRS 支持节点（GGSN，Gateway GPRS Supported Node）。</li></ul><h4 id="3G"><a href="#3G" class="headerlink" title="3G"></a>3G</h4><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031085721337.png" alt="image-20191031085721337"></p><p>到了 3G 时代，主要是无线通信技术有了改进，大大增加了无线的带宽。以 W-CDMA 为例，理论最高 2M 的下行速度。</p><p>改变：</p><ul><li>基站改变了，一面朝外的是 Node B，一面朝内连接核心网的是无线网络控制器（RNC，Radio Network Controller）。</li><li>核心网以及连接的 IP 网络没有什么变化。</li></ul><h4 id="4G"><a href="#4G" class="headerlink" title="4G"></a>4G</h4><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031090122178.png" alt="image-20191031090122178"></p><p>4G 网络，基站为 eNodeB，包含了原来 Node B 和 RNC 的功能，下行速度向百兆级别迈进。另外，核心网实现了控制面和数据面的分离。</p><p>在前面的核心网里面，有接待员 MSC 或者 SGSN，你会发现检查是否合法是它负责，转发数据也是它负责，也即控制面和数据面是合二为一的，这样灵活性比较差，因为控制面主要是指令，多是小包，往往需要高的及时性；数据面主要是流量，多是大包，往往需要吞吐量。</p><p><strong>HSS</strong> 用于存储用户签约信息的数据库，其实就是你这个号码归属地是哪里的，以及一些认证信息。<strong>MME</strong> 是核心控制网元，是控制面的核心，当手机通过 eNodeB 连上的时候，MME 会根据 HSS 的信息，判断你是否合法。如果允许连上来，MME 不负责具体的数据的流量，而是 MME 会选择数据面的<strong>SGW 和 PGW</strong>，然后告诉 eNodeB，我允许你连上来了，你连接它们吧。于是手机直接通过 eNodeB 连接 SGW，连上核心网，SGW 相当于数据面的接待员，并通过 PGW 连到 IP 网络。<strong>PGW 就是出口网关</strong>。在出口网关，有一个组件 <strong>PCRF</strong>，称为策略和计费控制单元，用来控制上网策略和流量的计费。</p><h3 id="4G网络协议解析"><a href="#4G网络协议解析" class="headerlink" title="4G网络协议解析"></a>4G网络协议解析</h3><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031210620836.png" alt="image-20191031210620836"></p><h4 id="控制面协议"><a href="#控制面协议" class="headerlink" title="控制面协议"></a>控制面协议</h4><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031211958857.png" alt="image-20191031211958857"></p><p>eNodeB 还是两面派，朝前对接无线网络，朝后对接核心网络，在控制面对接的是 MME。</p><p>eNodeB 和 MME 之间的连接就是很正常的 IP 网络，但是这里面在 IP 层之上，却既不是 TCP，也不是 UDP，而是 SCTP。这也是传输层的协议，也是面向连接的，但是更加适合移动网络。 它继承了 TCP 较为完善的拥塞控制并改进 TCP 的一些不足之处。</p><p>SCTP优势：</p><ul><li>多宿主（Multi-homing）</li><li>多流（Multi-streaming）</li><li>初始化保护（Initiation protection）</li><li>消息分帧（Message framing）</li><li>可配置的无序发送（Configurable unordered delivery）</li><li>平滑关闭（Graceful shutdown）</li></ul><p>参考链接<a href="https://www.ibm.com/developerworks/cn/linux/l-sctp/index.html" target="_blank" rel="noopener">使用 SCTP 优化网络</a></p><p>当 MME 通过认证鉴权，同意这个手机上网的时候，需要建立一个数据面的数据通路。建立通路的过程还是控制面的事情，因而使用的是控制面的协议 GTP-C。GTP-C是基于UDP的，GTP有序列号，所以不用 TCP，GTP-C 自己就可以实现可靠性，为每个输出信令消息分配一个依次递增的序列号，以确保信令消息的按序传递，并便于检测重复包。对于每个输出信令消息启动定时器，在定时器超时前未接收到响应消息则进行重发。</p><p>建设的数据通路分两段路，其实是两个隧道。</p><ul><li>一段是从 eNodeB 到 SGW，这个数据通路由 MME 通过 S1-MME 协议告诉 eNodeB，它是隧道的一端，通过 S11 告诉 SGW，它是隧道的另一端。</li><li>第二端是从 SGW 到 PGW，SGW 通过 S11 协议知道自己是其中一端，并主动通过 S5 协议，告诉 PGW 它是隧道的另一端。</li></ul><h4 id="数据面协议"><a href="#数据面协议" class="headerlink" title="数据面协议"></a>数据面协议</h4><p>当两个隧道都打通，接在一起的时候，PGW 会给手机分配一个 IP 地址，这个 IP 地址是隧道内部的 IP 地址，可以类比为 IPsec 协议里面的 IP 地址。这个 IP 地址是归手机运营商管理的。然后，手机可以使用这个 IP 地址，连接 eNodeB，从 eNodeB 经过 S1-U 协议，通过第一段隧道到达 SGW，再从 SGW 经过 S8 协议，通过第二段隧道到达 PGW，然后通过 PGW 连接到互联网。数据面的协议都是通过 GTP-U，如图所示。</p><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031212716706.png" alt="image-20191031212716706"></p><p>手机每发出的一个包，都由 GTP-U 隧道协议封装起来，格式如下。</p><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031212859940.png" alt="image-20191031212859940"></p><h4 id="手机上网流程"><a href="#手机上网流程" class="headerlink" title="手机上网流程"></a>手机上网流程</h4><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031213555048.png" alt="image-20191031213555048"></p><ol><li>手机开机以后，在附近寻找基站 eNodeB，找到后给 eNodeB 发送 Attach Request，说“我来啦，我要上网”。</li><li>eNodeB 将请求发给 MME，说“有个手机要上网”。</li><li>MME 去请求手机，一是认证，二是鉴权，还会请求 HSS 看看有没有钱，看看是在哪里上网。</li><li>当 MME 通过了手机的认证之后，开始分配隧道，先告诉 SGW，说要创建一个会话（Create Session）。在这里面，会给 SGW 分配一个隧道 ID t1，并且请求 SGW 给自己（MME）也分配一个隧道 ID。</li><li>SGW 转头向 PGW 请求建立一个会话，为 PGW 的控制面分配一个隧道 ID t2，也给 PGW 的数据面分配一个隧道 ID t3，并且请求 PGW 给自己（SGW）的控制面和数据面分配隧道 ID。</li><li>PGW 回复 SGW 说“创建会话成功”，使用自己的控制面隧道 ID t2，回复里面携带着给 SGW 控制面分配的隧道 ID t4 和控制面的隧道 ID t5，至此 SGW 和 PGW 直接的隧道建设完成。双方请求对方，都要带着对方给自己分配的隧道 ID，从而标志是这个手机的请求。</li><li>接下来 SGW 回复 MME 说“创建会话成功”，使用自己的隧道 ID t1 访问 MME，回复里面有给 MME 分配隧道 ID t6，也有 SGW 给 eNodeB 分配的隧道 ID t7。</li><li>当 MME 发现后面的隧道都建设成功之后，就告诉 eNodeB，“后面的隧道已经建设完毕，SGW 给你分配的隧道 ID 是 t7，你可以开始连上来了，但是你也要给 SGW 分配一个隧道 ID”。</li><li>eNodeB 告诉 MME 自己给 SGW 分配一个隧道，ID 为 t8。</li><li>MME 将 eNodeB 给 SGW 分配的隧道 ID t8 告知 SGW，从而前面的隧道也建设完毕</li></ol><p>这样，手机就可以通过建立的隧道成功上网了。</p><h4 id="为什么你的手机在国外上不了facebook"><a href="#为什么你的手机在国外上不了facebook" class="headerlink" title="为什么你的手机在国外上不了facebook"></a>为什么你的手机在国外上不了facebook</h4><p>为什么要分 SGW 和 PGW 呢，一个 GW 不可以吗？SGW 是你本地的运营商的设备，而 PGW 是你所属的运营商的设备。</p><p>如果你在巴塞罗那，一下飞机，手机开机，周围搜寻到的肯定是巴塞罗那的 eNodeB。通过 MME 去查寻国内运营商的 HSS，看你是否合法，是否还有钱。如果允许上网，你的手机和巴塞罗那的 SGW 会建立一个隧道，然后巴塞罗那的 SGW 和国内运营商的 PGW 建立一个隧道，然后通过国内运营商的 PGW 上网。</p><p><img src="/2019/10/25/《趣谈网络协议》第14-23讲笔记/images/image-20191031214043695.png" alt="image-20191031214043695"></p><p><strong>这样判断你是否能上网的在国内运营商的 HSS，控制你上网策略的是国内运营商的 PCRF，给手机分配的 IP 地址也是国内运营商的 PGW 负责的，给手机分配的 IP 地址也是国内运营商里统计的。运营商由于是在 PGW 里面统计的，这样你的上网流量全部通过国内运营商即可，只不过巴塞罗那运营商也要和国内运营商进行流量结算。由于你的上网策略是由国内运营商在 PCRF 中控制的，因而你还是上不了脸书。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;HTTP 协议虽然很常用，也很复杂，重点记住 GET、POST、 PUT、DELETE 这几个方法，以及重要的首部字段；&lt;/p&gt;
&lt;h3 id=&quot;HTTP2-0&quot;&gt;&lt;a href=&quot;#HTTP2-0&quot; class=&quot;headerlink&quot; title=&quot;HTTP2.0&quot;&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《图解TCP/IP》笔记</title>
    <link href="http://shiyangtao.github.io/2019/09/19/%E3%80%8A%E5%9B%BE%E8%A7%A3TCP-IP%E3%80%8B%E7%AC%94%E8%AE%B0/"/>
    <id>http://shiyangtao.github.io/2019/09/19/《图解TCP-IP》笔记/</id>
    <published>2019-09-19T13:32:37.000Z</published>
    <updated>2019-09-19T13:34:49.315Z</updated>
    
    <content type="html"><![CDATA[<h3 id="中继器到底能否完成不同媒介之间的转接工作？"><a href="#中继器到底能否完成不同媒介之间的转接工作？" class="headerlink" title="中继器到底能否完成不同媒介之间的转接工作？"></a>中继器到底能否完成不同媒介之间的转接工作？</h3><p>《图解TCP/IP》中说道“一般，中继器两端连接的是相同的通信媒介，<strong>但有的中继器也可以完成不同媒介之间的转接工作。</strong>如，可以在同轴电缆与光缆之间调整信号。然而，这种情况下，中继器也只是单纯负责信号在0和1比特流之间的替换，并不负责判断数据是否有错误。同时，它只负责将电信号转换为光信号，<strong>因此不能在传输速度不同的媒介之间转发</strong>”</p><p>请问是否前后矛盾，不能理解</p><p>这句话有问题，所以不正确也正确。</p><p>比如光网络可以跑万兆以上 但是你的光猫转换器是1000M的电口，符合你这个说法，因为光和电 2个媒介是传输速度不同的媒介。</p><p>但是 如果你的光口进来的数据的速率是100M的 那么这时候你的电口是可以满足的 这时候你这句话就是错误的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;中继器到底能否完成不同媒介之间的转接工作？&quot;&gt;&lt;a href=&quot;#中继器到底能否完成不同媒介之间的转接工作？&quot; class=&quot;headerlink&quot; title=&quot;中继器到底能否完成不同媒介之间的转接工作？&quot;&gt;&lt;/a&gt;中继器到底能否完成不同媒介之间的转接工作？&lt;/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>知识盲点</title>
    <link href="http://shiyangtao.github.io/2019/09/11/%E7%9F%A5%E8%AF%86%E7%9B%B2%E7%82%B9/"/>
    <id>http://shiyangtao.github.io/2019/09/11/知识盲点/</id>
    <published>2019-09-11T07:19:00.000Z</published>
    <updated>2020-05-19T06:04:07.259Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://juejin.im/post/5d3685146fb9a07ed064f11b" target="_blank" rel="noopener">git pull&amp;git pull rebase</a></p><p><a href="http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html" target="_blank" rel="noopener">字符编码笔记：ASCII，Unicode 和 UTF-8</a></p><h3 id="长连接相关"><a href="#长连接相关" class="headerlink" title="长连接相关"></a>长连接相关</h3><p><a href="https://blog.csdn.net/chrisnotfound/article/details/80112736" target="_blank" rel="noopener">TCP Keepalive机制与应用层心跳Heartbeat</a></p><p><a href="https://feichashao.com/linux_tcp_keepalive/" target="_blank" rel="noopener">如何使用 Linux 中的 TCP keepalive?</a></p><p><a href="http://www.blogjava.net/yongboy/archive/2015/04/14/424413.html" target="_blank" rel="noopener">随手记之TCP Keepalive笔记</a></p><p><a href="https://juejin.im/entry/5aa6144e51882555731bc3e5" target="_blank" rel="noopener">高效保活长连接：手把手教你实现 自适应的心跳保活机制</a></p><p><a href="https://www.cnblogs.com/wshenjin/p/11354123.html" target="_blank" rel="noopener"><a href="https://www.cnblogs.com/wshenjin/p/11354123.html" target="_blank" rel="noopener">nginx的so_keepalive和timeout相关小计</a></a></p><h3 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h3><p>NAT大致分为两大类：圆锥形和对称型</p><p>圆锥形又分为三种</p><ul><li><em>Full Cone：</em>计算机A链接公网计算机M后，NAT打开一个端口，以后公网上任何发送到这个端口的数据（不限于M）都可以访问到A，这个时候任何从公网上发过来的数据都可以通过该端口到达内网计算机A，不限制M、端口和IP</li><li><em>Address Restricted Cone：</em>内网计算机A通过路由器链接了外网计算机M，NAT打开一个端口，这个时候外网计算机M（只限于M）可以通过任何端口和内网计算A进行通信。限制了ip地址，没有限制端口。</li><li><em>Port Restricted Cone：</em>内网计算机A通过路由器链接了外网计算机M，NAT打开一个端口，M可以通过这个端口跟A进行通信，这种即限制了ip地址又限制了端口。</li></ul><p><strong>Symmetric NAT（对称形）</strong></p><p>对称型NAT和圆锥形不同的地方在于：</p><ul><li>圆锥形NAT对于同一台内网计算机，无论与那一台的外网服务器通信，NAT所分配的端口不变。</li><li>对称型NAT对于同一台内网计算机与不同的外网计算机通讯会分配不同的端口号，对称型NAT一般不能用于p2p软件。</li></ul><p><a href="https://segmentfault.com/a/1190000008044600" target="_blank" rel="noopener">NAT穿透介绍</a></p><p><a href="https://segmentfault.com/q/1010000002389520" target="_blank" rel="noopener">SNAT DNAT</a></p><h3 id="linux命令"><a href="#linux命令" class="headerlink" title="linux命令"></a>linux命令</h3><p><a href="https://man.linuxde.net/netstat" target="_blank" rel="noopener">netstat</a>     <a href="https://man.linuxde.net/ss" target="_blank" rel="noopener">ss</a></p><h3 id="网络相关"><a href="#网络相关" class="headerlink" title="网络相关"></a>网络相关</h3><p><a href="https://medium.com/@factoryhr/http-2-the-difference-between-http-1-1-benefits-and-how-to-use-it-38094fa0e95b" target="_blank" rel="noopener">HTTP/2: the difference between HTTP/1.1, benefits and how to use it</a></p><p><a href="https://www.zhihu.com/question/24723688" target="_blank" rel="noopener">正向代理和反向代理</a></p><p><a href="https://cloud.tencent.com/developer/article/1115747" target="_blank" rel="noopener">你真的掌握lvs工作原理吗？</a></p><h3 id="consul-架构"><a href="#consul-架构" class="headerlink" title="consul 架构"></a>consul 架构</h3><p><img src="/2019/09/11/知识盲点/images/image-20191209201457970.png" alt="image-20191209201457970" style="zoom: 33%;"></p><p><img src="/2019/09/11/知识盲点/images/image-20191209212023558.png" alt="image-20191209212023558" style="zoom:50%;"></p><h3 id="阻塞IO和非阻塞IO"><a href="#阻塞IO和非阻塞IO" class="headerlink" title="阻塞IO和非阻塞IO"></a>阻塞IO和非阻塞IO</h3><p><img src="/2019/09/11/知识盲点/images/image-20191210082044204.png" alt="image-20191210082044204" style="zoom:50%;"></p><p><img src="/2019/09/11/知识盲点/images/image-20200103102827571.png" alt="image-20200103102827571" style="zoom: 33%;"></p><p><a href="https://tech.meituan.com/2016/11/04/nio.html" target="_blank" rel="noopener">Java NIO浅析</a></p><h3 id="原码-反码-补码"><a href="#原码-反码-补码" class="headerlink" title="原码 反码 补码"></a>原码 反码 补码</h3><p><a href="https://cloud.tencent.com/developer/article/1353672" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1353672</a></p><h3 id="IaaS-Paas-Saas的区别"><a href="#IaaS-Paas-Saas的区别" class="headerlink" title="IaaS, Paas, Saas的区别"></a>IaaS, Paas, Saas的区别</h3><p> <a href="http://www.ruanyifeng.com/blog/2017/07/iaas-paas-saas.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2017/07/iaas-paas-saas.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://juejin.im/post/5d3685146fb9a07ed064f11b&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;git pull&amp;amp;git pull rebase&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>gc案例</title>
    <link href="http://shiyangtao.github.io/2019/09/11/gc%E6%A1%88%E4%BE%8B/"/>
    <id>http://shiyangtao.github.io/2019/09/11/gc案例/</id>
    <published>2019-09-11T05:21:38.000Z</published>
    <updated>2019-09-11T06:43:29.972Z</updated>
    
    <content type="html"><![CDATA[<h3 id="2019-09-11-频繁cms-gc"><a href="#2019-09-11-频繁cms-gc" class="headerlink" title="2019-09-11 频繁cms gc"></a>2019-09-11 频繁cms gc</h3><p><img src="/2019/09/11/gc案例/images/image-20190911132251445.png" alt="image-20190911132251445"></p><p>jvm参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xmx6g -Xms6g -XX:SurvivorRatio=8 -XX:NewRatio=2 -XX:PermSize=128m -XX:MaxPermSize=512m -XX:+DisableExplicitGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintCommandLineFlags -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ParallelCMSThreads=4 -XX:+CMSClassUnloadingEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=1 -XX:CMSInitiatingOccupancyFraction=50</span><br></pre></td></tr></table></figure><p>现象：老年代远远没有达到配置的50%阀值就开始进行了频繁cms gc</p><p>解决办法：增加jvm参数 -XX:+UseCMSInitiatingOccupancyOnly，加上后效果明显</p><p><img src="/2019/09/11/gc案例/images/image-20190911144255254.png" alt="image-20190911144255254"></p><p>如果不配置 -XX:+UseCMSInitiatingOccupancyOnly cms会动态决定什么时候执行cms gc，详情观看下文</p><p>参考链接 <a href="https://mp.weixin.qq.com/s/Mu-Xz4CLgdxJhcMJ7aKAHg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Mu-Xz4CLgdxJhcMJ7aKAHg</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;2019-09-11-频繁cms-gc&quot;&gt;&lt;a href=&quot;#2019-09-11-频繁cms-gc&quot; class=&quot;headerlink&quot; title=&quot;2019-09-11 频繁cms gc&quot;&gt;&lt;/a&gt;2019-09-11 频繁cms gc&lt;/h3&gt;&lt;p&gt;&lt;i
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《网络是怎么连接的》第二章笔记</title>
    <link href="http://shiyangtao.github.io/2019/09/10/%E3%80%8A%E7%BD%91%E7%BB%9C%E6%98%AF%E6%80%8E%E4%B9%88%E8%BF%9E%E6%8E%A5%E7%9A%84%E3%80%8B%E7%AC%AC%E4%BA%8C%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://shiyangtao.github.io/2019/09/10/《网络是怎么连接的》第二章笔记/</id>
    <published>2019-09-10T00:07:38.000Z</published>
    <updated>2019-11-08T05:42:06.491Z</updated>
    
    <content type="html"><![CDATA[<h2 id="用电信号传输TCP-IP数据—–探索协议栈和网卡"><a href="#用电信号传输TCP-IP数据—–探索协议栈和网卡" class="headerlink" title="用电信号传输TCP/IP数据—–探索协议栈和网卡"></a>用电信号传输TCP/IP数据—–探索协议栈和网卡</h2><h3 id="创建套接字"><a href="#创建套接字" class="headerlink" title="创建套接字"></a>创建套接字</h3><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190910082802049.png" alt="image-20190910082802049" style="zoom:50%;"></p><p>浏览器、邮件等应用程序收发数据时一般使用TCP;DNS查询等收发较短的控制数据时使用UDP;</p><h4 id="套接字是什么？"><a href="#套接字是什么？" class="headerlink" title="套接字是什么？"></a>套接字是什么？</h4><p>协议栈内部有一块用于存放控制信息的内存空间，这里记录了用于控制通信操作的控制信息，例如通信对象的IP地址、端口号、通信操作的进行状态等。本来套接字只是一个概念而已，并不存在实体，如果一定要赋予它一个实体，我们可以说这些控制信息就是套接字的实体，或者说存放控制信息的内存空间就是套接字的实体。协议栈在执行操作时需要参阅这些控制信息。例如发送数据时需要看一看套接字中的通信对象的IP地址和端口号。发送数据后</p><p>套接字中必须要记录是否已经收到响应，以及发送数据后经过多长时间，才能根据这些信息按照需要进行重发操作。</p><p><strong><em>协议栈是根据套接字中记录的控制信息来工作的。</em></strong></p><p>可以在操作系统里输入<strong>netstat</strong>命令来显示套接字内容。每一行相当于一个套接字，当创建套接字时，就会在这里增加一行新的控制信息。</p><h4 id="调用socket时的操作"><a href="#调用socket时的操作" class="headerlink" title="调用socket时的操作"></a>调用socket时的操作</h4><p>看过套接字的样子后，我们看一看当浏览器调用socket、connect等Socket库中的程序组件时，协议栈内部时如何工作的。<img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190910085021177.png" alt="image-20190910085021177" style="zoom:50%;"></p><p><strong>创建套接字的阶段</strong></p><p>如图2.3(1)所示，应用程序调用socket申请创建套接字，协议栈根据应用程序的申请执行创建套接字的操作。在这个过程中协议栈首先会分配用于存放一个套接字所需的内存空间，相当于为控制信息准备一个容器。但是光有容器并没有什么用，还需要往里边存入控制信息。套接字刚刚创建时，数据收发操作还没有开始，因此需要在套接字的内存空间写入表示这一初始状态的控制信息。</p><p>接下来，需要将这个套接字的描述符告知应用程序。描述符相当于用来区分协议栈中的多个套接字的号码牌。</p><p>收到描述符之后，应用程序在向协议栈进行收发数据委托时就需要提供这个描述符。由于套接字中记录了通信双方的信息以及通信处于怎样的状态，所以只要通过描述符确定了相应的套接字，协议栈就能获取所有相关信息，这样一来，应用程序就不需要每次都告诉协议栈应该和谁进行通信了。</p><h3 id="连接服务器"><a href="#连接服务器" class="headerlink" title="连接服务器"></a>连接服务器</h3><p><strong>连接实际上是通信双方交换控制信息</strong></p><p>2.3(1)步骤套接字刚刚创建完成的时候，套接字里写的只是初始数据，并不知道通信的对方是谁，在这个状态下，即便是应用程序要求发送数据，协议栈也不知道数据该发给谁。因为调用socket来创建套接字时，这些信息并没有传递给协议栈。因此我们在connect时需要把服务器的IP地址和端口号等信息告诉协议帧，这是连接操作的目的之一。</p><p>服务器那边又是怎样的情况呢？服务器上也会创建套接字，服务器上的协议栈和客户端上的一样，只创建套接字是不知道和谁进行通信的。<strong>而且和客户端不同的是，在服务器上连应用程序也不知道通信的对象是谁，于是需要客户端告知服务器必要的信息</strong>，这也是连接操作的目的之一。</p><p><strong>双方交换的的控制信息，除了IP和端口，还有其他。</strong></p><p><strong>此外，当执行数据收发操作的时候，我们还需要一块用于临时存放要收发的数据的内存空间，这块内存空间称为缓冲区，它也是在连接操作的过程中分配的。</strong></p><p><strong>通信操作中使用的控制信息分为两类。</strong></p><p><strong>（1）头部中记录的信息</strong></p><p><strong>（2）套接字（协议栈中的内存空间）中记录的信息</strong></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911080009925.png" alt="TCP头部格式" style="zoom:50%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911080131316.png" alt="image-20190911080131316" style="zoom:50%;"></p><h4 id="连接操作的实际过程"><a href="#连接操作的实际过程" class="headerlink" title="连接操作的实际过程"></a>连接操作的实际过程</h4><p>这个过程是从应用程序调用Socket库的connect开始的(图2.3(2))。</p><p>connect(&lt;描述符&gt;,&lt;服务器IP地址和端口号&gt;)</p><p>上面的提供的服务器IP地址和端口号，会传递给协议栈中的TCP模块。然后TCP模块会与该IP地址对应的对象(服务器TCP模块)交换控制信息，这一交互过程包括下面几个步骤。</p><ol><li>客户端先创建一个包含表示开始数据收发操作的控制信息的头部。重点关注的是发送方和接收方的端口号，可以通过端口号准确的找到服务器的套接字。然后将头部控制位的SYN比特值设置为1</li><li>当TCP头创建好了后，TCP模块把信息传递给IP模块并委托他进行发送。IP模块执行发送网络包操作，网络包到达服务器，服务器IP模块会把收到的数据传递给TCP模块，服务器根据头部的端口号找到对应的套接字，找到后写入相应的信息，并将状态改为正在连接。返回响应，SYN、ACK比特位设置为1</li><li>然后网络包到达客户端，通过IP模块到达TCP模块，并通过TCP头部的信息确认连接服务器的操作是否成功。如果SYN为1表示连接成功，这时会向套接字中写入服务器的IP地址、端口号等信息，同时将状态改为连接完毕。客户端返回ACK，ACK比特设置为1</li></ol><h3 id="收发数据"><a href="#收发数据" class="headerlink" title="收发数据"></a>收发数据</h3><p>协议栈并不关心应用程序传来的数据是什么内容。应用程序在调用write时会指定发送数据的长度，在协议栈看来，要发送的数据就是一定长度的二进制字节码而已。</p><p>协议栈并不是一收到数据就会马上发出去，而是将数据放在内部的发送缓冲区中，并等待应用程序的下一段数据，因此需要在数据累计到一定的量在发出去，这样做是为了<strong>防止网络效率下降</strong></p><p>关于协议栈发送数据的时机有两个要素决定</p><ul><li><p>第一个判断要素是每个网络包能容纳的数据长度，协议栈会根据一个叫做<strong>MTU</strong>的参数来进行判断。MTU表示一个网络包的最大长度，在以太网中一般是1500字节。MTU是包含头部的总长度，因此需要减去头部，TCP和IP头部加起来一共40字节，所以MTU减去这个长度就是一个网络包中能容纳的最大数据长度，这一长度叫做<strong>MSS</strong>。当协议栈收到的数据长度超过或者接近MSS时再发出去，就可以避免发送大量小包的问题了。</p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911084141190.png" alt="image-20190911084141190" style="zoom:33%;"></p></li><li><p>另一个判断要素是时间。当应用程序发送数据频率不高的时候，如果每次都等到接近MSS时在发送，可能会因为等待时间太长而造成发送延迟，这种情况下即便缓冲区中的数据长度没有达到MSS，也应该果断发送出去。为此协议栈内部有一个计时器，当经过一定时间后，就把网络包发送出去。</p></li></ul><p>这两个要素其实是相互矛盾的。长度优先，效率会提高，但是会产生延迟；想反时间优先，延迟降低，网络效率降低。TCP协议规格中并没有告诉我们怎么才能平衡，因此如果判断是由协议栈的开发者来决定的，正是这个原因，不同种类和版本的操作系统在相关操作上也就存在差异。仅靠协议栈来判断发送时机也会带来问题，因此协议栈给应用程序保留了控制发送时机的余地，应用程序可以指定一些选项。</p><p><strong>对较大的数据进行拆分</strong>：当HTTP消息长度超过一个网络包所能容纳的数据量时，就需要进行拆分了，如图所示。</p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911085729794.png" alt="image-20190911085729794" style="zoom:33%;"></p><p><strong>使用ACK号确认网络包已收到</strong>：TCP模块在拆分数据时，会先算好每一块数据相当于从头开始的第几个字节，这个值就是<strong>“序号”</strong>，然后长度也需要告知接收方，不过这个不是放在TCP头部中，而是用数据包长度减去头部的长度就可以得到数据的长度。接收方回复的ACK号代表，到第XX字节前的数据我都已经收到了，所以已收到多少字节加1作为ACK号。<strong>实际通信中序号并不是从1开始的，而是需要随机计算出一个初始值，防止能预测序号发动攻击，如果序号是随机的，为了让对方清楚初始值，所以在建立连接的过程中，需要把序号告诉对方。</strong><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911090123203.png" alt="image-20190911090123203" style="zoom: 50%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911091137967.png" alt="image-20190911091137967" style="zoom:50%;"></p><p><strong>通过“序号”和“ACK号”可以确认接收方是否收到了网络包。</strong>TCP采用这样的方式确认对方是否收到数据，在得到对方确认之前，发送过的包都会保存在发送缓冲区，如果没有返回某些包对应的ACK号，那么就重新发送这些包。这一机制非常强大，有了这一机制，我们就不需要在其他地方对错误进行补救了。因此网卡、集线器、路由器都没有错误补偿机制，一旦检测到错误就直接丢弃相应的包。应用程序和IP模块也一样只管自顾自的发送数据就好了。</p><p><strong>根据网络包平均往返时间调整ACK号等待的时间，等待时间需要设置一个合适的值，不能太长也不能太短，太长会延迟，太短会增加重传对于拥堵的网络无疑是雪上加霜，因此，TCP采用了动态调整等待时间的做法，这个等待时间是根据ACK号返回所需要的时间来判断的。具体来说，TCP会在发送数据的过程中持续测量ACK号的返回时间，如果ACK号返回变慢，则相应延长等待时间；相对地，如果ACK号马上就能返回，则相应缩短等待时间。</strong></p><p><strong>每发送一个包就等待一个ACK号的方式是最简单的也是最容易理解的，但是在等ACK号的这段时间内，如果什么也不做就太浪费了。为了减少这样的浪费，TCP采用滑动窗口方式来管理数据发送和ACK号的操作。所谓滑动窗口，就是在发送完一个包之后，不等ACK号返回，而是直接发送后续一系列的包。如图所示</strong></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911204642815.png" alt="image-20190911204642815"></p><p>但是滑动窗口的方式发送的数据接收方处理不过来怎么办呢？</p><p>接收方的TCP模块收到包后，会先将数据存放到接收缓存区中。接收方需要计算ACK号，将数据块组装起来还原成原本的数据并传递给应用程序，如果数据到达的速率比处理这些数据并传递给应用程序的速率还快，那么缓存区中的数据就会越堆越多，最后就会溢出。</p><p>怎么解决呢？</p><p>首先接收方要告诉发送方自己最多能接收多少数据，然后发送方根据这个值对数据发送操作进行控制，这就是滑动窗口方式的基本思路。</p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190911205602646.png" alt="image-20190911205602646" style="zoom:67%;"></p><p><strong>ACK与窗口的合并</strong></p><p>要想提高发送数据的效率，还要考虑另一个问题，就是返回ACK号和更新窗口时机。</p><p>当收到的数据刚刚填入缓冲区时，其实没必要每次都像发送方更新窗口大小，发送方可以自己算出来。</p><p>更新窗口的时机是，<strong>接收方从缓冲区中取出数据传递给应用程序的时候</strong>。这时缓冲区容量增加，发送方是不知道的，所以需要告诉发送方。</p><p>ACK号的发送时机是，<strong>接收方收到数据时，确认内容没有问题，就会向发送方返回ACK号</strong>，因此我们可以认为收到数据之后马上就进行这一操作。</p><p>如果将前面两个因素结合起来看，每收到一个包接收方需要发送两个包，数据到达接收方发ACK包，数据传递给应用程序需要通知发送方更新窗口大小。这样一来，导致网络效率下降。</p><p>因此，接收方发送ACK号和窗口更新时，并不会立马发送，而是等待一段时间，这个过程可能会出现其他通知操作，这样就可以把多个通知合并在一个包里发送了。</p><p>举个大例子，在等待ACK号的时候正好需要更新窗口，这样就可以把ACK和窗口更新放在一个包里发送了。</p><p>当需要连续发送多个ACK号时，也可以减少包的数量，这是因为ACK号表示的是已经收到的数据量，也就是说，它是告诉发送方目前已接收的数据的最后的位置在哪里，因此当需要发送多个ACK号时，只要发送最后一个ACK就行了，中间的可以省略。</p><p>同样当需要连续发送多个窗口更新时也可以减少包的数量，这种情况和ACK号一样，可以省略中间过程，只要发送最终结果就可以了。</p><p><strong>协议栈接收数据的操作过程</strong></p><ol><li>首先，协议栈会检查收到的数据块和TCP头部的内容，判断数据是否有丢失，如果没有问题返回ACK号。</li><li>然后，协议栈将数据块暂存到接收缓冲区中，并将数据块按顺序连接起来还原出原始数据</li><li>最后，将数据交给应用程序，具体来说，协议栈会将接收到的数据复制到应用程序指定的内存地址中，然后将控制流程交给应用程序。将数据交给应用程序后，协议栈还需要找到合适的时机向发送方发送窗口更新。</li></ol><h3 id="从服务器断开并删除套接字"><a href="#从服务器断开并删除套接字" class="headerlink" title="从服务器断开并删除套接字"></a>从服务器断开并删除套接字</h3><p>客户端和服务器都可以发起断开过程，这里我们以服务器一方发起断开过程为例进行讲解。</p><p><strong>断开过程</strong></p><ol><li><p>首先服务器一方的应用程序会调用Socket库的close程序。然后，服务器的协议栈生成包含断开信息的TCP头部，就是将控制位中FIN比特设置为1。接下来协议栈委托IP模块向客户端发送数据。同时服务器套接字中也会记录下断开操作的相关信息。</p></li><li><p>当客户端收到服务器发来的FIN为1的TCP头部时，客户端的协议栈会将自己的套接字标记进入断开操作状态。然后告知服务器已经收到FIN为1的包，客户端会向服务器返回一个ACK号。这些操作完成后，协议栈就等着应用程序来取数据了。过了一会儿，应用程序就会调用read来读取数据。这时，协议栈不会向应用程序传递数据，而是告知应用程序来自服务器的数据已经全部收到了。</p></li><li><p>根据规则，服务器返回请求之后，web通信操作就全部结束了，因此只要收到服务器返回的所有数据，客户端的操作也就随之结束了。因此客户端应用程序会调用close来结束数据收发操作，这时客户端的协议栈也会和服务器一样，生成一个FIN比特为1的TCP包，然后委托IP模块发送给服务器。</p></li><li><p>一段时间后服务器就会返回ACK号，到这里，客户端和服务器的通信就全部结束了。</p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190912092110216.png" alt="image-20190912092110216" style="zoom:50%;"></p></li></ol><p><strong>删除套接字</strong></p><p>通信结束后，按理说用来通信的套接字不会再用了，这时我们就可以删除这个套接字了。不过实际上不会立马删除，而是等待一段时间然后删除。</p><p>等待这段时间是为了防止误操作，引发误操作的原因有好多，这里举一个最容易理解的例子。</p><p>假设客户端先发起断开，则断开的操作顺序如下。</p><p>（1）客户端发送FIN</p><p>（2）服务器返回ACK号</p><p>（3）服务器发送FIN</p><p>（4）客户端返回ACK号</p><p>假设最后客户端返回的ACK号丢了，这时服务器没有收到ACK号，可能会重发一次FIN。如果这时客户端的套接字被删了，那套接字保存的控制信息也就跟着消失了，套接字对应的端口就会被释放出来。这时如果别的应用程序要创建套接字，新套接字碰巧又被分配给了同一个接口，而服务器重发的FIN刚好到达，于是这个FIN就会错误的跑到新套接字里面，新套接字就开始执行断开操作了。<strong>之所以不马上删除套接字，就是为了防止这样的误操作</strong>。</p><p>那么应该等待多长时间呢，这和包重传的操作方式有关。网络包丢失之后会进行重传，这个操作通常要持续几分钟。如果重传了几分钟之后依然无效，则停止重传。但在这段时间内，网络中可能存在重传的包，因此需要等待到重传完全结束。协议中对于这个等待时间没有明确规定，一般来说会等待几分钟之后在删除套接字。</p><h3 id="数据收发操作小结"><a href="#数据收发操作小结" class="headerlink" title="数据收发操作小结"></a>数据收发操作小结</h3><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190912094359684.png" alt="image-20190912094359684" style="zoom: 80%;"></p><h3 id="IP与以太网的包收发操作"><a href="#IP与以太网的包收发操作" class="headerlink" title="IP与以太网的包收发操作"></a>IP与以太网的包收发操作</h3><h4 id="包的基本结构"><a href="#包的基本结构" class="headerlink" title="包的基本结构"></a>包的基本结构</h4><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190916212522816.png" alt="image-20190916212522816" style="zoom: 33%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190916212625561.png" alt="image-20190916212625561" style="zoom:33%;"></p><p>发送方的网路设备会负责创建包，接下来，包会被发往最近的网络转发设备。当到达最近的网络转发设备之后，转发设备会根据头部中的信息判断接下来应该发往哪里。这个过程需要一张表，这张表里记录了每一个地址对应的发送方向，也就是按照头部里记录的目的地址在表里进行查询，并根据查到的信息判断接下来发往哪个方向。接下来，包在向目的地移动的过程中，又会到达下一个转发设备。这样经过多个转发设备的接力以后，包就到达最终的设备。</p><p>（1）路由器根据目标地址判断下一个路由器的位置。</p><p>（2）集线器在子网中将网络包传输到下一个路由。</p><p>实际上，集线器是按照以太网规则传输包的设备，而路由器是按照IP规则传输包的设备，因此我们可以作如下理解。</p><p>（1）IP协议根据目标地址判断下一个IP转发设备的位置。</p><p>（2）子网中的以太网协议将包传输到下一个转发设备</p><p>TCP/IP包包含如下两个头部。</p><p>（a）MAC头部（用于以太网协议）</p><p>（b）IP头部（用于IP协议）</p><p>传输过程中ip头目标ip地址不变，mac头目标地址不断变化位下一个路由器的mac地址，更准确的说，收到包的时候MAC头会被舍弃，而当再次发送的时候又会加上包含新MAC地址的新MAC头部。如图所示<img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190916214241427.png" alt="image-20190916214241427" style="zoom:33%;"></p><h4 id="包收发概览"><a href="#包收发概览" class="headerlink" title="包收发概览"></a>包收发概览</h4><p>包收发操作的起点是TCP模块委托IP模块发送包的操作（如下图所示）。这个委托的过程就是TCP模块在数据块的前面加上TCP头部，然后整个传递给IP模块，这部分就是网络包的内容。收到委托后IP模块负责添加MAC头部和IP头部，接下来，封装好的包会被交给网卡。传递给网卡的网络包是由一连串0和1组成的数字信息，网卡将这些数字信息转换为电信号或光信号，并通过网线发送出去，然后这些信号就会到达集线器、路由器等转发设备，再由转发设备一步一步地送达接收方。<img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917072918732.png" alt="image-20190917072918732" style="zoom:33%;"></p><h4 id="生成IP头部"><a href="#生成IP头部" class="headerlink" title="生成IP头部"></a>生成IP头部</h4><p>IP模块接收TCP模块的委托负责包的收发操作，它会生成IP头部并附加在TCP头部前。最重要的是接收方IP地址，这个地址是TCP模块告知的，而TCP又是在执行连接操作时从应用程序那里获得的。发送方IP地址需要判断发送所使用的网卡，并填写该网卡的IP地址。<img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917074009477.png" alt="image-20190917074009477" style="zoom:50%;"></p><h4 id="生成以太网用的MAC头部"><a href="#生成以太网用的MAC头部" class="headerlink" title="生成以太网用的MAC头部"></a>生成以太网用的MAC头部</h4><p>IP头部中的接收方IP地址表示网络包的目的地，通过这个地址我们就可以判断将包发送到哪里，但在以太网的世界里，TCP/IP这个思路是行不通的。以太网判断网络包的目的地时和TCP/IP的方式不同，因此必须采用相匹配的方式才能在以太网中将包发往目的地，而<strong>MAC头部就是干这个用的</strong>。</p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917075510103.png" alt="image-20190917075510103" style="zoom:50%;"></p><p><strong>IP模块根据路由表Gateway栏的内容判断应该把包发给谁</strong>，解释看书内章节2.5.3</p><h4 id="通过ARP查询目标路由器的MAC地址"><a href="#通过ARP查询目标路由器的MAC地址" class="headerlink" title="通过ARP查询目标路由器的MAC地址"></a>通过ARP查询目标路由器的MAC地址</h4><p>在以太网中，有一种叫作广播的方法，可以把包发给连接在同一以太网的所有设备。ARP就是利用了广播。</p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917080554549.png" alt="image-20190917080554549" style="zoom:50%;"></p><p>为了防止每次发送包都用ARP查询一次，网络中就会增加很多ARP包，因此我们会将查询结果放到一块叫做ARP缓存的内存空间中留着以后用。在发送包时，先查下缓存，虽然能减少ARP包的数量，但是如果IP地址发生变化时，ARP缓存的内容就会和现实产生差异。为了防止这种问题的发生，ARP缓存中的值过一段时间就会被删除。</p><h4 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h4><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917081545274.png" alt="image-20190917081545274" style="zoom:50%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917082139686.png" alt="image-20190917082139686"></p><h4 id="以太网基本知识（建议看原文2-5-6-2-5-11）"><a href="#以太网基本知识（建议看原文2-5-6-2-5-11）" class="headerlink" title="以太网基本知识（建议看原文2.5.6-2.5.11）"></a>以太网基本知识（建议看原文2.5.6-2.5.11）</h4><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917081202264.png" alt="image-20190917081202264" style="zoom: 33%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917081228940.png" alt="image-20190917081228940" style="zoom: 33%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917081321242.png" alt="image-20190917081321242" style="zoom:50%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917081354405.png" alt="image-20190917081354405" style="zoom: 33%;"></p><p><img src="/2019/09/10/《网络是怎么连接的》第二章笔记/images/image-20190917081414596.png" alt="image-20190917081414596" style="zoom:33%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;用电信号传输TCP-IP数据—–探索协议栈和网卡&quot;&gt;&lt;a href=&quot;#用电信号传输TCP-IP数据—–探索协议栈和网卡&quot; class=&quot;headerlink&quot; title=&quot;用电信号传输TCP/IP数据—–探索协议栈和网卡&quot;&gt;&lt;/a&gt;用电信号传输TCP/IP数据
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《网络是怎么连接的》第一章笔记</title>
    <link href="http://shiyangtao.github.io/2019/08/30/%E3%80%8A%E7%BD%91%E7%BB%9C%E6%98%AF%E6%80%8E%E4%B9%88%E8%BF%9E%E6%8E%A5%E7%9A%84%E3%80%8B%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://shiyangtao.github.io/2019/08/30/《网络是怎么连接的》第一章笔记/</id>
    <published>2019-08-30T00:00:05.000Z</published>
    <updated>2019-11-08T09:01:05.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="浏览器生成消息"><a href="#浏览器生成消息" class="headerlink" title="浏览器生成消息"></a>浏览器生成消息</h2><h3 id="生成HTTP请求"><a href="#生成HTTP请求" class="headerlink" title="生成HTTP请求"></a>生成HTTP请求</h3><p><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917082427342.png" alt="image-20190917082427342" style="zoom: 50%;"></p><h4 id="一条请求消息中只能写一个URI。如果需要获取多个文件，必须对每个文件单独发送1条请求。"><a href="#一条请求消息中只能写一个URI。如果需要获取多个文件，必须对每个文件单独发送1条请求。" class="headerlink" title="一条请求消息中只能写一个URI。如果需要获取多个文件，必须对每个文件单独发送1条请求。"></a>一条请求消息中只能写一个URI。如果需要获取多个文件，必须对每个文件单独发送1条请求。</h4><h3 id="向DNS服务器查询Web服务器的IP地址"><a href="#向DNS服务器查询Web服务器的IP地址" class="headerlink" title="向DNS服务器查询Web服务器的IP地址"></a>向DNS服务器查询Web服务器的IP地址</h3><h4 id="域名和IP地址并用的理由"><a href="#域名和IP地址并用的理由" class="headerlink" title="域名和IP地址并用的理由"></a>域名和IP地址并用的理由</h4><p>问：为啥不直接使用IP地址，而是要用域名</p><p>答：这样其实也是能正常工作的，想想日常上网，你查某个网站肯定是输入域名比如baidu,tianmao之类的，你肯定记不住一长串的IP地址</p><p>问：那干脆不用IP地址，用域名来确定通信对象不好吗</p><p>答：应该也能实现，但是从运行效率上来看，这并不能算是一个好主意。互联网中存在无数的路由器，他们之间相互配合，根据IP地址来判断应该把数据传送到什么地方。IP地址的长度为32bit，也就是4个字节，相对的域名最短也要几十个字节，增加了路由器的负担。</p><h4 id="DNS解析器"><a href="#DNS解析器" class="headerlink" title="DNS解析器"></a>DNS解析器</h4><p>查询DNS其实是想DNS服务器发送根据域名查询IP的请求，对于DNS服务器我们计算机上的有DNS客户端，相当于DNS客户端的部分我们称为<strong>DNS解析器</strong></p><p>解析器其实就是一段程序，它包含在操作系统的Socket库中，Socket库是用于调用网络功能的程序组件集合。</p><p>根据域名查询IP地址时，浏览器会使用Socket库中的解析器。</p><p>解析器内部原理图<img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917082803603.png" alt="image-20190917082803603" style="zoom:50%;"></p><p>向DNS服务器发送消息时，我们当然需要知道DNS服务器的IP地址。这个是事先设置好的。如图window系统</p><p><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083122504.png" alt="image-20190917083122504" style="zoom:50%;"></p><h3 id="全世界DNS服务器的大接力"><a href="#全世界DNS服务器的大接力" class="headerlink" title="全世界DNS服务器的大接力"></a>全世界DNS服务器的大接力</h3><h4 id="DNS查询消息"><a href="#DNS查询消息" class="headerlink" title="DNS查询消息"></a>DNS查询消息</h4><p>包含3种消息</p><p>域名：服务器、邮件服务器</p><p>Class：最早设计DNS方案时，DNS在互联网以外的其他网络中的应用也被考虑到了，而Class就是用来识别网络的信息。不过如今除了互联网没有其他网络了，因此Class的值永远代表互联网的IN</p><p>记录类型：表示域名对应何种类型的记录。例如当类型为A时，表示域名对应的是IP地址;当类型为MX时，表示域名对应的是邮件服务器。不同的记录类型，服务器返回给客户端的信息也会不同。</p><p><strong>DNS服务器会从域名与IP地址的对照表中查找相应的记录，并返回IP地址</strong></p><p><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083148571.png" alt="image-20190917083148571" style="zoom:50%;"></p><h4 id="域名的层次结构"><a href="#域名的层次结构" class="headerlink" title="域名的层次结构"></a>域名的层次结构</h4><p>问题1：</p><p>互联网中存在不计其数的IP地址，将这些IP地址保存到一台DNS服务器是不可能的，因此肯定会存在在DNS服务器中找不到查询信息的情况。</p><p>解决方案1：</p><p>DNS服务器按照域名以分层次的结构来保存到多个DNS服务器上。</p><p>比如域名<a href="http://www.lab.glasscom.com,这里的点号代表了不同层级之间的界限。在域名中**越靠右的位置表示层级越高**，其中，每一层级的部分称为域。因此com域的下一层是glasscom域,在下一层是lab域,在下面才是www这个名字。" target="_blank" rel="noopener">www.lab.glasscom.com,这里的点号代表了不同层级之间的界限。在域名中**越靠右的位置表示层级越高**，其中，每一层级的部分称为域。因此com域的下一层是glasscom域,在下一层是lab域,在下面才是www这个名字。</a></p><p><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083238747.png" alt="image-20190917083238747" style="zoom:50%;"><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083326388.png" alt="image-20190917083326388"><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083238747.png" alt="image-20190917083238747" style="zoom:50%;"><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083326388.png" alt="image-20190917083326388"></p><p>负责下级域的DNS服务器的IP地址注册到他们上级DNS服务器中，然后上级在注册到更上级的DNS服务器中，以此类推。</p><p><strong>根域</strong>：似乎com、net这些域就是最顶层了，他们各自负责保存下级DNS服务器的信息，但实际上他们上层还有一级域，称为根域，可以认为<a href="http://www.lab.glasscom.com" target="_blank" rel="noopener">www.lab.glasscom.com</a>. 最后那个.代表根域但是一般不写最后的点，因此根域往往被忽略，根域的DNS服务器保存着com、net等DNS服务器的信息。所以我们从根域开始一路顺藤摸瓜找到任意域的DNS服务器，除此之外还需要完成另一项工作，那就是将根域的DNS服务器信息保存在互联网中所有的DNS服务器中。这样一来客户端找到任意一台DNS服务器，就可以通过他找到根域DNS服务器，然后在顺藤摸瓜找到位于下层的目标服务器。</p><h3 id="委托协议栈发送消息"><a href="#委托协议栈发送消息" class="headerlink" title="委托协议栈发送消息"></a>委托协议栈发送消息</h3><p>收发数据操作之前双方需要建立管道。建立管道的关键在于管道两端的数据入口，这些入口称为<strong>套接字</strong>。</p><p>我们需要先创建套接字，然后将套接字连接起来形成管道。</p><p><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083405442.png" alt="image-20190917083405442"></p><p><img src="/2019/08/30/《网络是怎么连接的》第一章笔记/images/image-20190917083436355.png" alt="image-20190917083436355"></p><ol><li>创建套接字阶段</li></ol><p>&lt;描述符&gt; = socket(&lt;使用IPV4&gt;,&lt;流模式&gt;, ….);</p><p>创建套接字调用Socket库中的socket程序组件就可以了，套接字创建完成后，协议栈会返回一个描述符，应用程序会将收到的描述符放在内存中。描述符是用来识别不同的套接字的。<strong>应用程序是通过“描述符”这一类号码牌的东西来识别套接字的</strong></p><ol start="2"><li>连接阶段:把管道接上去</li></ol><p>connect(&lt;描述符&gt;,&lt;服务器的IP地址和端口号&gt;,…);</p><p>关于端口号</p><p>可能大家还有疑问，既然确定连接对象的套接字需要使用端口号，那么服务器也得知道客户端套接字的端口号才行吧？这个问题怎么解决的呢？首先客户端在创建套接字时，协议栈会为这个套接字随便分配一个端口号。接下来协议栈执行连接操作时，会将这个随便分配的端口号通知给服务器。总之当连接成功后，协议栈会将对方的IP地址和端口号等信息保存在套接字中，这样我们可以开始收发数据了</p><p><strong>描述符：应用程序用来识别套接字的机制</strong></p><p><strong>IP地址和端口号：客户端和服务器之间用来识别对方套接字的机制</strong></p><ol start="3"><li>通信阶段：传递消息</li><li>断开连接：收发数据结束</li></ol><p><strong>本笔记中出现Socket、socket、套接字(英文也是socket)等看起来非常容易混淆的词，其中小写的socket表示程序组件的名称，大写字母开头的Socket表示库，而汉字“套接字”则表示管道两端的接口。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;浏览器生成消息&quot;&gt;&lt;a href=&quot;#浏览器生成消息&quot; class=&quot;headerlink&quot; title=&quot;浏览器生成消息&quot;&gt;&lt;/a&gt;浏览器生成消息&lt;/h2&gt;&lt;h3 id=&quot;生成HTTP请求&quot;&gt;&lt;a href=&quot;#生成HTTP请求&quot; class=&quot;headerli
      
    
    </summary>
    
    
      <category term="网络是怎么连接的" scheme="http://shiyangtao.github.io/tags/%E7%BD%91%E7%BB%9C%E6%98%AF%E6%80%8E%E4%B9%88%E8%BF%9E%E6%8E%A5%E7%9A%84/"/>
    
  </entry>
  
  <entry>
    <title>《趣谈网络协议》第10-13讲笔记</title>
    <link href="http://shiyangtao.github.io/2019/08/19/%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E7%AC%AC10-13%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    <id>http://shiyangtao.github.io/2019/08/19/《趣谈网络协议》第10-13讲笔记/</id>
    <published>2019-08-19T12:01:41.000Z</published>
    <updated>2020-01-15T23:30:08.563Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TCP和UDP的区别"><a href="#TCP和UDP的区别" class="headerlink" title="TCP和UDP的区别"></a>TCP和UDP的区别</h3><p><strong>TCP面向连接，UDP无连接</strong>，所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。</p><p><strong>TCP提供可靠交付</strong>，通过 TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达。UDP不保证不丢失，不保证顺序</p><p><strong>TCP 是面向字节流的</strong>。发送的时候发的是一个流，没头没尾。IP 包可不是一个流，而是一个个的 IP 包。之所以变成了流，这也是 TCP 自己的状态维护做的事情。而UDP 继承了 IP 的特性，基于数据报的，一个一个地发，一个一个地收。</p><p><strong>TCP是有拥塞控制的</strong>。当它意识到包丢弃了或者网络不好，就会根据情况调整自己的行为，决定是不是要发慢点。</p><p>UDP就不会，应用让我发我就发。</p><p><strong>TCP是一个有状态的服务</strong>，里面精确的精确的记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点就不行。而UDP是无状态的服务。</p><p>我们可以这样比喻，如果 MAC 层定义了本地局域网的传输行为，IP 层定义了整个网络端到端的传输行为，这两层基本定义了这样的基因：网络传输是以包为单位的，二层叫帧，网络层叫包，传输层叫段。我们笼统地称为包。包单独传输，自行选路，在不同的设备封装解封装，不保证到达。基于这个基因，生下来的孩子 UDP 完全继承了这些特性，几乎没有自己的思想。</p><h3 id="UDP包头"><a href="#UDP包头" class="headerlink" title="UDP包头"></a>UDP包头</h3><p>发送的时候，我知道我发的是一个 UDP 的包，收到的那台机器咋知道的呢？所以在 IP 头里面有个 8 位协议，这里会存放，数据里面到底是 TCP 还是 UDP，当然这里是 UDP。</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190819205223499.png" alt="image-20190819205223499"></p><p>当我们看到 UDP 包头的时候，发现的确有端口号，有源端口号和目标端口号。因为是两端通信嘛，这很好理解。但是你还会发现，UDP 除了端口号，再没有其他的了。和下两节要讲的 TCP 头比起来，这个简直简单得一塌糊涂啊！</p><h3 id="UDP三大适用场景"><a href="#UDP三大适用场景" class="headerlink" title="UDP三大适用场景"></a>UDP三大适用场景</h3><ul><li>需要资源少，网络情况比较好的内网，或者丢包不敏感的应用。</li><li>不需要一对一沟通，建立连接，而是可以广播的应用。</li><li>需要处理速度快，时延低，可以容忍少量丢包，但是要求即使网络拥塞也毫不退缩，一往无前的时候。</li></ul><h3 id="TCP包头"><a href="#TCP包头" class="headerlink" title="TCP包头"></a>TCP包头</h3><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190819212153643.png" alt="image-20190819212153643"></p><p><strong>序号</strong>是解决乱序的问题，哪个包先发哪个包后发</p><p><strong>确认序号</strong>是解决不丢包的问题，发出去的包得有确认，没有确认得重新发送直到送达。</p><p><strong>状态位</strong>SYN表示发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接。</p><p><strong>窗口大小</strong>TCP要做流量控制，通信双方各声明一个窗口，标示当前自己能处理的能力，别发送的太快，也别发送的太慢</p><h3 id="总结下TCP"><a href="#总结下TCP" class="headerlink" title="总结下TCP"></a>总结下TCP</h3><ul><li>顺序问题，稳重不乱</li><li>丢包问题，承诺靠谱</li><li>连接维护，有始有终</li><li>流量控制，把握分寸</li><li>拥塞控制，知进知退</li></ul><h3 id="TCP三次握手"><a href="#TCP三次握手" class="headerlink" title="TCP三次握手"></a>TCP三次握手</h3><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190820080652723.png" alt="image-20190820080652723"></p><p>只要保证双方的包有去有回就行</p><p>三次握手除了建立连接外，主要还是为了沟通一件事，就是<strong>TCP包的序号的问题</strong>。</p><h3 id="TCP序号的问题"><a href="#TCP序号的问题" class="headerlink" title="TCP序号的问题"></a>TCP序号的问题</h3><p>序号不能都从1开始，因为A发1/2/3，三个包给B，中途B丢了，这时A掉线了重新连上B，如果序号又从1开始，发送1/2，这时之前丢的3绕回来了，B就认为这个3是下一个包，于是发生了错误。</p><p><strong>因此每个连接都有不同的序号</strong>这个起始序号是随着时间变化的可以看成一个32位的计数器</p><h3 id="TCP四次挥手"><a href="#TCP四次挥手" class="headerlink" title="TCP四次挥手"></a>TCP四次挥手</h3><p>为什么建立连接是三次握手，关闭连接确是四次挥手呢？</p><p>建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。<br>而关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。<br>举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。</p><p><a href="https://blog.csdn.net/qzcsu/article/details/72861891" target="_blank" rel="noopener">TCP的三次握手与四次挥手（详解+动图）</a></p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190820081048627.png" alt="image-20190820081048627"></p><h3 id="TCP状态机"><a href="#TCP状态机" class="headerlink" title="TCP状态机"></a>TCP状态机</h3><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190820081702591.png" alt="image-20190820081702591"></p><p>在这个图中，加黑加粗的部分，是上面说到的主要流程，其中阿拉伯数字的序号，是连接过程中的顺序，而大写中文数字的序号，是连接断开过程中的顺序。加粗的实线是客户端 A 的状态变迁，加粗的虚线是服务端 B 的状态变迁。</p><h3 id="TCP缓存"><a href="#TCP缓存" class="headerlink" title="TCP缓存"></a>TCP缓存</h3><h4 id="发送端的缓存"><a href="#发送端的缓存" class="headerlink" title="发送端的缓存"></a>发送端的缓存</h4><p>第一部分：发送并已经确认的</p><p>第二部分：发送还并且尚未确认</p><p>第三部分：没有发送，但是已经等待发送</p><p>第四部分：没有发送，并且暂时不会发送</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821073855294.png" alt="image-20190821073855294"></p><p>TCP里接收端会给发送端报一个窗口大小，叫做<strong>AdvertisedWindow</strong>,窗口大小=第二部分+第三部分</p><h4 id="接收端的缓存"><a href="#接收端的缓存" class="headerlink" title="接收端的缓存"></a>接收端的缓存</h4><p>第一部分：接收已确认,之后是已经接收了，但是还没被应用层读取的</p><p>第二部分：等待接收，也就是能承受的最大工作量。也就是<strong>AdvertisedWindow</strong></p><p>第三部分：还没接收，无法接受，因为超过了最大承受量</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821074819492.png" alt="image-20190821074819492"></p><p>其中第二部分里面，由于收到的包可能不是顺序的，会出现空挡，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。</p><h4 id="顺序与丢包问题"><a href="#顺序与丢包问题" class="headerlink" title="顺序与丢包问题"></a>顺序与丢包问题</h4><p>分析刚才接受端和发送端的图</p><p>状态如下：</p><ul><li>1、2、3没有问题达成一致</li><li>4、5接收方说ACK了，但是发送方还没有收到，有可能丢了，有可能在路上</li><li>6、7、8、9肯定都发了，但是8、9已经到了，但是6、7没到出现乱序，没办法ACK</li></ul><p>根据这个例子，知道丢包和顺序问题都可能发生。</p><p><strong>假设4的ACK收到了，5的ACK丢了，6、7的数据包丢了，咋办呢</strong></p><ol><li><p><strong>超时重试</strong>，那超时时间如何控制呢。需要<strong>自适应重传算法（Adaptive+Retransmission+Algorithm）</strong>[^1]，如果过一段时间5、6、7都超时了，发送端会重发5、6、7的包，接收端发现5收到过就会丢弃5,不回ACK[^2]，6收到回ACK，<strong>不幸的是7又丢了</strong>，当再次超时的时候，TCP的策略是<strong>超时间隔加倍，每当遇到一次超时重传的时候，都会将下一次超时时间设置为先前两倍。两次超时说明网络环境差，不宜频繁反复发送</strong>,超时重传的问题是超时周期可能相对较长，是不是有更快的方式，有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的 ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。例如，接收方发现 6、8、9 都已经接收了，就是 7 没来，那肯定是丢了，于是发送三个 6 的 ACK，要求下一个是 7。客户端收到 3 个，就会发现 7 的确又丢了，不等超时，马上重发。</p></li><li><p><strong>Selective Acknowledgment(SAK)</strong></p><p>这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。</p></li></ol><h3 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h3><p>对于包的确认中，同时会携带一个窗口大小。我们假设窗口不变，始终为9，4的ACK来的时候会右移一个，这个时候第13个包也可以发送了</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821085658843.png" alt="image-20190821085658843"></p><p>这个时候如果发送端发送过猛，会将10、11、12、13都发送了，之后停止发送，因为未发送可发送部分为0。</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821090108801.png" alt="image-20190821090108801"></p><p>这时候又收到了5的ACK，窗口又会往右滑动一格，这时才可以有更多包可以发送</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821090331167.png" alt="image-20190821090331167"></p><p>如果接收方处理的太慢，导致缓存中没有空间，可以通过确认信息修改窗口大小，甚至可以设置为0，让发送方定制发送。</p><p>我们假设一个极端情况，接收端的应用一直不读取缓存里的数据，当数据包6确认后，窗口大小不能在是9了，就要缩小一个变成8。</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821090711406.png" alt="image-20190821090711406"></p><p>这个新窗口8通过6的确认消息，到达发送端的时候，你会发现发送端窗口没有右移，而只是左边的移动了，窗口大小由9变成8</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821091011217.png" alt="image-20190821091011217"></p><p>如果接收端的应用还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，甚至变为0<img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821091137217.png" alt="image-20190821091137217"></p><p>当窗口大小通过14包的ACK到达发送端时，发送端的窗口也变为0。</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821091254307.png" alt="image-20190821091254307"></p><p>如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。</p><h3 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h3><h4 id="解决什么问题"><a href="#解决什么问题" class="headerlink" title="解决什么问题"></a>解决什么问题</h4><p>最后，我们看一下拥塞控制的问题，也是通过窗口的大小来控制的，前面的滑动窗口 rwnd 是怕发送方把接收方缓存塞满，而拥塞窗口 cwnd，是怕把网络塞满。</p><p>这里有一个公式 LastByteSent - LastByteAcked &lt;= min {cwnd, rwnd} ，是拥塞窗口和滑动窗口共同控制发送的速度。</p><p>那发送方怎么判断网络是不是满呢？这其实是个挺难的事情，因为对于 TCP 协议来讲，他压根不知道整个网络路径都会经历什么，对他来讲就是一个黑盒。</p><p>假设tcp发送的速度超过了带宽，中间设备处理不了的包就会<strong>被丢弃</strong>， 为了解决这个问题中间经过的设备会会加上缓存，处理不过来的在队列里排着，虽然避免了丢包，但是造成了<strong>超时重传</strong></p><p>于是TCP的拥塞控制住要来解决这两种现象，<strong>包丢失和超时重传</strong>，一旦出现了这些现象就说明太快了，要慢一点。<strong>TCP 的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽</strong></p><h4 id="如何解决"><a href="#如何解决" class="headerlink" title="如何解决"></a>如何解决</h4><p>如果我们通过漏斗往瓶子里灌水，我们就知道，不能一桶水一下子倒进去，肯定会溅出来，要一开始慢慢的倒，然后发现总能够倒进去，就可以越倒越快。这叫作慢启动。</p><p>一条 TCP 连接开始，cwnd 设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd 加一，于是一次能够发送两个；当这两个的确认到来的时候，每个确认 cwnd 加一，两个确认 cwnd 加二，于是一次能够发送四个；当这四个的确认到来的时候，每个确认 cwnd 加一，四个确认 cwnd 加四，于是一次能够发送八个。可以看出这是<strong>指数增长</strong>，增长到啥时候是个头呢。</p><p>涨到什么时候是个头呢？有一个值 ssthresh 为 65535 个字节，当超过这个值的时候，就要小心一点了，不能倒这么快了，可能快满了，再慢下来。</p><p>每收到一个确认后，cwnd 增加 1/cwnd，我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加 1/8，八个确认一共 cwnd 增加 1，于是一次能够发送九个，变成了<strong>线性增长</strong>。</p><p>但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。</p><p>拥塞的一种表现形式是丢包，需要超时重传，这个时候，将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。</p><p>前面讲过<strong>快速重传算法</strong>。当接收端发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速的重传，不必等待超时再重传。TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd 减半为 cwnd/2，然后 sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821173648904.png" alt="image-20190821173648904"></p><h4 id="有什么问题"><a href="#有什么问题" class="headerlink" title="有什么问题"></a>有什么问题</h4><p><strong>第一个问题</strong>是是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。</p><p><strong>第二个问题</strong>是 TCP 的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实 TCP 只要填满管道就可以了，不应该接着填，直到连缓存也填满。</p><p>为了优化这两个问题出来了<strong>TCP拥塞 BBR算法</strong>。它企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。</p><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190821174314010.png" alt="image-20190821174314010"></p><h3 id="基于TCP协议Socket函数调用过程"><a href="#基于TCP协议Socket函数调用过程" class="headerlink" title="基于TCP协议Socket函数调用过程"></a>基于TCP协议Socket函数调用过程</h3><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190822091137492.png" alt="image-20190822091137492"></p><p>服务端调用accept函数，拿出一个已经完成的连接进行处理。如果没有完成就需要等待，在服务器等待的时候，客户端可以通过connect函数发起连接。先在参数中指明要连接的IP地址和端口号，然后开始发起三次握手。内核会给客户端分配一个临时端口。一旦握手成功，服务端的accept就会返回另一个Socket。</p><p>这是经常考的知识点，就是监听的Socket和真正用来传数据的Socket是两个，一个叫做<strong>监听Socket</strong>，一个叫做<strong>已连接Socket</strong>。</p><h3 id="基于UDP协议Socket函数调用过程"><a href="#基于UDP协议Socket函数调用过程" class="headerlink" title="基于UDP协议Socket函数调用过程"></a>基于UDP协议Socket函数调用过程</h3><p><img src="/2019/08/19/《趣谈网络协议》第10-13讲笔记/images/image-20190822091728341.png" alt="image-20190822091728341"></p><p>对于 UDP 来讲，过程有些不一样。UDP 是没有连接的，所以不需要三次握手，也就不需要调用 listen 和 connect，但是，UDP 的的交互仍然需要 IP 和端口号，因而也需要 bind。UDP 是没有维护连接状态的，因而不需要每对连接建立一组 Socket，而是只要有一个 Socket，就能够和多个客户端通信。也正是因为没有连接状态，每次通信的时候，都调用 sendto 和 recvfrom，都可以传入 IP 地址和端口。</p><p>[^1]:估计往返时间，需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个值，而且这个值还是要不断变化的，因为网络状况不断的变化。除了采样 RTT，还要采样 RTT 的波动范围，计算出一个估计的超时时间。由于重传时间是不断变化的，我们称为<br>[^2]:不会有问题。接收方的 ACK 应答，不是和发送方的数据包一一对应的，也不是一个个回复。当某个包做了 ACK 应答，表示它之前的所有包都收到了。所以这里的例子里，5 虽然没有 ACK 应答，但是之后收到的 6、7、8，只需要 ACK 应答 8 就表示之前的包都收到并确认了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;TCP和UDP的区别&quot;&gt;&lt;a href=&quot;#TCP和UDP的区别&quot; class=&quot;headerlink&quot; title=&quot;TCP和UDP的区别&quot;&gt;&lt;/a&gt;TCP和UDP的区别&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;TCP面向连接，UDP无连接&lt;/strong&gt;，所谓的建立连
      
    
    </summary>
    
    
      <category term="网络" scheme="http://shiyangtao.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>《趣谈网络协议》第3-7讲笔记</title>
    <link href="http://shiyangtao.github.io/2019/08/14/%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E7%AC%AC3-7%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    <id>http://shiyangtao.github.io/2019/08/14/《趣谈网络协议》第3-7讲笔记/</id>
    <published>2019-08-14T07:00:10.000Z</published>
    <updated>2019-10-23T07:23:37.013Z</updated>
    
    <content type="html"><![CDATA[<h3 id="ifconfig-和-ip-addr的区别"><a href="#ifconfig-和-ip-addr的区别" class="headerlink" title="ifconfig 和 ip addr的区别"></a>ifconfig 和 ip addr的区别</h3><p>这是有关net-tools 和 iproute2的故事，后续回答这个问题</p><h3 id="IP是什么"><a href="#IP是什么" class="headerlink" title="IP是什么"></a>IP是什么</h3><p>ip地址是一个网卡在网络世界的通信地址，相当于我们现实世界的地址。</p><h3 id="IPv6出现原因"><a href="#IPv6出现原因" class="headerlink" title="IPv6出现原因"></a>IPv6出现原因</h3><p>这个地址被分为四个部分，每个部分8个bit，所以一共32位，所以IP地址的数量很快就不够用了。当时设计IP地址时，哪里知道会有这么多计算机，因为不够用所以，就有了IPv6，这个有128位。</p><h3 id="IP分类"><a href="#IP分类" class="headerlink" title="IP分类"></a>IP分类</h3><p>本来 32 位的 IP 地址就不够，还被分成了 5 类。现在想想，当时分配地址的时候，真是太奢侈了。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190814200829762.png" alt="image-20190814200829762"></p><p>A/B/C类分网络号和主机号两部分</p><h3 id="IP-A-B-C-类包含的主机数量"><a href="#IP-A-B-C-类包含的主机数量" class="headerlink" title="IP A/B/C 类包含的主机数量"></a>IP A/B/C 类包含的主机数量</h3><table><thead><tr><th style="text-align:center">类别</th><th style="text-align:center">IP地址范围</th><th style="text-align:center">最大主机数</th><th style="text-align:center">私有IP地址范围</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">0.0.0.0-127.255.255.255</td><td style="text-align:center">16,777,214</td><td style="text-align:center">10.0.0.0-10.255.255.255</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">128.0.0.0-191.255.255.255</td><td style="text-align:center">65534</td><td style="text-align:center">172.16.0.0-172.31.255.255</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">192.0.0.0-223.255.255.255</td><td style="text-align:center">254</td><td style="text-align:center">192.168.0.0-192.168.255.255</td></tr></tbody></table><p>C类主机数太少了只有254个，一般大网吧都满足不了。B类又太多了，6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。为了解决C类主机数太少，B类主机数太多的窘状，出现了CIDR</p><h3 id="无类型域间选路（CIDR）"><a href="#无类型域间选路（CIDR）" class="headerlink" title="无类型域间选路（CIDR）"></a>无类型域间选路（CIDR）</h3><p>这种方式打破了原来设计的分类方案，将原来的ip地址一分为二，前边是<strong>网络号</strong>，后边是<strong>主机号</strong></p><p>从哪里分呢？你如果注意观察的话可以看到，10.100.122.2/24，这个 IP 地址中有一个斜杠，斜杠后面有个数字 24。这种地址表示形式，就是 CIDR。后面 24 的意思是，32 位中，前 24 位是网络号，后 8 位是主机号。</p><p>CIDR里有几个概念</p><p><strong>广播地址</strong>:    10.100.122.255。如果发送这个地址，所有 10.100.122 网络里面的机器都可以收到</p><p><strong>子网掩码</strong>:     255.255.255.0</p><p><strong>子网掩码&amp;IP地址=网络号</strong>:    10.100.122.2&amp;255.255.255.0=10.100.122.0</p><p>CIDR 可以用来判断是不是本地人，网络号一样就是本地人</p><h3 id="公有IP和私有IP"><a href="#公有IP和私有IP" class="headerlink" title="公有IP和私有IP"></a>公有IP和私有IP</h3><p>平时我们看到的数据中心里，办公室、家里或学校的 IP 地址，一般都是私有 IP 地址段。因为这些地址允许组织内部的 IT 人员自己管理、自己分配，而且可以重复。因此，你学校的某个私有 IP 地址段和我学校的可以是一样的。</p><p>公有 IP 地址有个组织统一分配，你需要去买。如果你搭建一个网站，给你学校的人使用，让你们学校的 IT 人员给你一个 IP 地址就行。但是假如你要做一个类似网易 163 这样的网站，就需要有公有 IP 地址，这样全世界的人才能访问。</p><p>表格中的 192.168.0.x 是最常用的私有 IP 地址。你家里有 Wi-Fi，对应就会有一个 IP 地址。一般你家里地上网设备不会超过 256 个，所以 /24 基本就够了。有时候我们也能见到 /16 的 CIDR，这两种是最常见的，也是最容易理解的。  </p><p>不需要将十进制转换为二进制 32 位，就能明显看出 192.168.0 是网络号，后面是主机号。而整个网络里面的第一个地址 192.168.0.1，往往就是你这个私有网络的出口地址。例如，你家里的电脑连接 Wi-Fi，Wi-Fi 路由器的地址就是 192.168.0.1，而 192.168.0.255 就是广播地址。一旦发送这个地址，整个 192.168.0 网络里面的所有机器都能收到。  但是也不总都是这样的情况。因此，其他情况往往就会很难理解，还容易出错。</p><h3 id="ip-addr-和ifconfig-查ip信息内容分析"><a href="#ip-addr-和ifconfig-查ip信息内容分析" class="headerlink" title="ip addr 和ifconfig 查ip信息内容分析"></a>ip addr 和ifconfig 查ip信息内容分析</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">root@test:~# ip addr</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default </span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:fec7:7975/64 scope link </span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><h4 id="scope"><a href="#scope" class="headerlink" title="scope"></a>scope</h4><p>在 IP 地址的后面有个 scope，对于 eth0 这张网卡来讲，是 global，说明这张网卡是可以对外的，可以接收来自各个地方的包。对于 lo 来讲，是 host，说明这张网卡仅仅可以供本机相互通信。 lo 全称是loopback，又称环回接口，往往会被分配到 127.0.0.1 这个地址。这个地址用于本机通信，经过内核处理后直接返回，不会在任何网络中出现。</p><h4 id="MAC地址"><a href="#MAC地址" class="headerlink" title="MAC地址"></a>MAC地址</h4><p>在 IP 地址的上一行是 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff，这个被称为MAC 地址，是一个网卡的物理地址，用十六进制，6 个 byte 表示。 </p><p> MAC 地址是一个很容易让人“误解”的地址。因为 MAC 地址号称全局唯一，不会有两个网卡有相同的 MAC 地址，而且网卡自生产出来，就带着这个地址。</p><p>很多人看到这里就会想，既然这样，整个互联网的通信，全部用 MAC 地址好了，只要知道了对方的 MAC 地址，就可以把信息传过去。  这样当然是不行的。 一个网络包要从一个地方传到另一个地方，除了要有确定的地址，还需要有定位功能。 而有门牌号码属性的 <strong>IP 地址，才是有远程定位功能的。</strong>  例如，你去杭州市网商路 599 号 B 楼 6 层找刘超，你在路上问路，可能被问的人不知道 B 楼是哪个，但是可以给你指网商路怎么去。但是如果你问一个人，你知道这个身份证号的人在哪里吗？可想而知，没有人知道。</p><p><strong>MAC 地址是有一定定位功能的，只不过范围非常有限。</strong>你可以根据 IP 地址，找到杭州市网商路 599 号 B 楼 6 层，但是依然找不到我，你就可以靠吼了，大声喊身份证 XXXX 的是哪位？我听到了，我就会站起来说，是我啊。但是如果你在上海，到处喊身份证 XXXX 的是哪位，我不在现场，当然不会回答，因为我在杭州不在上海。  所以，MAC 地址的通信范围比较小，局限在一个子网里面。例如，从 192.168.0.2/24 访问 192.168.0.3/24 是可以用 MAC 地址的。一旦跨子网，即从 192.168.0.2/24 到 192.168.1.2/24，MAC 地址就不行了，需要 IP 地址起作用了。</p><p><strong>MAC 地址更像是身份证，是一个唯一的标识。</strong>它的唯一性设计是为了组网的时候，不同的网卡放在一个网络里面的时候，可以不用担心冲突。从硬件角度，保证不同的网卡有不同的标识。  </p><h4 id="网络设备的状态标识"><a href="#网络设备的状态标识" class="headerlink" title="网络设备的状态标识"></a>网络设备的状态标识</h4><p>解析完了 MAC 地址，我们再来看 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 是干什么的？这个叫作net_device flags，网络设备的状态标识。  UP 表示网卡处于启动的状态；BROADCAST 表示这个网卡有广播地址，可以发送广播包；MULTICAST 表示网卡可以发送多播包；LOWER_UP 表示 L1 是启动的，也即网线插着呢。</p><h4 id="mtu"><a href="#mtu" class="headerlink" title="mtu"></a>mtu</h4><p>MTU1500 是指什么意思呢？是哪一层的概念呢？最大传输单元 MTU 为 1500，这是以太网的默认值。网络包是层层封装的。MTU 是二层 MAC 层的概念。MAC 层有 MAC 的头，以太网规定连 MAC 头带正文合起来，不允许超过 1500 个字节。正文里面有 IP 的头、TCP 的头、HTTP 的头。如果放不下，就需要分片来传输。</p><h4 id="qdisc"><a href="#qdisc" class="headerlink" title="qdisc"></a>qdisc</h4><p>qdisc pfifo_fast 是什么意思呢？qdisc 全称是queueing discipline，中文叫排队规则。内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。  </p><p>最简单的 qdisc 是 pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。</p><p>pfifo_fast 稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。  三个波段（band）的优先级也不相同。band 0 的优先级最高，band 2 的最低。如果 band 0 里面有数据包，系统就不会处理 band 1 里面的数据包，band 1 和 band 2 之间也是一样。  数据包是按照服务类型（Type of Service，TOS）被分配到三个波段（band）里面的。TOS 是 IP 头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。  队列是个好东西，后面我们讲云计算中的网络的时候，会有很多用户共享一个网络出口的情况，这个时候如何排队，每个队列有多粗，队列处理速度应该怎么提升，我都会详细为你讲解。</p><h3 id="如何手动设计IP地址"><a href="#如何手动设计IP地址" class="headerlink" title="如何手动设计IP地址"></a>如何手动设计IP地址</h3><p>使用net-tools:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo ifconfig eth1 10.0.0.1/24</span><br><span class="line"><span class="meta">$</span> sudo ifconfig eth1 up</span><br></pre></td></tr></table></figure><p>使用iproute2:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sudo ip addr add 10.0.0.1/24 dev eth1</span><br><span class="line"><span class="meta">$</span> sudo ip link set up eth1</span><br></pre></td></tr></table></figure><h3 id="如果身边的人的IP都是192-68-1-x，我设置成-172-10-168-2会怎么样"><a href="#如果身边的人的IP都是192-68-1-x，我设置成-172-10-168-2会怎么样" class="headerlink" title="如果身边的人的IP都是192.68.1.x，我设置成 172.10.168.2会怎么样"></a>如果身边的人的IP都是192.68.1.x，我设置成 172.10.168.2会怎么样</h3><p>包发不出去，因为linux会首先判断这个ip和本机是一个网段的吗？只有是一个网段才会发送ARP协议获取MAC地址。如果判断不是同一网段，Linux 默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。所以如果你要配置ip一定要问好你的网络管理员，人少还行，如果每个人都去问那管理员会疯的，所以有了DHCP</p><h3 id="动态主机配置协议（DHCP）"><a href="#动态主机配置协议（DHCP）" class="headerlink" title="动态主机配置协议（DHCP）"></a>动态主机配置协议（DHCP）</h3><p>Dynamic Host Configuration Protocol</p><p>有了这个协议，网络管理员就轻松多了。他只需要配置一段共享的 IP 地址。每一台新接入的机器都通过 DHCP 协议，来这个共享的 IP 地址里申请，然后自动配置好就可以了。等人走了，或者用完了，还回去，这样其他的机器也能用。</p><h3 id="DHCP工作模式"><a href="#DHCP工作模式" class="headerlink" title="DHCP工作模式"></a>DHCP工作模式</h3><h4 id="第一步（DHCP-Discover）"><a href="#第一步（DHCP-Discover）" class="headerlink" title="第一步（DHCP Discover）"></a>第一步（DHCP Discover）</h4><p>新来的机器使用 IP 地址 0.0.0.0 发送了一个广播包，目的 IP 地址为 255.255.255.255。广播包封装了 UDP，UDP 封装了 BOOTP。其实 DHCP 是 BOOTP 的增强版，但是如果你去抓包的话，很可能看到的名称还是 BOOTP 协议。 在这个广播包里面，新人大声喊：我是新来的（Boot request），我的 MAC 地址是这个，我还没有 IP，谁能给租给我个 IP 地址！</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815082342442.png" alt="image-20190815082342442" style="zoom:50%;"></p><h4 id="第二步（DHCP-Offer）"><a href="#第二步（DHCP-Offer）" class="headerlink" title="第二步（DHCP Offer）"></a>第二步（DHCP Offer）</h4><p>如果配置了DHCP server，他会立马知道来了个新人，是通过MAC地址判断的，所以唯一的MAC地址是多么重要。他会立马回DHCP Offer，DHCP Server 仍然使用广播地址作为目的地址，因为，此时请求分配 IP 的新人还没有自己的 IP。DHCP Server 回复说，我分配了一个可用的 IP 给你，你看如何？除此之外，服务器还发送了子网掩码、网关和 IP 地址租用期等信息。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815082810510.png" alt="image-20190815082810510" style="zoom:50%;"></p><h4 id="第三步（选一个Offer）"><a href="#第三步（选一个Offer）" class="headerlink" title="第三步（选一个Offer）"></a>第三步（选一个Offer）</h4><p>新来的机器很开心，它的广播得到了回复，并且有人愿意租给它一个 IP 地址了，这意味着它可以在网络上立足了。</p><p>当然更令人开心的是，如果有多个 DHCP Server，这台新机器会收到多个 IP 地址，简直受宠若惊。  它会选择其中一个 DHCP Offer，一般是最先到达的那个，并且会向网络发送一个 DHCP Request 广播数据包，包中包含客户端的 MAC 地址、接受的租约中的 IP 地址、提供此租约的 DHCP 服务器地址等，并告诉所有 DHCP Server 它将接受哪一台服务器提供的 IP 地址，告诉其他 DHCP 服务器，谢谢你们的接纳，并请求撤销它们提供的 IP 地址，以便提供给下一个 IP 租用请求者。</p><p>此时，由于还没有得到 DHCP Server 的最后确认，客户端仍然使用 0.0.0.0 为源 IP 地址、255.255.255.255 为目标地址进行广播。在 BOOTP 里面，接受某个 DHCP Server 的分配的 IP。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815083218963.png" alt="image-20190815083218963" style="zoom:50%;"></p><h4 id="第四步（DHCP-Server-Ack）"><a href="#第四步（DHCP-Server-Ack）" class="headerlink" title="第四步（DHCP Server Ack）"></a>第四步（DHCP Server Ack）</h4><p>当 DHCP Server 接收到客户机的 DHCP request 之后，会广播返回给客户机一个 DHCP ACK 消息包，表明已经接受客户机的选择，并将这一 IP 地址的合法租用信息和其他的配置信息都放入该广播包，发给客户机，欢迎它加入网络大家庭。</p><p>最终租约达成的时候，还是需要广播一下，让大家都知道一下。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815083442134.png" alt="image-20190815083442134" style="zoom:50%;"></p><h3 id="IP地址的收回与续租"><a href="#IP地址的收回与续租" class="headerlink" title="IP地址的收回与续租"></a>IP地址的收回与续租</h3><p>既然是租房子，就是有租期的。租期到了，管理员就要将 IP 收回。  如果不用的话，收回就收回了。就像你租房子一样，如果还要续租的话，不能到了时间再续租，而是要提前一段时间给房东说。DHCP 也是这样。  客户机会在租期过去 50% 的时候，直接向为其提供 IP 地址的 DHCP Server 发送 DHCP request 消息包。客户机接收到该服务器回应的 DHCP ACK 消息包，会根据包中所提供的新的租期以及其他已经更新的 TCP/IP 参数，更新自己的配置。这样，IP 租用更新就完成了。</p><h3 id="HUB"><a href="#HUB" class="headerlink" title="HUB"></a>HUB</h3><p>有一个叫作Hub的东西，也就是集线器。这种设备有多个口，可以将宿舍里的多台电脑连接起来。但是，和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。</p><p>你可能已经发现问题了。Hub 采取的是广播的模式，如果每一台电脑发出的包，宿舍的每个电脑都能收到，那就麻烦了。这就需要解决几个问题：包是发给谁的，顺序，包是否完整，这几个问题都是MAC层需要解决的</p><h3 id="MAC层如何解决上述问题"><a href="#MAC层如何解决上述问题" class="headerlink" title="MAC层如何解决上述问题"></a>MAC层如何解决上述问题</h3><h4 id="根据ip如何获取MAC地址"><a href="#根据ip如何获取MAC地址" class="headerlink" title="根据ip如何获取MAC地址"></a>根据ip如何获取MAC地址</h4><p>根据IP地址获取MAC地址</p><p>####第一个问题，发给谁，谁接收？</p><p>这里用到一个物理地址，叫作链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC 地址。  解决第一个问题就牵扯到第二层的网络包格式。对于以太网，第二层的最开始，就是目标的 MAC 地址和源的 MAC 地址。</p><h4 id="第二个问题，顺序问题"><a href="#第二个问题，顺序问题" class="headerlink" title="第二个问题，顺序问题"></a>第二个问题，顺序问题</h4><p>MAC的全称是<strong>Medium Access Control</strong>，即媒体访问控制。控制什么呢？其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问。有很多算法可以解决这个问题。就像车管所管束马路上跑的车，能想的办法都想过了。</p><ul><li><p>方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作<strong>信道划分</strong>；  </p></li><li><p>方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作<strong>轮流协议</strong>；  </p></li><li><p>方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作<strong>随机接入协议</strong>。著名的以太网，用的就是这个方式。</p></li></ul><h4 id="第三个问题，如何校验包是否完整"><a href="#第三个问题，如何校验包是否完整" class="headerlink" title="第三个问题，如何校验包是否完整"></a>第三个问题，如何校验包是否完整</h4><p>对于以太网，第二层的最后面是CRC，也就是循环冗余检测。通过 XOR 异或的算法，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815085131527.png" alt="image-20190815085131527" style="zoom: 40%;"></p><h3 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h3><p>一旦机器数目增多，问题就出现了。因为 Hub 是广播的，不管某个接口是否需要，所有的 Bit 都会被发送出去，然后让主机来判断是不是需要。这种方式路上的车少就没问题，车一多，产生冲突的概率就提高了。而且把不需要的包转发过去，纯属浪费。看来 Hub 这种不管三七二十一都转发的设备是不行了，需要点儿智能的。因为每个口都只连接一台电脑，这台电脑又不怎么换 IP 和 MAC 地址，只要记住这台电脑的 MAC 地址，如果目标 MAC 地址不是这台电脑的，这个口就不用转发了。  谁能知道目标 MAC 地址是否就是连接某个口的电脑的 MAC 地址呢？这就需要一个能把 MAC 头拿下来，检查一下目标 MAC 地址，然后根据策略转发的设备，这个设备是个二层设备，我们称为交换机。</p><p>交换机是有 MAC 地址学习能力的，学完了它就知道谁在哪儿了，不用广播了。</p><h3 id="拓扑结构怎么形成的"><a href="#拓扑结构怎么形成的" class="headerlink" title="拓扑结构怎么形成的"></a>拓扑结构怎么形成的</h3><p>当机器变得很多的时候，一个交换机肯定不够用，需要多台交换机，交换机之间连接起来，就形成一个稍微复杂的拓扑结构。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815085800720.png" alt="image-20190815085800720" style="zoom:33%;"></p><h3 id="如何解决拓扑结构中的环路问题"><a href="#如何解决拓扑结构中的环路问题" class="headerlink" title="如何解决拓扑结构中的环路问题"></a>如何解决拓扑结构中的环路问题</h3><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815085848811.png" alt="image-20190815085848811" style="zoom:33%;"></p><p>需要<strong>STP协议</strong>来解决</p><h3 id="STP协议"><a href="#STP协议" class="headerlink" title="STP协议"></a>STP协议</h3><p>在数据结构中，有一个方法叫作<strong>最小生成树</strong>。有环的我们常称为图。将图中的环破了，就生成了树。在计算机网络中，生成树的算法叫作<strong>STP</strong>，全称Spanning Tree Protocol。</p><h3 id="如何解决广播问题和安全问题？"><a href="#如何解决广播问题和安全问题？" class="headerlink" title="如何解决广播问题和安全问题？"></a>如何解决广播问题和安全问题？</h3><p>毕竟机器多了，交换机也多了，就算交换机比 Hub 智能一些，但是还是难免有广播的问题，一大波机器，相关的部门、不相关的部门，广播一大堆，性能就下来了。就像一家公司，创业的时候，一二十个人，坐在一个会议室，有事情大家讨论一下，非常方便。但是如果变成了 50 个人，全在一个会议室里面吵吵，就会乱的不得了。有两种解决方法，物理隔离、虚拟隔离</p><h4 id="物理隔离"><a href="#物理隔离" class="headerlink" title="物理隔离"></a>物理隔离</h4><p>每个部门设一个单独的会议室，对应到网络方面，就是每个部门有单独的交换机，配置单独的子网，这样部门之间的沟通就需要路由器了。路由器咱们还没讲到，以后再说。这样的问题在于，有的部门人多，有的部门人少。人少的部门慢慢人会变多，人多的部门也可能人越变越少。如果每个部门有单独的交换机，口多了浪费，少了又不够用。</p><h4 id="虚拟隔离"><a href="#虚拟隔离" class="headerlink" title="虚拟隔离"></a>虚拟隔离</h4><p>就是用我们常说的<strong>VLAN</strong>，或者叫<strong>虚拟局域网</strong>。使用 VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？</p><p>我们只需要在原来的二层的头上加一个 TAG，里面有一个 VLAN ID，一共 12 位。为什么是 12 位呢？因为 12 位可以划分 4096 个 VLAN。这样是不是还不够啊。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815130704532.png" alt="image-20190815130704532" style="zoom:50%;"></p><p>现在的情况证明，目前云计算厂商里面绝对不止 4096 个用户。当然每个用户需要一个 VLAN 了啊，怎么办呢，这个我们在后面的章节再说。  如果我们买的交换机是支持 VLAN 的，当这个交换机把二层的头取下来的时候，就能够识别这个 VLAN ID。这样只有相同 VLAN 的包，才会互相转发，不同 VLAN 的包，是看不到的。这样广播问题和安全问题就都能够解决了。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815130742460.png" alt="image-20190815130742460" style="zoom:50%;"></p><p>有人会问交换机之间怎么连接呢？将两个交换机连接起来的口应该设置成什么 VLAN 呢？对于支持 VLAN 的交换机，有一种口叫作Trunk 口。它可以转发属于任何 VLAN 的口。交换机之间可以通过这种口相互连接。</p><h3 id="ICMP协议"><a href="#ICMP协议" class="headerlink" title="ICMP协议"></a>ICMP协议</h3><p>ICMP全称Internet Control Message Protocol，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢。</p><p>ICMP 报文是封装在 IP 包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815091433474.png" alt="image-20190815091433474"></p><p>ICMP 报文有很多的类型，不同的类型有不同的代码。<strong>最常用的类型是主动请求为 8,主动请求的应答为0</strong></p><h4 id="查询报文类型"><a href="#查询报文类型" class="headerlink" title="查询报文类型"></a>查询报文类型</h4><p>常用的<strong>ping</strong> 就是查询报文，是一种主动请求，并且获得主动应答的 ICMP 协议。所以，ping 发的包也是符合 ICMP 协议格式的，只不过它在后面增加了自己的格式。</p><p><img src="/2019/08/14/《趣谈网络协议》第3-7讲笔记/images/image-20190815205514192.png" alt="image-20190815205514192"></p><h4 id="差错报文类型"><a href="#差错报文类型" class="headerlink" title="差错报文类型"></a>差错报文类型</h4><p>Traceroute 使用差错报文,详细请看专栏</p><h3 id="路由器"><a href="#路由器" class="headerlink" title="路由器"></a>路由器</h3><p>网关往往是一个路由器，是一个三层转发的设备。啥叫三层设备？前面也说过了，就是把 MAC 头和 IP 头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;ifconfig-和-ip-addr的区别&quot;&gt;&lt;a href=&quot;#ifconfig-和-ip-addr的区别&quot; class=&quot;headerlink&quot; title=&quot;ifconfig 和 ip addr的区别&quot;&gt;&lt;/a&gt;ifconfig 和 ip addr的区别&lt;/
      
    
    </summary>
    
    
      <category term="网络" scheme="http://shiyangtao.github.io/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
