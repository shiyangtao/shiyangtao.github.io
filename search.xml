<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2F2020%2F06%2F05%2FJVM%2F</url>
    <content type="text"><![CDATA[JVM源码分析之FinalReference PhantomReference导致CMS GC耗时严重 一次 Young GC 的优化实践（FinalReference 相关） 强/软/弱/虚引用 【译】深入理解G1的GC日志（一）]]></content>
  </entry>
  <entry>
    <title><![CDATA[java基础]]></title>
    <url>%2F2020%2F05%2F08%2Fjava%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[Java类初始化顺序]]></content>
  </entry>
  <entry>
    <title><![CDATA[k8s面试题]]></title>
    <url>%2F2020%2F03%2F31%2Fk8s%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Kubernetes面试题理论篇简要说下Kubernetes有哪些核心组件以及这些组件负责什么工作？123456789Master节点etcd：提供数据库服务保存了整个集群的状态kube-apiserver：提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制,这将公开Kubernetes主节点组件的所有API，并负责在Kubernetes节点和Kubernetes主组件之间建立通信。kube-controller-manager(控制循环与状态协调机制)：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等。kub-scheduler：负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上Node节点kubelet：负责维护容器的生命周期，同时也负责Volume和网络的管理kube-proxy：负责为Service提供内部的服务发现和负载均衡，并维护网络规则container-runtime：是负责管理运行容器的软件，比如docker Pod的生命周期1234567Pending 部署Pod事务已被集群受理，但当前容器镜像还未下载完。Running 所有容器已被创建，并被部署到k8s节点。Successed Pod成功退出，并不会被重启。Failed Pod中有容器被终止。Unknown 未知原因，如kube-apiserver无法与Pod进行通讯。详 k8s的pod内容器之间的关系1Pod里面的容器共享网络，因此可使用localhost通讯。由于也共享存储，所以可以使用IPC和共享内存进行通讯。 详述kube-proxy原理123问题：详述kube-proxy原理，一个请求是如何经过层层转发落到某个pod上的整个过程。请求可能来自pod也可能来自外部。kube-proxy部署在每个Node节点上，通过监听集群状态变更，并对本机iptables做修改，从而实现网络路由。 负载均衡的两种实现：1. iptables 2. ipvs + iptables 问题：deployment/rs有什么区别。其使用方式、使用条件和原理是什么。1deployment是rs的超集，提供更多的部署功能，如：回滚、暂停和重启、 版本记录、事件和状态查看、滚动升级和替换升级。 命令篇查看ops这个命名空间下的所有pod，并显示pod的IP地址 1kubectl get pods -n ops -o wide 查看tech命名空间下的pod名称为tech-12ddde-fdfde的日志，并显示最后30行 1kubectl logs tech-12ddde-fdfde -n tech|tail -n 30 怎么查看test的命名空间下pod名称为test-5f7f56bfb7-dw9pw的状态 1kubectl describe pod test-5f7f56bfb7-dw9pw -n test 如何查看test命名空间下的所有endpoints 1kubectl get ep -n test 如何查看test命名空间下的所有ingress 1kubeclt get ingress -n test 如何删除test命名空间下某个deploymemt，名称为gitlab 1kubectl delete deploy gitlab -n test 如何缩减test命名空间下deployment名称为gitlab的副本数为1 1kubeclt scale deployment gitlab -n test --replicas=1 如何在不进入pod内查看命名空间为test,pod名称为test-5f7f56bfb7-dw9pw的hosts 1kubectl exec -it test-5f7f56bfb7-dw9pw -n test -- cat /etc/hosts 如何设置节点test-node-10为不可调度以及如何取消不可调度 12kubectl taint nodes test-node-10 foo=bar:NoSchedulekubectl taint nodes test-node-10 foo=bar:NoSchedule- or 12kubectl cordon test-node-10kubectl uncordon test-node-10 taint vs cordon 考察实际生产经验(最重要)Downward API某个pod启动的时候需要用到pod的名称，请问怎么获取pod的名称，简要写出对应的yaml配置(考察是否对k8s的Downward API有所了解) 12345env:- name: test valueFrom: fieldRef: fieldPath: metadata.name hosts某个pod需要配置某个内网服务的hosts，比如数据库的host，刑如：192.168.4.124 db.test.com，请问有什么方法可以解决，简要写出对应的yaml配置或其他解决办法 1234hostAliases:- ip: &quot;192.168.4.124&quot; hostnames: - &quot;db.test.com 写dockerfile请用系统镜像为centos:latest制作一个jdk版本为1.8.142的基础镜像，请写出dockerfile（考察dockerfile的编写能力） 1234567FROM centos:latestARG JDK_HOME=/root/jdk1.8.0_142WORKDIR /rootADD jdk-8u142-linux-x64.tar.gz /rootENV JAVA_HOME=/root/jdk1.8.0_142ENV PATH=$PATH:$JAVA_HOME/binCMD [&quot;bash&quot;] podAntiAffinity假如某个pod有多个副本，如何让两个pod分布在不同的node节点上，请简要写出对应的yaml文件（考察是否对pod的亲和性有所了解） 1234567affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: &quot;kubernetes.io/hostname&quot; labelSelector: matchLabels: app: test 日志收集pod的日志如何收集，简要写出方案(考察是否真正有生产经验，日志收集是必须解决的一个难题) https://jimmysong.io/kubernetes-handbook/practice/app-log-collection.html 1logstash + elk https://blog.51cto.com/14154700/2452179 1234567891011121314151617181920212223classDiagram class Abstraction &#123; &#125; class RefinedAbstraction&#123; &#125; class Implementation&#123; &#125; class ConcreteImplementation&#123; &#125; Implementation &lt;|.. ConcreteImplementation Implementation &lt;-- Abstraction Abstraction &lt;|-- RefinedAbstraction]]></content>
  </entry>
  <entry>
    <title><![CDATA[遇到过的问题]]></title>
    <url>%2F2020%2F03%2F28%2F%E9%81%87%E5%88%B0%E8%BF%87%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[工作中遇到的问题open feignnot a type supported by this encoderhttps://blog.csdn.net/F_QWERDF/article/details/89713165 https://blog.csdn.net/gududedabai/article/details/79895893 https://github.com/spring-cloud/spring-cloud-netflix/issues/2746 依赖的项目中有定义全局的Encoder Spring bootspring外部配置优先级https://docs.spring.io/spring-boot/docs/1.5.6.BUILD-SNAPSHOT/reference/html/boot-features-external-config.html Spring@Profile踩坑https://www.jianshu.com/p/75de79fba705 错误用法 1234567891011@Configurationpublic class ProfileDatabaseConfig &#123; @Bean @Profile("development") public DataSource database() &#123; ... &#125; @Bean @Profile("production") public DataSource database() &#123; ... &#125;&#125; 正确用法 1234567891011@Configurationpublic class ProfileDatabaseConfig &#123; @Bean("dataSource") @Profile("development") public DataSource embeddedDatabase() &#123; ... &#125; @Bean("dataSource") @Profile("production") public DataSource productionDatabase() &#123; ... &#125;&#125; 对于相同Java方法名称的重载@Bean方法(类似于构造函数重载), 需要在所有重载方法上一致地声明@Profile条件, 如果条件不一致, 则只有重载方法中第一个声明上的条件才生效。]]></content>
  </entry>
  <entry>
    <title><![CDATA[容器网络]]></title>
    <url>%2F2020%2F03%2F02%2F%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[docker网络原理原理被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥(docker0)的方式，实现了跟同其他容器的数据交换。 这样在同一个宿主机上的容器之间可以通信，并且容器可以访问外网 不足在 Docker 的默认配置下，不同宿主机上的容器通过 IP 地址进行互相访问是根本做不到的。 在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了。 跨主机网络FlannelUDP原理 数据包是怎么到了flannel0呢？ 由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥，从而出现在宿主机上。而出现在宿主机上。这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示： 1234567# 在Node 1上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2 如何找到对应node的ip？ flanneld 进程根据容器IP地址找到对应node的ip，如下所示： 这里，就用到了 Flannel 项目里一个非常重要的概念：子网（Subnet）。 事实上，在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。 1234$ etcdctl ls /coreos.com/network/subnets/coreos.com/network/subnets/100.96.1.0-24/coreos.com/network/subnets/100.96.2.0-24/coreos.com/network/subnets/100.96.3.0-24 123$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24&#123;&quot;PublicIP&quot;:&quot;10.168.0.3&quot;&#125; node2 flannel0是怎么到的docker0呢 而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示： 123456# 在Node 2上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.3 由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。 所以需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可： 12$ FLANNEL_SUBNET=100.96.1.1/24$ dockerd --bip=$FLANNEL_SUBNET ... 不足UDP 模式有严重的性能问题，实际上，相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN （三层设备）设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示： VXLAN 数据包是怎么到了flannel.1呢？ 和上边UDP的方式是一样的 当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则： 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.1.16.0 10.1.16.0 255.255.255.0 UG 0 0 0 flannel.1 这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。 “目的 VTEP 设备”的 MAC 地址是什么？ 此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示： 123# 在Node 1上$ ip neigh show dev flannel.110.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。 如何找到对应node的ip？ 在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示： 1234# 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:375e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent VNI作用 而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备（二层设备）识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。 host-gw参考下文 k8s flannel vxlan的实现以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已，如下所示： k8s flannel host-gw实现当你设置 Flannel 使用 host-gw 模式之后，flanneld 会在宿主机上创建这样一条规则，以 Node 1 为例： 123$ ip route...10.244.1.0/24 via 10.168.0.3 dev eth0 这条路由规则的含义是：目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是 10.168.0.3（即：via 10.168.0.3）。所谓下一跳地址就是：如果 IP 包从主机 A 发到主机 B，需要经过路由设备 X 的中转。那么 X 的 IP 地址就应该配置为主机 A 的下一跳地址。 可以看到，host-gw 模式的工作原理，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。 也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。 所以说，Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。 而在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。 k8s Calico二层互通的情况实际上，Calico 项目提供的网络解决方案，与 Flannel 的 host-gw 模式，几乎是完全一样的。 不过，不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了一个“重型武器”BGP来自动地在整个集群中分发路由信息。 除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。 此外，由于 Calico 没有使用 CNI 的网桥模式，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。比如，宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示： 1210.233.2.3 dev cali5863f3 scope link 即：发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。 三层互通（IPIP模式） 在 Calico 的 IPIP 模式下，Felix 进程在 Node 1 上添加的路由规则，会稍微不同，如下所示： 110.233.2.0/24 via 192.168.2.2 tunl0 可以看到，尽管这条规则的下一跳地址仍然是 Node 2 的 IP 地址，但这一次，要负责将 IP 包发出去的设备，变成了 tunl0。注意，是 T-U-N-L-0，而不是 Flannel UDP 模式使用的 T-U-N-0（tun0），这两种设备的功能是完全不一样的。 由于宿主机之间已经使用路由器配置了三层转发，也就是设置了宿主机之间的“下一跳”。所以这个 IP 包在离开 Node 1 之后，就可以经过路由器，最终“跳”到 Node 2 上。 Calico 使用的这个 tunl0 设备，是一个 IP 隧道（IP tunnel）设备。在上面的例子中，IP 包进入 IP 隧道设备之后，就会被 Linux 内核的 IPIP 驱动接管。IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中，如下所示： 不难看到，当 Calico 使用 IPIP 模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。所以，在实际使用时，如非硬性需求，我建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。 总结我以网桥类型的 Flannel 插件为例，为你讲解了 Kubernetes 里容器网络和 CNI 插件的主要工作原理。不过，除了这种模式之外，还有一种纯三层（Pure Layer 3）网络方案非常值得你注意。其中的典型例子，莫过于 Flannel 的 host-gw 模式和 Calico 项目了。 三层（host-gw和calico的二层互通）和隧道模式(IPIP)的异同三层的优点：少了封包和解包的过程，性能肯定是更高的。三层的缺点：需要自己想办法维护路由规则。隧道的优点：简单，原因是大部分工作都是由 Linux 内核的模块实现了，应用层面工作量较少。隧道的缺点：主要的问题就是性能低。 需要注意的是，在大规模集群里，三层网络方案在宿主机上的路由规则可能会非常多，这会导致错误排查变得困难。此外，在系统故障的时候，路由规则出现重叠冲突的概率也会变大。基于上述原因，如果是在公有云上，由于宿主机网络本身比较“直白”，我一般会推荐更加简单的 Flannel host-gw 模式。]]></content>
  </entry>
  <entry>
    <title><![CDATA[学习方法]]></title>
    <url>%2F2020%2F03%2F02%2F%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本篇文章内容总结自《认知天性》这本书。 比如读一本书，看完以后就忘了，要怎样才能够记住？ 错误的方式重复阅读，划重点80%的人都是采取划重点、重复阅读直到记住为止。心理学家告诉你这是白费力气，因为要想运用知识，就必须记忆，而重复阅读并不能带出长久记忆或者说效率很低。当你重复阅读的时候，会产生自己已经记住的错觉（而实际你还差的远）所以读的时候不知道哪里是自己比较弱的部分，因为重复接触并不能强化记忆 刻意练习在一段时间内反复练习同一件事，这种频繁而集中的训练只会产生短期记忆。对于产生长期记忆或者掌握某项技能来说，毫无助益。 正确的方式用纸笔重新回想下书中的内容，写出大致的架构，然后从架构中，抓取自己想要学习的重点或者是自己不清楚的知识，然后用自己的话描述成人人都懂的故事内容。 检索检索就是主动回忆某件事的过程。 考试就是检索的一种，回忆的时候比较痛苦这时候学习效果反而会加倍，通过一个个的考试，就会不断的回忆做自我检测，就能提高记忆的效果。 反思也是检索的一种，原理同考试一样。 检索的过程虽然痛苦，但是在学习上越轻松，效果越不好。 那些看起来非常勤奋，不停的背书，不停的做笔记的人，只是看起来非常努力，耗费了大量时间。 间隔练习间隔练习就是每隔一段时间，来练习下之前学过的东西。 间隔练习使知识存储更牢固，可以巩固知识的记忆 穿插练习在练习中穿插两个以上的主题和技能也是一种能胜过集中练习的方法 如同费曼的理论 1.选择并教授 2. 发现不能理解的地方 3.重新学习 4.简化 时间管理GTD 时间管理之术 1.收集 2.整理 2min能做完的事项、等待事项、特定日程、单步骤事项、多步骤事项 3.just do it 自己提炼出的学习方法学习一门技术 先把这个领域最有代表性的并且适合自己这个阶段去看的书籍或者博客学一遍(不要贪多，一两本足以)，一定要细心阅读，做好笔记，最最最重要的是实践。 根据记忆或者书籍目录回忆书中的知识点，形成脑图，发现自己比较薄弱的部分 巩固比较薄弱的地方 “考试”，找到相关的面试题来检索。偏编程实践要coding验证 把测试的面试题和答案整理成册，方便自己间隔练习，如果遇到比较薄弱的地方，回到第三步 间隔练习，看整理的笔记、“考试” 看一本方法论类的书比如《自控力》《时间管理》这一类的 看完后，找出纸笔回忆出书籍的脉络，或者看着目录回忆。 整理出读书笔记。 隔一段时间去翻一下读书笔记。]]></content>
  </entry>
  <entry>
    <title><![CDATA[java程序员的能力要求]]></title>
    <url>%2F2020%2F02%2F29%2Fjava%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%83%BD%E5%8A%9B%E8%A6%81%E6%B1%82%2F</url>
    <content type="text"><![CDATA[从boss直聘里翻看了一些热门工作岗位的职位要求 Java基础 多线程，并发编程能力 集合 JVM IO&amp;网络编程 编程能力 良好的设计和抽象能力 设计模式 掌握主流开发框架 Spring 全家桶 Spring MVC Spring Boot Spring Cloud mybatis 数据库 关系型 以Mysql代表 NoSql按照出现次数排序，斜体的出现次数比较少 K-V型：解决关系型数据库无法存储数据结构的问题，以Redis为代表。 文档数据库：解决关系型数据库强schema约束的问题，以MongoDB为代表。 列式数据库：解决关系数据库大数据场景下的I/O问题，以HBase为代表。 全文搜索引擎：解决关系数据库的全文搜索性能问题，以Elasticsearch为代表。 熟悉分布式系统的设计和应用 高可用&amp;高性能&amp;可扩展设计 RPC调用 (Dobbo\Grpc\Thrift\OpenFeign) 缓存设计 消息中间件 分布式事务 加分项在大部分公司是加分项，顺序按照出现的次数 数据结构&amp;算法 基础能力 Linux 网络 性能调优 其他语言 go、python 容器技术]]></content>
  </entry>
  <entry>
    <title><![CDATA[k8s_command]]></title>
    <url>%2F2020%2F02%2F21%2Fk8s-command%2F</url>
    <content type="text"><![CDATA[minikube 搭建https://kubernetes.io/zh/docs/tasks/tools/install-minikube/ 1sudo minikube start --vm-driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --image-repository=&apos;registry.cn-hangzhou.aliyuncs.com/google_containers&apos; 外网访问dashboardhttps://blog.haohtml.com/archives/19201 1nohup kubectl proxy --address=&apos;0.0.0.0&apos; --port=8001 --accept-hosts=&apos;^*$&apos; &amp; 外网访问dashboard 1http://&lt;ip&gt;:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ 1http://47.92.136.48:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ headless 1kubectl exec -it busybox -- nslookup busybox-1.default-subdomain.default.svc.cluster.local 1kubectl exec -it busybox -- nslookup default-subdomain.default.svc.cluster.local 外网访问内部servcie服务 1http://&lt;ip&gt;:8001/api/v1/namespaces/default/services/SERVICE-NAME:PORT-NAME/proxy/ 123http://47.92.136.48:8001/api/v1/namespaces/default/services/hostnames:default/proxy/http://47.92.136.48:8001/api/v1/namespaces/default/services/coffee-svc:http/proxy/http://47.92.136.48:8001/api/v1/namespaces/default/services/tea-svc:http/proxy/ ingress 搭建kubeadm安装 kubeadm 和 Docker123456789curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOFapt-get updateapt-get install -y docker.io kubeadm# 安装指定版本sudo apt-get install kubeadm=1.14.3-00 kubectl=1.14.3-00 kubelet=1.14.3-00 部署 Kubernetes 的 Master 节点123456789101112cat kubeadm.yamlapiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationcontrollerManager: extraArgs: horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; horizontal-pod-autoscaler-sync-period: &quot;10s&quot; node-monitor-grace-period: &quot;10s&quot;apiServer: extraArgs: runtime-config: &quot;api/all=true&quot;kubernetesVersion: &quot;stable-1.14&quot; 1kubeadm init --config kubeadm.yaml --ignore-preflight-errors=NumCPU 就可以完成 Kubernetes Master 的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令：这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。 12kubeadm join 172.26.87.130:6443 --token 29kgba.dytvremded8eicnv \ --discovery-token-ca-cert-hash sha256:83a982aaefeb3935dcbcce7da005de080a7bd43086ecf26503605bd71b01f6cc 配置kubectl 授权信息而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。 1234mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 1234$ kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster NotReady master 1d v1.11.1 1234567$ kubectl describe node master...Conditions:...Ready False ... KubeletNotReady runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 1234567891011$ kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-78fcdf6894-j9s52 0/1 Pending 0 1hcoredns-78fcdf6894-jm4wf 0/1 Pending 0 1hetcd-master 1/1 Running 0 2skube-apiserver-master 1/1 Running 0 1skube-controller-manager-master 0/1 Pending 0 1skube-proxy-xbd47 1/1 NodeLost 0 1hkube-scheduler-master 1/1 Running 0 1s 可以看到，CoreDNS、kube-controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。 部署网络插件12$ kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&quot; 123456789101112$ kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-78fcdf6894-j9s52 1/1 Running 0 1dcoredns-78fcdf6894-jm4wf 1/1 Running 0 1detcd-master 1/1 Running 0 9skube-apiserver-master 1/1 Running 0 9skube-controller-manager-master 1/1 Running 0 9skube-proxy-xbd47 1/1 Running 0 1dkube-scheduler-master 1/1 Running 0 9sweave-net-cmk27 2/2 Running 0 19s 部署 Kubernetes 的 Worker 节点12kubeadm join 172.26.87.130:6443 --token 29kgba.dytvremded8eicnv \ --discovery-token-ca-cert-hash sha256:83a982aaefeb3935dcbcce7da005de080a7bd43086ecf26503605bd71b01f6cc 通过 Taint/Toleration 调整 Master 执行 Pod 的策略打污点(taint)指令 12$ kubectl taint nodes node1 foo=bar:NoSchedule pod容忍(tolerations)这个污点指令 12345678910apiVersion: v1kind: Pod...spec: tolerations: - key: &quot;foo&quot; operator: &quot;Equal&quot; value: &quot;bar&quot; effect: &quot;NoSchedule&quot; 我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。 查看master默认污点 123456$ kubectl describe node masterName: masterRoles: masterTaints: node-role.kubernetes.io/master:NoSchedule 删除master默认污点 12$ kubectl taint nodes --all node-role.kubernetes.io/master-node.kubernetes.io/not-ready 或者Pod容忍这个污点 可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上： 123456789apiVersion: v1kind: Pod...spec: tolerations: - key: &quot;foo&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; 部署 Dashboard 可视化插件1$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml 部署容器存储插件123$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml 创建容器化应用创建1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 1$ kubectl create -f nginx-deployment.yaml 替换修改yaml文件 1234567... spec: containers: - name: nginx image: nginx:1.8 #这里被从1.7.9修改为1.8 ports: - containerPort: 80 1$ kubectl replace -f nginx-deployment.yaml apply12345$ kubectl apply -f nginx-deployment.yaml# 修改nginx-deployment.yaml的内容$ kubectl apply -f nginx-deployment.yaml 删除1$ kubectl delete -f nginx-deployment.yaml Pod属性Volume12345678910111213141516171819202122232425apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.8 ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: nginx-vol volumes: - name: nginx-vol emptyDir: &#123;&#125; emptyDir它其实就等同于我们之前讲过的 Docker 的隐式 Volume 参数，即：不显式声明宿主机目录的 Volume。所以，Kubernetes 也会在宿主机上创建一个临时目录，这个目录将来就会被绑定挂载到容器所声明的 Volume 目录上。备注：不难看到，Kubernetes 的 emptyDir 类型，只是把 Kubernetes 创建的临时目录作为 Volume 的宿主机目录，交给了 Docker。这么做的原因，是 Kubernetes 不想依赖 Docker 自己创建的那个 _data 目录。 hostPath当然，Kubernetes 也提供了显式的 Volume 定义，它叫做 hostPath。比如下面的这个 YAML 文件： 12345... volumes: - name: nginx-vol hostPath: path: /var/data 进入容器 12$ kubectl exec -it nginx-deployment-5c678cfb6d-lg9lw -- /bin/bash# ls /usr/share/nginx/html Projected Volume在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。到目前为止，Kubernetes 支持的 Projected Volume 一共有四种： Secret； ConfigMap； Downward API； ServiceAccountToken。 Secret它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: test-projected-volume spec: containers: - name: test-secret-volume image: busybox args: - sleep - &quot;86400&quot; volumeMounts: - name: mysql-cred mountPath: &quot;/projected-volume&quot; readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: user - secret: name: pass 1234567$ cat ./username.txtadmin$ cat ./password.txtc1oudc0w!$ kubectl create secret generic user --from-file=./username.txt$ kubectl create secret generic pass --from-file=./password.txt 1234$ kubectl get secretsNAME TYPE DATA AGEuser Opaque 1 51spass Opaque 1 51s 1$ kubectl create -f test-projected-volume.yaml 12345678$ kubectl exec -it test-projected-volume -- /bin/sh$ ls /projected-volume/userpass$ cat /projected-volume/userroot$ cat /projected-volume/pass1f2d1e2e67df ConfigMap与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件 1234567891011121314151617181920212223# .properties文件的内容$ cat example/ui.propertiescolor.good=purplecolor.bad=yellowallow.textmode=truehow.nice.to.look=fairlyNice# 从.properties文件创建ConfigMap$ kubectl create configmap ui-config --from-file=example/ui.properties# 查看这个ConfigMap里保存的信息(data)$ kubectl get configmaps ui-config -o yamlapiVersion: v1data: ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNicekind: ConfigMapmetadata: name: ui-config ... Downward API它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: test-downwardapi-volume labels: zone: us-est-coast cluster: test-cluster1 rack: rack-22spec: containers: - name: client-container image: k8s.gcr.io/busybox command: [&quot;sh&quot;, &quot;-c&quot;] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en &apos;\n\n&apos;; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo projected: sources: - downwardAPI: items: - path: &quot;labels&quot; fieldRef: fieldPath: metadata.labels 在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。 通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。 而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示： 12345$ kubectl create -f dapi-volume.yaml$ kubectl logs test-downwardapi-volumecluster=&quot;test-cluster1&quot;rack=&quot;rack-22&quot;zone=&quot;us-est-coast&quot; 目前Downward API支持的字段更加丰富了 1234567891011121314151617181. 使用fieldRef可以声明使用:spec.nodeName - 宿主机名字status.hostIP - 宿主机IPmetadata.name - Pod的名字metadata.namespace - Pod的Namespacestatus.podIP - Pod的IPspec.serviceAccountName - Pod的Service Account的名字metadata.uid - Pod的UIDmetadata.labels[&apos;&lt;KEY&gt;&apos;] - 指定&lt;KEY&gt;的Label值metadata.annotations[&apos;&lt;KEY&gt;&apos;] - 指定&lt;KEY&gt;的Annotation值metadata.labels - Pod的所有Labelmetadata.annotations - Pod的所有Annotation2. 使用resourceFieldRef可以声明使用:容器的CPU limit容器的CPU request容器的memory limit容器的memory request Service Accountdefault Service Account 12345678910$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lwContainers:... Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)Volumes: default-token-s8rbq: Type: Secret (a volume populated by a Secret) SecretName: default-token-s8rbq Optional: false 12$ ls /var/run/secrets/kubernetes.io/serviceaccount ca.crt namespace token NodeSelector是一个供用户将 Pod 与 Node 进行绑定的字段 1234567apiVersion: v1kind: Pod...spec: nodeSelector: disktype: ssd NodeName一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。 12345apiVersion: v1kind: Pod...spec: nodeName: k8s.node1 #指定调度节点为k8s.node1 HostAliases定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下： 1234567891011apiVersion: v1kind: Pod...spec: hostAliases: - ip: &quot;10.1.2.3&quot; hostnames: - &quot;foo.remote&quot; - &quot;bar.remote&quot;... 12345678cat /etc/hosts# Kubernetes-managed hosts file.127.0.0.1 localhost...10.244.135.10 hostaliases-pod10.1.2.3 foo.remote10.1.2.3 bar.remote shareProcessNamespace=true Pod 里的容器要共享 PID Namespace。 12345678910111213apiVersion: v1kind: Podmetadata: name: nginxspec: shareProcessNamespace: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true 12$ kubectl create -f nginx.yaml 123456789$ kubectl attach -it nginx -c shell/ # ps axPID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off; 14 101 0:00 nginx: worker process 15 root 0:00 sh 21 root 0:00 ps ax 共享宿主机的Linux namespace123456789101112131415apiVersion: v1kind: Podmetadata: name: nginxspec: hostNetwork: true hostIPC: true hostPID: true containers: - name: nginx image: nginx - name: shell image: busybox stdin: true tty: true Probe这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。 ####livenessProbe 123456789101112131415161718192021apiVersion: v1kind: Podmetadata: labels: test: liveness name: test-liveness-execspec: containers: - name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 1$ kubectl create -f test-liveness-exec.yaml 123$ kubectl get podNAME READY STATUS RESTARTS AGEtest-liveness-exec 1/1 Running 0 10s 可以看到，由于已经通过了健康检查，这个 Pod 就进入了 Running 状态。 而 30 s 之后，我们再查看一下 Pod 的 Events： 1$ kubectl describe pod test-liveness-exec 1234你会发现，这个 Pod 在 Events 报告了一个异常：FirstSeen LastSeen Count From SubobjectPath Type Reason Message--------- -------- ----- ---- ------------- -------- ------ -------2s 2s 1 &#123;kubelet worker0&#125; spec.containers&#123;liveness&#125; Warning Unhealthy Liveness probe failed: cat: can&apos;t open &apos;/tmp/healthy&apos;: No such file or directory 123$ kubectl get pod test-liveness-execNAME READY STATUS RESTARTS AGEliveness-exec 1/1 Running 1 1m 这时我们发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态。这是为什么呢？其实，如果你注意到 RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了。在这个过程中，Pod 保持 Running 状态不变。 除了在容器中执行命令外，livenessProbe 也可以定义为发起 HTTP 或者 TCP 请求的方式，定义格式如下： 12345678910...livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome initialDelaySeconds: 3 periodSeconds: 3 123456... livenessProbe: tcpSocket: port: 8080 initialDelaySeconds: 15 periodSeconds: 20 readinessProbe虽然它的用法与 livenessProbe 类似，但作用却大不一样。readinessProbe 检查结果的成功与否，决定的这个 Pod 是不是能被通过 Service 的方式访问到，而并不影响 Pod 的生命周期。 restartPolicy它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。 但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。但是Deployment可以跑到别的节点上去。 而作为用户，你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况： Always：在任何情况下，只要容器不在运行状态，就自动重启容器； OnFailure: 只在容器 异常时才自动重启容器； Never: 从来不重启容器。 实际上，你根本不需要死记硬背这些对应关系，只要记住如下两个基本的设计原理即可 只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。 对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数，比如 123$ kubectl get pod test-liveness-execNAME READY STATUS RESTARTS AGEliveness-exec 0/1 Running 1 1m PodPreset举个例子，现在开发人员编写了如下一个 pod.yaml 文件： 12345678910111213apiVersion: v1kind: Podmetadata: name: website labels: app: website role: frontendspec: containers: - name: website image: nginx ports: - containerPort: 80 这个时候，运维人员就可以定义一个 PodPreset 对象。在这个对象中，凡是他想在开发人员编写的 Pod 里追加的字段，都可以预先定义好。比如这个 preset.yaml: 1234567891011121314151617apiVersion: settings.k8s.io/v1alpha1kind: PodPresetmetadata: name: allow-databasespec: selector: matchLabels: role: frontend env: - name: DB_PORT value: &quot;6379&quot; volumeMounts: - mountPath: /cache name: cache-volume volumes: - name: cache-volume emptyDir: &#123;&#125; 12$ kubectl create -f preset.yaml$ kubectl create -f pod.yaml 12345678910111213141516171819202122232425$ kubectl get pod website -o yamlapiVersion: v1kind: Podmetadata: name: website labels: app: website role: frontend annotations: podpreset.admission.kubernetes.io/podpreset-allow-database: &quot;resource version&quot;spec: containers: - name: website image: nginx volumeMounts: - mountPath: /cache name: cache-volume ports: - containerPort: 80 env: - name: DB_PORT value: &quot;6379&quot; volumes: - name: cache-volume emptyDir: &#123;&#125; Container 属性ImagePullPolicy它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。 ImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。而如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。 Lifecycle它定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子： 12345678910111213141516apiVersion: v1kind: Podmetadata: name: lifecycle-demospec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;] preStop: exec: command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;] 作业副本与水平扩展ReplicaSet 结构12345678910111213141516171819apiVersion: apps/v1kind: ReplicaSetmetadata: name: nginx-set labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 Deployment结构123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 Deployment和ReplicaSet关系 部署deployment12$ kubectl create -f nginx-deployment.yaml --record 1234$ kubectl get deploymentsNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEnginx-deployment 3 0 0 0 1s 1234$ kubectl rollout status deployment/nginx-deploymentWaiting for rollout to finish: 2 out of 3 new replicas have been updated...deployment.apps/nginx-deployment successfully rolled out 查看replicaSet状态1234$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-3167673210 3 3 3 20s edit deployment12345678910$ kubectl edit deployment/nginx-deployment... spec: containers: - name: nginx image: nginx:1.9.1 # 1.7.9 -&gt; 1.9.1 ports: - containerPort: 80...deployment.extensions/nginx-deployment edited 查看deployment 状态变化 1234$ kubectl rollout status deployment/nginx-deploymentWaiting for rollout to finish: 2 out of 3 new replicas have been updated...deployment.extensions/nginx-deployment successfully rolled out 也可以查看deployment的event 12345678910111213$ kubectl describe deployment nginx-deployment...Events: Type Reason Age From Message ---- ------ ---- ---- -------... Normal ScalingReplicaSet 24s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 1 Normal ScalingReplicaSet 22s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 2 Normal ScalingReplicaSet 22s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 2 Normal ScalingReplicaSet 19s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 1 Normal ScalingReplicaSet 19s deployment-controller Scaled up replica set nginx-deployment-1764197365 to 3 Normal ScalingReplicaSet 14s deployment-controller Scaled down replica set nginx-deployment-3167673210 to 0 在这个“滚动更新”过程完成之后，你可以查看一下新、旧两个 ReplicaSet 的最终状态： 1234$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-1764197365 3 3 3 6snginx-deployment-3167673210 0 0 0 30s scale12$ kubectl scale deployment nginx-deployment --replicas=4deployment.apps/nginx-deployment scaled RollingUpdateStrategy为了进一步保证服务的连续性，Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。 1234567891011121314apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec:... strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。同时，这两个配置还可以用前面我们介绍的百分比形式来表示。 kubectl set image 的指令，直接修改 nginx-deployment 所使用的镜像。这个命令的好处就是，你可以不用像 kubectl edit 那样需要打开编辑器。不过这一次，我把这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。 123$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91deployment.extensions/nginx-deployment image updated 这时来检查下replicaSet的状态 123456$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-deployment-1764197365 2 2 2 24snginx-deployment-3167673210 0 0 0 35snginx-deployment-2156724341 2 2 0 7s 回滚到上一个版本123$ kubectl rollout undo deployment/nginx-deploymentdeployment.extensions/nginx-deployment 回滚到指定版本1234567$ kubectl rollout history deployment/nginx-deploymentdeployments &quot;nginx-deployment&quot;REVISION CHANGE-CAUSE1 kubectl create -f nginx-deployment.yaml --record2 kubectl edit deployment/nginx-deployment3 kubectl set image deployment/nginx-deployment nginx=nginx:1.91 当然，你还可以通过这个 kubectl rollout history 指令，看到每个版本对应的 Deployment 的 API 对象的细节，具体命令如下所示： 1$ kubectl rollout history deployment/nginx-deployment --revision=2 回滚到指定版本 12$ kubectl rollout undo deployment/nginx-deployment --to-revision=2deployment.extensions/nginx-deployment 多次修改deployment只生成一个rs你可能已经想到了一个问题：我们对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象，是不是有些多余，甚至浪费资源呢？ 123$ kubectl rollout pause deployment/nginx-deploymentdeployment.extensions/nginx-deployment paused 这个 kubectl rollout pause 的作用，是让这个 Deployment 进入了一个“暂停”状态。 所以接下来，你就可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。 由于此时 Deployment 正处于“暂停”状态，所以我们对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet。 12 而等到我们对 Deployment 修改操作都完成之后，只需要再执行一条 kubectl rollout resume 指令，就可以把这个 Deployment“恢复”回来，如下所示： 123$ kubectl rollout resume deployment/nginx-deploymentdeployment.extensions/nginx-deployment resumed 查看rs状态 12345$ kubectl get rsNAME DESIRED CURRENT READY AGEnginx-1764197365 0 0 0 2mnginx-3196763511 3 3 3 28s 控制这些“历史”ReplicaSet 的数量呢？很简单，Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了。 StatefulSet拓扑状态svc.yaml 12345678910111213apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginx 当你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录，如下所示： 12&lt;pod-name&gt;.&lt;svc-name&gt;.&lt;namespace&gt;.svc.cluster.local statefulset.yaml 这个 YAML 文件，和我们在前面文章中用到的 nginx-deployment 的唯一区别，就是多了一个 serviceName=nginx 字段。这个字段的作用，就是告诉 StatefulSet 控制器，在执行控制循环（Control Loop）的时候，请使用 nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。 123456789101112131415161718192021apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: serviceName: &quot;nginx&quot; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web 所以，当你通过 kubectl create 创建了上面这个 Service 和 StatefulSet 之后，就会看到如下两个对象 123456789$ kubectl create -f svc.yaml$ kubectl get service nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx ClusterIP None &lt;none&gt; 80/TCP 10s$ kubectl create -f statefulset.yaml$ kubectl get statefulset webNAME DESIRED CURRENT AGEweb 2 1 19s 这时候，如果你手比较快的话，还可以通过 kubectl 的 -w 参数，即：Watch 功能，实时查看 StatefulSet 创建两个有状态实例的过程： 12345678910$ kubectl get pods -w -l app=nginxNAME READY STATUS RESTARTS AGEweb-0 0/1 Pending 0 0sweb-0 0/1 Pending 0 0sweb-0 0/1 ContainerCreating 0 0sweb-0 1/1 Running 0 19sweb-1 0/1 Pending 0 0sweb-1 0/1 Pending 0 0sweb-1 0/1 ContainerCreating 0 0sweb-1 1/1 Running 0 20s 我们使用 kubectl exec 命令进入到容器中查看它们的 hostname： 1234$ kubectl exec web-0 -- sh -c &apos;hostname&apos;web-0$ kubectl exec web-1 -- sh -c &apos;hostname&apos;web-1 通过这条命令，我们启动了一个一次性的 Pod，因为–rm 意味着 Pod 退出后就会被删除掉。然后，在这个 Pod 的容器里面，我们尝试用 nslookup 命令，解析一下 Pod 对应的 Headless Service： 1234567891011121314$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh$ nslookup web-0.nginxServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.localName: web-0.nginxAddress 1: 10.244.1.7$ nslookup web-1.nginxServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.localName: web-1.nginxAddress 1: 10.244.2.7 删掉pod看看会出现什么情况 123$ kubectl delete pod -l app=nginxpod &quot;web-0&quot; deletedpod &quot;web-1&quot; deleted 123456789$ kubectl get pod -w -l app=nginxNAME READY STATUS RESTARTS AGEweb-0 0/1 ContainerCreating 0 0sNAME READY STATUS RESTARTS AGEweb-0 1/1 Running 0 2sweb-1 0/1 Pending 0 0sweb-1 0/1 ContainerCreating 0 0sweb-1 1/1 Running 0 32s 可以看到，当我们把这两个 Pod 删除之后，Kubernetes 会按照原先编号的顺序，创建出了两个新的 Pod。并且，Kubernetes 依然为它们分配了与原来相同的“网络身份”：web-0.nginx 和 web-1.nginx。 123456789101112131415$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh $ nslookup web-0.nginxServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.localName: web-0.nginxAddress 1: 10.244.1.8$ nslookup web-1.nginxServer: 10.0.0.10Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.localName: web-1.nginxAddress 1: 10.244.2.8 不过，相信你也已经注意到了，尽管 web-0.nginx 这条记录本身不会变，但它解析到的 Pod 的 IP 地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式，而绝不应该直接访问这些 Pod 的 IP 地址。 存储状态PVC如果你还是不太理解 PVC 的话，可以先记住这样一个结论：PVC 其实就是一种特殊的 Volume。只不过一个 PVC 具体是什么类型的 Volume，要在跟某个 PV 绑定之后才知道。 12345678910kind: PersistentVolumeClaimapiVersion: v1metadata: name: pv-claimspec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 在Pod中声明这个PVC12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pv-podspec: containers: - name: pv-container image: nginx ports: - containerPort: 80 name: &quot;http-server&quot; volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: pv-storage volumes: - name: pv-storage persistentVolumeClaim: claimName: pv-claim PV可以看到，在这个 Pod 的 Volumes 定义中，我们只需要声明它的类型是 persistentVolumeClaim，然后指定 PVC 的名字，而完全不必关心 Volume 本身的定义。 这时候，只要我们创建这个 PVC 对象，Kubernetes 就会自动为它绑定一个符合条件的 Volume。可是，这些符合条件的 Volume 又是从哪里来的呢？ 答案是，它们来自于由运维人员维护的 PV（Persistent Volume）对象。接下来，我们一起看一个常见的 PV 对象的 YAML 文件。 pv.yaml 12345678910111213141516171819202122kind: PersistentVolumeapiVersion: v1metadata: name: pv-volume labels: type: localspec: capacity: storage: 10Gi accessModes: - ReadWriteOnce rbd: monitors: - &apos;rook-ceph-mon-a-698fc4d55f-q4x8d&apos; - &apos;rook-ceph-mon-b-7cf9c7bcb7-wjld6&apos; - &apos;rook-ceph-mon-c-5fb4dfb454-gnrll&apos; pool: kube image: foo fsType: ext4 readOnly: true user: admin keyring: /etc/ceph/keyring –注monitors –修改为 kubectl get pods -n rook-ceph 查看 rook-ceph-mon- 开头的Pod id 所以，Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。 StatefulSet 集成PVC123456789101112131415161718192021222324252627282930313233apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: serviceName: &quot;nginx&quot; replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi create 1kubectl create -f pv.yaml 12345$ kubectl create -f statefulset.yaml$ kubectl get pvc -l app=nginxNAME STATUS VOLUME CAPACITY ACCESSMODES AGEwww-web-0 Bound pvc-15c268c7-b507-11e6-932f-42010a800002 1Gi RWO 48swww-web-1 Bound pvc-15c79307-b507-11e6-932f-42010a800002 1Gi RWO 48s 12$ for i in 0 1; do kubectl exec web-$i -- sh -c &apos;echo hello $(hostname) &gt; /usr/share/nginx/html/index.html&apos;; done 1234$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; donehello web-0hello web-1 1234# 删除pod$ kubectl delete pod -l app=nginxpod &quot;web-0&quot; deletedpod &quot;web-1&quot; deleted 1234# 在被重新创建出来的Pod容器里访问http://localhost$ kubectl exec -it web-0 -- curl localhosthello web-0 daemonSet滚动更新123$ kubectl patch statefulset mysql --type=&apos;json&apos; -p=&apos;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;mysql:5.7.23&quot;&#125;]&apos;statefulset.apps/mysql patched 灰度发布123$ kubectl patch statefulset mysql -p &apos;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:2&#125;&#125;&#125;&#125;&apos;statefulset.apps/mysql patched 这样，我就指定了当 Pod 模板发生变化的时候，比如 MySQL 镜像更新到 5.7.23，那么只有序号大于或者等于 2 的 Pod 会被更新到这个版本。并且，如果你删除或者重启了序号小于 2 的 Pod，等它再次启动后，也会保持原先的 5.7.2 版本，绝不会被升级到 5.7.23 版本。 deamonSet结构12345678910111213141516171819202122232425262728293031323334353637383940414243apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-loggingspec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers nodeAffinity在 Kubernetes 项目里，nodeSelector 其实已经是一个将要被废弃的字段了。因为，现在有了一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子： 1234567891011121314apiVersion: v1kind: Podmetadata: name: with-node-affinityspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: metadata.name operator: In values: - node-geektime TolerationDaemonSet 的“过人之处”，其实就是依靠 Toleration 实现的。假如当前 DaemonSet 管理的，是一个网络插件的 Agent Pod，那么你就必须在这个 DaemonSet 的 YAML 文件里，给它的 Pod 模板加上一个能够“容忍”node.kubernetes.io/network-unavailable“污点”的 Toleration。正如下面这个例子所示： 1234567891011...template: metadata: labels: name: network-plugin-agent spec: tolerations: - key: node.kubernetes.io/network-unavailable operator: Exists effect: NoSchedule daemonSet 如何确保能在在mater上部署 1234tolerations:- key: node-role.kubernetes.io/master effect: NoSchedule 创建deamonSet12$ kubectl create -f fluentd-elasticsearch.yaml 12345$ kubectl get pod -n kube-system -l name=fluentd-elasticsearchNAME READY STATUS RESTARTS AGEfluentd-elasticsearch-dqfv9 1/1 Running 0 53mfluentd-elasticsearch-pf9z5 1/1 Running 0 53m 1234$ kubectl get ds -n kube-system fluentd-elasticsearchNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEfluentd-elasticsearch 2 2 2 2 2 &lt;none&gt; 1h 回滚daemonSet12345$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-systemdaemonsets &quot;fluentd-elasticsearch&quot;REVISION CHANGE-CAUSE1 &lt;none&gt; 滚动更新 12$ kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record -n=kube-system 查看滚动更新状态 123456$ kubectl rollout status ds/fluentd-elasticsearch -n kube-systemWaiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 0 out of 2 new pods have been updated...Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 0 out of 2 new pods have been updated...Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 1 of 2 updated pods are available...daemon set &quot;fluentd-elasticsearch&quot; successfully rolled out 123456$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-systemdaemonsets &quot;fluentd-elasticsearch&quot;REVISION CHANGE-CAUSE1 &lt;none&gt;2 kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --namespace=kube-system --record=true ControllerRevision对象而我在前面的文章中讲解 Deployment 对象的时候，曾经提到过，Deployment 管理这些版本，靠的是“一个版本对应一个 ReplicaSet 对象”。 可是，DaemonSet 控制器操作的直接就是 Pod，不可能有 ReplicaSet 这样的对象参与其中。那么，它的这些版本又是如何维护的呢？所谓，一切皆对象！在 Kubernetes 项目中，任何你觉得需要记录下来的状态，都可以被用 API 对象的方式实现。当然，“版本”也不例外。Kubernetes v1.7 之后添加了一个 API 对象，名叫 ControllerRevision 1234$ kubectl get controllerrevision -n kube-system -l name=fluentd-elasticsearchNAME CONTROLLER REVISION AGEfluentd-elasticsearch-64dc6799c9 daemonset.apps/fluentd-elasticsearch 2 1h 12345678910111213141516171819202122232425$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 -n kube-systemName: fluentd-elasticsearch-64dc6799c9Namespace: kube-systemLabels: controller-revision-hash=2087235575 name=fluentd-elasticsearchAnnotations: deprecated.daemonset.template.generation=2 kubernetes.io/change-cause=kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record=true --namespace=kube-systemAPI Version: apps/v1Data: Spec: Template: $ Patch: replace Metadata: Creation Timestamp: &lt;nil&gt; Labels: Name: fluentd-elasticsearch Spec: Containers: Image: k8s.gcr.io/fluentd-elasticsearch:v2.2.0 Image Pull Policy: IfNotPresent Name: fluentd-elasticsearch...Revision: 2Events: &lt;none&gt; 12$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision=1 -n kube-systemdaemonset.extensions/fluentd-elasticsearch rolled back 执行完这次回滚完成后，你会发现，DaemonSet 的 Revision 并不会从 Revision=2 退回到 1，而是会增加成 Revision=3。这是因为，一个新的 ControllerRevision 被创建了出来。 Job1234567891011121314apiVersion: batch/v1kind: Jobmetadata: name: pispec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo &apos;scale=10000; 4*a(1)&apos; | bc -l &quot;] restartPolicy: Never backoffLimit: 4 job里的restartPolicy只有两种选择： restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。 1234spec: backoffLimit: 5 #最多失败几次就不在创建Pod默认是6 activeDeadlineSeconds: 100 #最长运行时间 123spec: parallelism: 2 #同一时间最多有几个Pod执行 completions: 4 #总共执行多少个Pod CronJob格式12345678910111213141516171819apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 这个 Cron 表达式里 /1 中的 表示从 0 开始，/ 表示“每”，1 表示偏移量。 所以，它的意思就是：从 0 开始，每 1 个时间单位执行一次。那么，时间单位又是什么呢？Cron 表达式中的五个部分分别代表： 分钟、小时、日、月、星期。 12345678$ kubectl create -f ./cronjob.yamlcronjob &quot;hello&quot; created# 一分钟后$ kubectl get jobsNAME DESIRED SUCCESSFUL AGEhello-4111706356 1 1 2s 1234$ kubectl get cronjob helloNAME SCHEDULE SUSPEND ACTIVE LAST-SCHEDULEhello */1 * * * * False 0 Thu, 6 Sep 2018 14:34:00 -070 spec.concurrencyPolicy需要注意的是，由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如： concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在； concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过； concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。 spec.startingDeadlineSeconds而如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job。这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。 Initializer（Dynamic Admission Control）istio架构图 Istio 最根本的组件，是运行在每一个应用 Pod 里的 Envoy 容器 这个 Envoy 项目是 Lyft 公司推出的一个高性能 C++ 网络代理，也是 Lyft 公司对 Istio 项目的唯一贡献。 而 Istio 项目，则把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。我们知道，Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。 这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。 Dynamic Admission ControlIstio 项目明明需要在每个 Pod 里安装一个 Envoy 容器，又怎么能做到“无感”的呢？ 实际上，Istio 项目使用的，是 Kubernetes 中的一个非常重要的功能，叫作 Dynamic Admission Control。 在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，总有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。 原理举个例子 123456789101112apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: containers: - name: myapp-container image: busybox command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo Hello Kubernetes! &amp;&amp; sleep 3600&apos;] Istio 项目要做的，就是在这个 Pod YAML 被提交给 Kubernetes 之后，在它对应的 API 对象里自动加上 Envoy 容器的配置，使这个对象变成如下所示的样子 12345678910111213141516apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myappspec: containers: - name: myapp-container image: busybox command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo Hello Kubernetes! &amp;&amp; sleep 3600&apos;] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [&quot;/usr/local/bin/envoy&quot;] ... 如何做到的呢 首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。这个 ConfigMap（名叫：envoy-initializer）的定义如下所示： 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: ConfigMapmetadata: name: envoy-initializerdata: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [&quot;/usr/local/bin/envoy&quot;] args: - &quot;--concurrency 4&quot; - &quot;--config-path /etc/envoy/envoy.json&quot; - &quot;--mode serve&quot; ports: - containerPort: 80 protocol: TCP resources: limits: cpu: &quot;1000m&quot; memory: &quot;512Mi&quot; requests: cpu: &quot;100m&quot; memory: &quot;64Mi&quot; volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy 接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。这个 Pod 的定义非常简单，如下所示： 123456789101112apiVersion: v1kind: Podmetadata: labels: app: envoy-initializer name: envoy-initializerspec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always 这个pod会做什么工作呢 如果这个 Pod 里面已经添加过 Envoy 容器，那么就“放过”这个 Pod，进入下一个检查周期。 而如果还没有添加过 Envoy 容器的话，它就要进行 Initialize 操作了，即：修改该 Pod 的 API 对象。Initializer 的代码拿到config-map里的data数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求。这样，一个用户提交的 Pod 对象里，就会被自动加上 Envoy 容器相关的字段。 InitializerConfiguration当然，Kubernetes 还允许你通过配置，来指定要对什么样的资源进行这个 Initialize 操作，比如下面这个例子： 123456789101112131415apiVersion: admissionregistration.k8s.io/v1alpha1kind: InitializerConfigurationmetadata: name: envoy-configinitializers: // 这个名字必须至少包括两个 &quot;.&quot; - name: envoy.initializer.kubernetes.io rules: - apiGroups: - &quot;&quot; // 前面说过， &quot;&quot;就是core API Group的意思 apiVersions: - v1 resources: - pods 这个配置，就意味着 Kubernetes 要对所有的 Pod 进行这个 Initialize 操作，并且，我们指定了负责这个操作的 Initializer，名叫：envoy-initializer。 而一旦这个 InitializerConfiguration 被创建，Kubernetes 就会把这个 Initializer 的名字，加在所有新创建的 Pod 的 Metadata 上，格式如下所示： 12345678910apiVersion: v1kind: Podmetadata: initializers: pending: - name: envoy.initializer.kubernetes.io name: myapp-pod labels: app: myapp... 此外，除了上面的配置方法，你还可以在具体的 Pod 的 Annotation 里添加一个如下所示的字段，从而声明要使用某个 Initializer： 123456apiVersion: v1kind: Podmetadata annotations: &quot;initializer.kubernetes.io/envoy&quot;: &quot;true&quot; ... 在这个 Pod 里，我们添加了一个 Annotation，写明： initializer.kubernetes.io/envoy=true。这样，就会使用到我们前面所定义的 envoy-initializer 了。 API对象API对象结构 构建过程 自定义API资源CR（Custom Resource）12345678apiVersion: samplecrd.k8s.io/v1kind: Networkmetadata: name: example-networkspec: cidr: &quot;192.168.0.0/16&quot; gateway: &quot;192.168.0.1&quot; CRD（Custom Resource Definition）这就是一个 Network API 资源类型的 API 部分的宏观定义了。这就等同于告诉了计算机：“兔子是哺乳动物”。所以这时候，Kubernetes 就能够认识和处理所有声明了 API 类型是“samplecrd.k8s.io/v1/network”的 YAML 文件了 1234567891011apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata: name: networks.samplecrd.k8s.iospec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced 接下来，我还需要让 Kubernetes“认识”这种 YAML 文件里描述的“网络”部分，比如“cidr”（网段），“gateway”（网关）这些字段的含义。这就相当于我要告诉计算机：“兔子有长耳朵和三瓣嘴”。 编写代码代码结构12345678910111213141516$ tree $GOPATH/src/github.com/&lt;your-name&gt;/k8s-controller-custom-resource.├── controller.go├── crd│ └── network.yaml├── example│ └── example-network.yaml├── main.go└── pkg └── apis └── samplecrd ├── register.go └── v1 ├── doc.go ├── register.go └── types.go 其中，pkg/apis/samplecrd 就是 API 组的名字，v1 是版本，而 v1 下面的 types.go 文件里，则定义了 Network 对象的完整描述。 pkg/apis/samplecrd/register.goregister.go,用来放置后面要用到的全局变量。这个文件的内容如下所示： 123456package samplecrdconst ( GroupName = &quot;samplecrd.k8s.io&quot; Version = &quot;v1&quot;) pkg/apis/samplecrd/v1/doc.godoc.go 文件（Golang 的文档源文件）。这个文件里的内容如下所示： 1234// +k8s:deepcopy-gen=package// +groupName=samplecrd.k8s.iopackage v1 其中，+k8s:deepcopy-gen=package 意思是，请为整个 v1 包里的所有类型定义自动生成 DeepCopy 方法；而+groupName=samplecrd.k8s.io，则定义了这个包对应的 API 组的名字。可以看到，这些定义在 doc.go 文件的注释，起到的是全局的代码生成控制的作用，所以也被称为 Global Tags。 pkg/apis/samplecrd/v1/types.go接下来，我需要添加 types.go 文件。顾名思义，它的作用就是定义一个 Network 类型到底有哪些字段（比如，spec 字段里的内容）。这个文件的主要内容如下所示： 123456789101112131415161718192021222324252627282930313233343536package v1...// +genclient// +genclient:noStatus// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// Network describes a Network resourcetype Network struct &#123; // TypeMeta is the metadata for the resource, like kind and apiversion metav1.TypeMeta `json:",inline"` // ObjectMeta contains the metadata for the particular object, including // things like... // - name // - namespace // - self link // - labels // - ... etc ... metav1.ObjectMeta `json:"metadata,omitempty"` Spec networkspec `json:"spec"`&#125;// networkspec is the spec for a Network resourcetype networkspec struct &#123; Cidr string `json:"cidr"` Gateway string `json:"gateway"`&#125;// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object// NetworkList is a list of Network resourcestype NetworkList struct &#123; metav1.TypeMeta `json:",inline"` metav1.ListMeta `json:"metadata"` Items []Network `json:"items"`&#125; +genclient 的意思是：请为下面这个 API 资源类型生成对应的 Client 代码,需要注意的是，+genclient 只需要写在 Network 类型上，而不用写在 NetworkList 上。因为 NetworkList 只是一个返回值类型，Network 才是“主类型”。 +genclient:noStatus 的意思是：这个 API 资源类型定义里，没有 Status 字段。否则，生成的 Client 就会自动带上 UpdateStatus 方法。 +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object的注释。它的意思是，请在生成 DeepCopy 的时候，实现 Kubernetes 提供的 runtime.Object 接口。否则，在某些版本的 Kubernetes 里，你的这个类型定义会出现编译错误。这是一个固定的操作，记住即可。 pkg/apis/samplecrd/v1/register.go在前面对 APIServer 工作原理的讲解中，我已经提到，“registry”的作用就是注册一个类型（Type）给 APIServer。其中，Network 资源类型在服务器端的注册的工作，APIServer 会自动帮我们完成。但与之对应的，我们还需要让客户端也能“知道”Network 资源类型的定义。这就需要我们在项目里添加一个 register.go 文件。它最主要的功能，就是定义了如下所示的 addKnownTypes() 方法： 12345678910111213141516package v1...// addKnownTypes adds our types to the API scheme by registering// Network and NetworkListfunc addKnownTypes(scheme *runtime.Scheme) error &#123; scheme.AddKnownTypes( SchemeGroupVersion, &amp;Network&#123;&#125;, &amp;NetworkList&#123;&#125;, ) // register the type in the scheme metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil&#125; 有了这个方法，Kubernetes 就能够在后面生成客户端的时候，“知道”Network 以及 NetworkList 类型的定义了。 自动生成代码接下来，我就要使用 Kubernetes 提供的代码生成工具，为上面定义的 Network 资源类型自动生成 clientset、informer 和 lister。 这个代码生成工具名叫k8s.io/code-generator，使用方法如下所示： 设置环境变量,把以下环境变量写到/etc/environment 1234567891011vim /etc/environmentGOPATH=/root/go# 代码生成的工作目录，也就是我们的项目路径ROOT_PACKAGE=&quot;github.com/shiyangtao/k8s-controller-custom-resource&quot;# API GroupCUSTOM_RESOURCE_NAME=&quot;samplecrd&quot;# API VersionCUSTOM_RESOURCE_VERSION=&quot;v1&quot;source /etc/environment 安装k8s.io/code-generator 1234567$ go get -u k8s.io/code-generator/...# 如果go get有问题的话换一种方式mkdir $GOPATH/src/k8s.iocd $GOPATH/src/k8s.iogit clone https://github.com/kubernetes/code-generator.gitrm -rf text/.git 下载代码 代码路径 $GOPATH/src/github.com/shiyangtao/k8s-controller-custom-resource 123mkdir -p $GOPATH/src/github.com/shiyangtao/cd $GOPATH/src/github.com/shiyangtao/git clone https://github.com/resouer/k8s-controller-custom-resource.git 生成代码 123$ cd $GOPATH/src/k8s.io/code-generator# 执行代码自动生成，其中pkg/client是生成目标目录，pkg/apis是类型定义目录$ ./generate-groups.sh all &quot;$ROOT_PACKAGE/pkg/client&quot; &quot;$ROOT_PACKAGE/pkg/apis&quot; &quot;$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION&quot; 如果有报错试着执行下 这段代码go get -u k8s.io/apimachinery 代码生成工作完成之后，我们再查看一下这个项目的目录结构： 12345678910111213141516171819202122$ tree.├── controller.go├── crd│ └── network.yaml├── example│ └── example-network.yaml├── main.go└── pkg ├── apis │ └── samplecrd │ ├── constants.go │ └── v1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go └── client ├── clientset ├── informers └── listers 其中，pkg/apis/samplecrd/v1 下面的 zz_generated.deepcopy.go 文件，就是自动生成的 DeepCopy 代码文件。 而整个 client 目录，以及下面的三个包（clientset、informers、 listers），都是 Kubernetes 为 Network 类型生成的客户端库，这些库会在后面编写自定义控制器的时候用到。 而有了这些内容，现在你就可以在 Kubernetes 集群里创建一个 Network 类型的 API 对象了。我们不妨一起来实验一下。首先，使用 network.yaml 文件，在 Kubernetes 中创建 Network 对象的 CRD（Custom Resource Definition） 123$ kubectl apply -f crd/network.yamlcustomresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created 然后，我们就可以创建一个 Network 对象了，这里用到的是 example-network.yaml： 123$ kubectl apply -f example/example-network.yaml network.samplecrd.k8s.io/example-network created 通过这个操作，你就在 Kubernetes 集群里创建了一个 Network 对象。它的 API 资源路径是samplecrd.k8s.io/v1/networks。 1234$ kubectl get networkNAME AGEexample-network 8s 123456789101112131415$ kubectl describe network example-networkName: example-networkNamespace: defaultLabels: &lt;none&gt;...API Version: samplecrd.k8s.io/v1Kind: NetworkMetadata: ... Generation: 1 Resource Version: 468239 ...Spec: Cidr: 192.168.0.0/16 Gateway: 192.168.0.1 自定义控制器编写main函数12345678910111213141516171819202122func main() &#123; ... cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) ... kubeClient, err := kubernetes.NewForConfig(cfg) ... networkClient, err := clientset.NewForConfig(cfg) ... networkInformerFactory := informers.NewSharedInformerFactory(networkClient, ...) controller := NewController(kubeClient, networkClient, networkInformerFactory.Samplecrd().V1().Networks()) go networkInformerFactory.Start(stopCh) if err = controller.Run(2, stopCh); err != nil &#123; glog.Fatalf("Error running controller: %s", err.Error()) &#125;&#125; 第一步：main 函数根据我提供的 Master 配置（APIServer 的地址端口和 kubeconfig 的路径），创建一个 Kubernetes 的 client（kubeClient）和 Network 对象的 client（networkClient）。但是，如果我没有提供 Master 配置呢？这时，main 函数会直接使用一种名叫 InClusterConfig 的方式来创建这个 client。这个方式，会假设你的自定义控制器是以 Pod 的方式运行在 Kubernetes 集群里的。 第二步：main 函数为 Network 对象创建一个叫作 InformerFactory（即：networkInformerFactory）的工厂，并使用它生成一个 Network 对象的 Informer，传递给控制器。 第三步：main 函数启动上述的 Informer，然后执行 controller.Run，启动自定义控制器。至此，main 函数就结束了 编写自定义控制器的定义123456789101112131415161718192021222324252627func NewController( kubeclientset kubernetes.Interface, networkclientset clientset.Interface, networkInformer informers.NetworkInformer) *Controller &#123; ... controller := &amp;Controller&#123; kubeclientset: kubeclientset, networkclientset: networkclientset, networksLister: networkInformer.Lister(), networksSynced: networkInformer.Informer().HasSynced, workqueue: workqueue.NewNamedRateLimitingQueue(..., "Networks"), ... &#125; networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: controller.enqueueNetwork, UpdateFunc: func(old, new interface&#123;&#125;) &#123; oldNetwork := old.(*samplecrdv1.Network) newNetwork := new.(*samplecrdv1.Network) if oldNetwork.ResourceVersion == newNetwork.ResourceVersion &#123; return &#125; controller.enqueueNetwork(new) &#125;, DeleteFunc: controller.enqueueNetworkForDelete, return controller&#125; 编写控制器里的业务逻辑123456789101112131415func (c *Controller) Run(threadiness int, stopCh &lt;-chan struct&#123;&#125;) error &#123; ... if ok := cache.WaitForCacheSync(stopCh, c.networksSynced); !ok &#123; return fmt.Errorf("failed to wait for caches to sync") &#125; ... for i := 0; i &lt; threadiness; i++ &#123; go wait.Until(c.runWorker, time.Second, stopCh) &#125; ... return nil&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566func (c *Controller) runWorker() &#123; for c.processNextWorkItem() &#123; &#125;&#125;func (c *Controller) processNextWorkItem() bool &#123; obj, shutdown := c.workqueue.Get() ... err := func(obj interface&#123;&#125;) error &#123; ... if err := c.syncHandler(key); err != nil &#123; return fmt.Errorf("error syncing '%s': %s", key, err.Error()) &#125; c.workqueue.Forget(obj) ... return nil &#125;(obj) ... return true&#125;func (c *Controller) syncHandler(key string) error &#123; namespace, name, err := cache.SplitMetaNamespaceKey(key) ... network, err := c.networksLister.Networks(namespace).Get(name) if err != nil &#123; if errors.IsNotFound(err) &#123; glog.Warningf("Network does not exist in local cache: %s/%s, will delete it from Neutron ...", namespace, name) glog.Warningf("Network: %s/%s does not exist in local cache, will delete it from Neutron ...", namespace, name) // FIX ME: call Neutron API to delete this network by name. // // neutron.Delete(namespace, name) return nil &#125; ... return err &#125; glog.Infof("[Neutron] Try to process network: %#v ...", network) // FIX ME: Do diff(). // // actualNetwork, exists := neutron.Get(namespace, name) // // if !exists &#123; // neutron.Create(namespace, name) // &#125; else if !reflect.DeepEqual(actualNetwork, network) &#123; // neutron.Update(namespace, name) // &#125; return nil&#125; 执行控制器12345678910111213141516$ cd $GOPATH/src/github.com/shiyangtao/k8s-controller-custom-resource### Skip this part if you don&apos;t want to build# Install dependency$ go get github.com/tools/godep$ godep restore# Build$ go build -o samplecrd-controller .$ ./samplecrd-controller -kubeconfig=$HOME/.kube/config -alsologtostderr=trueI0915 12:50:29.051349 27159 controller.go:84] Setting up event handlersI0915 12:50:29.051615 27159 controller.go:113] Starting Network control loopI0915 12:50:29.051630 27159 controller.go:116] Waiting for informer caches to syncE0915 12:50:29.066745 27159 reflector.go:134] github.com/resouer/k8s-controller-custom-resource/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io)... 接下来可以打开另外一个命令行进行以下操作,观察下以上控制台的变化 123456789修改网段example/example-network.yaml$ kubectl apply -f example/example-network.yaml network.samplecrd.k8s.io/example-network configured删除$ kubectl delete -f example/example-network.yaml 所谓的 Informer，就是一个自带缓存和索引机制，可以触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。 Informer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。Reflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。 而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同。 在实际应用中，除了控制循环之外的所有代码，实际上都是 Kubernetes 为你自动生成的，即：pkg/client/{informers, listers, clientset}里的内容。 RBACRole12345678910kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: mynamespace name: example-rolerules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 类似的，Role 对象的 rules 字段也可以进一步细化。比如，你可以只针对某一个具体的对象进行权限设置，如下所示 12345rules:- apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;my-config&quot;] verbs: [&quot;get&quot;] 这个例子就表示，这条规则的“被作用者”，只对名叫“my-config”的 ConfigMap 对象，有进行 GET 操作的权限。 RoleBinding1234567891011121314kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: example-rolebinding namespace: mynamespacesubjects:- kind: User name: example-user apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 需要再次提醒的是，Role 和 RoleBinding 对象都是 Namespaced 对象（NamespacedObject），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。 ClusterRole对于非 Namespaced（Non-namespaced）对象（比如：Node），或者，某一个 Role 想要作用于所有的 Namespace 的时候，我们又该如何去做授权呢？ 123456789kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: example-clusterrolerules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 查看k8s内置clusterRole 1kubectl get clusterrole ClusterRoleBinding12345678910111213kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: example-clusterrolebindingsubjects:- kind: User name: example-user apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: example-clusterrole apiGroup: rbac.authorization.k8s.io User可以看到，这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 User，即 Kubernetes 里的用户。这个用户的名字是 example-user。 可是，在 Kubernetes 中，其实并没有一个叫作“User”的 API 对象。而且，我们在前面和部署使用 Kubernetes 的流程里，既不需要 User，也没有创建过 User。 这个 User 到底是从哪里来的呢？ 实际上，Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。或者，你也可以直接给 APIServer 指定一个用户名、密码文件。那么 Kubernetes 的授权系统，就能够从这个文件里找到对应的“用户”了。当然，在大多数私有的使用环境中，我们只要使用 Kubernetes 提供的内置“用户”，就足够了。 在大多数时候，我们其实都不太使用“用户”这个功能，而是直接使用 Kubernetes 里的“内置用户”。 ServiceAccount其实就是内置用户 定义一个ServiceAccount 12345apiVersion: v1kind: ServiceAccountmetadata: namespace: mynamespace name: example-sa 然后，我们通过编写 RoleBinding 的 YAML 文件，来为这个 ServiceAccount 分配权限： 12345678910111213kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: example-rolebinding namespace: mynamespacesubjects:- kind: ServiceAccount name: example-sa namespace: mynamespaceroleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io 1234$ kubectl create -f svc-account.yaml$ kubectl create -f role-binding.yaml$ kubectl create -f role.yaml 123456789101112$ kubectl get sa -n mynamespace -o yaml- apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2018-09-08T12:59:17Z name: example-sa namespace: mynamespace resourceVersion: &quot;409327&quot; ... secrets: - name: example-sa-token-vmfg6 可以看到，Kubernetes 会为一个 ServiceAccount 自动创建并分配一个 Secret 对象，即：上述 ServiceAcount 定义里最下面的 secrets 字段。这个 Secret，就是这个 ServiceAccount 对应的、用来跟 APIServer 进行交互的授权文件，我们一般称它为：Token。Token 文件的内容一般是证书或者密码，它以一个 Secret 对象的方式保存在 Etcd 当中。 这时候，用户的 Pod，就可以声明使用这个 ServiceAccount 了，比如下面这个例子： 12345678910apiVersion: v1kind: Podmetadata: namespace: mynamespace name: sa-token-testspec: containers: - name: nginx image: nginx:1.7.9 serviceAccountName: example-sa 1234567891011$ kubectl apply -f pod.yaml$ kubectl describe pod sa-token-test -n mynamespaceName: sa-token-testNamespace: mynamespace...Containers: nginx: ... Mounts: /var/run/secrets/kubernetes.io/serviceaccount from example-sa-token-vmfg6 (ro) 等这个 Pod 运行起来之后，我们就可以看到，该 ServiceAccount 的 token，也就是一个 Secret 对象，被 Kubernetes 自动挂载到了容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录下. 这时候，我们可以通过 kubectl exec 查看到这个目录里的文件： 1234$ kubectl exec -it sa-token-test -n mynamespace -- /bin/bashroot@sa-token-test:/# ls /var/run/secrets/kubernetes.io/serviceaccountca.crt namespace token defaultServiceAccount1234567891011121314151617181920212223242526272829$kubectl describe sa defaultName: defaultNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Image pull secrets: &lt;none&gt;Mountable secrets: default-token-s8rbqTokens: default-token-s8rbqEvents: &lt;none&gt;$ kubectl get secretNAME TYPE DATA AGEkubernetes.io/service-account-token 3 82d$ kubectl describe secret default-token-s8rbqName: default-token-s8rbqNamespace: defaultLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name=default kubernetes.io/service-account.uid=ffcb12b2-917f-11e8-abde-42010aa80002Type: kubernetes.io/service-account-tokenData====ca.crt: 1025 bytesnamespace: 7 bytestoken: &lt;TOKEN数据&gt; 可以看到，Kubernetes 会自动为默认 ServiceAccount 创建并绑定一个特殊的 Secret：它的类型是kubernetes.io/service-account-token；它的 Annotation 字段，声明了kubernetes.io/service-account.name=default，即这个 Secret 会跟同一 Namespace 下名叫 default 的 ServiceAccount 进行绑定。 Group如果你为 Kubernetes 配置了外部认证服务的话，这个“用户组”的概念就会由外部认证服务提供。 而对于 Kubernetes 的内置“用户”ServiceAccount 来说，上述“用户组”的概念也同样适用。 实际上，一个 ServiceAccount，在 Kubernetes 里对应的“用户”的名字是： 1system:serviceaccount:&lt;Namespace名字&gt;:&lt;ServiceAccount名字&gt; 而它对应的内置“用户组”的名字，就是： 1system:serviceaccounts:&lt;Namespace名字&gt; 比如，现在我们可以在 RoleBinding 里定义如下的 subjects： 12345subjects:- kind: Group name: system:serviceaccounts:mynamespace apiGroup: rbac.authorization.k8s.io 这就意味着这个 Role 的权限规则，作用于 mynamespace 里的所有 ServiceAccount。这就用到了“用户组”的概念。 12345subjects:- kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io 就意味着这个 Role 的权限规则，作用于整个系统里的所有 ServiceAccount。 所有Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。1234567891011kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:name: readonly-all-defaultsubjects:- kind: ServiceAccount name: system:serviceaccount:defaultroleRef:kind: ClusterRolename: viewapiGroup: rbac.authorization.k8s.io 最后，值得一提的是，在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。 一般来说，这些系统 ClusterRole，是绑定给 Kubernetes 系统组件对应的 ServiceAccount 使用的。比如，其中一个名叫 system:kube-scheduler 的 ClusterRole，定义的权限规则是 kube-scheduler（Kubernetes 的调度器组件）运行所需要的必要权限。你可以通过如下指令查看这些权限的列表： 12345678910111213$ kubectl describe clusterrole system:kube-schedulerName: system:kube-scheduler...PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- -----... services [] [] [get list watch] replicasets.apps [] [] [get list watch] statefulsets.apps [] [] [get list watch] replicasets.extensions [] [] [get list watch] poddisruptionbudgets.policy [] [] [get list watch] pods/status [] [] [patch update] 这个 system:kube-scheduler 的 ClusterRole，就会被绑定给 kube-system Namesapce 下名叫 kube-scheduler 的 ServiceAccount，它正是 Kubernetes 调度器的 Pod 声明使用的 ServiceAccount。 除此之外，Kubernetes 还提供了四个预先定义好的 ClusterRole 来供用户直接使用： cluster-admin； admin； edit； view。 通过它们的名字，你应该能大致猜出它们都定义了哪些权限。比如，这个名叫 view 的 ClusterRole，就规定了被作用者只有 Kubernetes API 的只读权限。而我还要提醒你的是，上面这个 cluster-admin 角色，对应的是整个 Kubernetes 项目中的最高权限（verbs=*），如下所示： 12345678910$ kubectl describe clusterrole cluster-admin -n kube-systemName: cluster-adminLabels: kubernetes.io/bootstrapping=rbac-defaultsAnnotations: rbac.authorization.kubernetes.io/autoupdate=truePolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- *.* [] [] [*] [*] [] [*] Operator可能你已经感觉到，在 Kubernetes 中，管理“有状态应用”是一个比较复杂的过程，尤其是编写 Pod 模板的时候，总有一种“在 YAML 文件里编程序”的感觉，让人很不舒服。 而在 Kubernetes 生态中，还有一个相对更加灵活和编程友好的管理“有状态应用”的解决方案，它就是：Operator。 接下来，我就以 Etcd Operator 为例，来为你讲解一下 Operator 的工作原理和编写方法。Operator的原理就是自定义控制器。 构建步骤第一步，将这个 Operator 的代码 Clone 到本地：1$ git clone https://github.com/coreos/etcd-operator 第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。12$ example/rbac/create_role.sh 这个脚本的作用，就是为 Etcd Operator 创建 RBAC 规则。这是因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象。 第三步，创建Etcd Operator而 Etcd Operator 本身，其实就是一个 Deployment，它的 YAML 文件如下所示： 123456789101112131415161718192021222324252627282930apiVersion: extensions/v1beta1 高版本改成 apps/v1kind: Deploymentmetadata: name: etcd-operatorspec: selector: matchLabels: name: etcd-operator replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.2 command: - etcd-operator env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name... 12$ kubectl create -f example/deployment.yaml 12345678$ kubectl get podsNAME READY STATUS RESTARTS AGEetcd-operator-649dbdb5cb-bzfzp 1/1 Running 0 20s$ kubectl get crdNAME CREATED ATetcdclusters.etcd.database.coreos.com 2018-09-18T11:42:55Z 这个 CRD 名叫etcdclusters.etcd.database.coreos.com 。你可以通过 kubectl describe 命令看到它的细节，如下所示： 123456789101112131415$ kubectl describe crd etcdclusters.etcd.database.coreos.com...Group: etcd.database.coreos.com Names: Kind: EtcdCluster List Kind: EtcdClusterList Plural: etcdclusters Short Names: etcd Singular: etcdcluster Scope: Namespaced Version: v1beta2 ... 所以说，通过上述两步操作，你实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型。而 Etcd Operator 本身，就是这个自定义资源类型对应的自定义控制器。 第四步，创建Etcd 集群不难想到，这个文件里定义的，正是 EtcdCluster 这个 CRD 的一个具体实例，也就是一个 Custom Resource（CR）。 12345678apiVersion: &quot;etcd.database.coreos.com/v1beta2&quot;kind: &quot;EtcdCluster&quot;metadata: name: &quot;example-etcd-cluster&quot;spec: size: 3 version: &quot;3.2.13&quot; 1$ kubectl apply -f example/example-etcd-cluster.yaml 123456$ kubectl get podsNAME READY STATUS RESTARTS AGEexample-etcd-cluster-dp8nqtjznc 1/1 Running 0 1mexample-etcd-cluster-mbzlg6sd56 1/1 Running 0 2mexample-etcd-cluster-v6v6s6stxd 1/1 Running 0 2m 而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作了。看到这里，相信你应该已经对 Operator 有了一个初步的认知： Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。 Etcd Operator原理分析Etcd静态集群构建123456789101112131415161718192021$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \ --listen-peer-urls http://10.0.1.10:2380 \... --initial-cluster-token etcd-cluster-1 \ --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \ --initial-cluster-state new $ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \ --listen-peer-urls http://10.0.1.11:2380 \... --initial-cluster-token etcd-cluster-1 \ --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \ --initial-cluster-state new $ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \ --listen-peer-urls http://10.0.1.12:2380 \... --initial-cluster-token etcd-cluster-1 \ --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \ --initial-cluster-state new 其中，这些节点启动参数里的–initial-cluster 参数，非常值得你关注。它的含义，正是当前节点启动时集群的拓扑结构。说得更详细一点，就是当前这个节点启动时，需要跟哪些节点通信来组成集群。 此外，一个 Etcd 集群还需要用–initial-cluster-token 字段，来声明一个该集群独一无二的 Token 名字。 像上述这样为每一个 Ectd 节点配置好它对应的启动参数之后把它们启动起来，一个 Etcd 集群就可以自动组建起来了。 EtcdCluster CRDtypes.go的定义 12345678910111213141516171819// +genclient// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Objecttype EtcdCluster struct &#123; metav1.TypeMeta `json:&quot;,inline&quot;` metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;` Spec ClusterSpec `json:&quot;spec&quot;` Status ClusterStatus `json:&quot;status&quot;`&#125;type ClusterSpec struct &#123; // Size is the expected size of the etcd cluster. // The etcd-operator will eventually make the size of the running // cluster equal to the expected size. // The vaild range of the size is from 1 to 7. Size int `json:&quot;size&quot;` ... &#125; 可以看到，EtcdCluster 是一个有 Status 字段的 CRD。在这里，我们可以不必关心 ClusterSpec 里的其他字段，只关注 Size（即：Etcd 集群的大小）字段即可。Size 字段的存在，就意味着将来如果我们想要调整集群大小的话，应该直接修改 YAML 文件里 size 的值，并执行 kubectl apply -f。 这样，Operator 就会帮我们完成 Etcd 节点的增删操作。这种“scale”能力，也是 Etcd Operator 自动化运维 Etcd 集群需要实现的主要功能。 而为了能够支持这个功能，我们就不再像前面那样在–initial-cluster 参数里把拓扑结构固定死 Etcd Operator scale原理首先，Etcd Operator 会创建一个“种子节点”； 然后，Etcd Operator 会不断创建新的 Etcd 节点，然后将它们逐一加入到这个集群当中，直到集群的节点数等于 size。 而这两种节点的不同之处，就在于一个名叫–initial-cluster-state 的启动参数： 当这个参数值设为 new 时，就代表了该节点是种子节点。而我们前面提到过，种子节点还必须通过–initial-cluster-token 声明一个独一无二的 Token。 12345678910$ etcd --data-dir=/var/etcd/data --name=infra0 --initial-advertise-peer-urls=http://10.0.1.10:2380 --listen-peer-urls=http://0.0.0.0:2380 --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://10.0.1.10:2379 --initial-cluster=infra0=http://10.0.1.10:2380 --initial-cluster-state=new --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8 而如果这个参数值设为 existing，那就是说明这个节点是一个普通节点，Etcd Operator 需要把它加入到已有集群里。 123456789101112# 第一步：通过 Etcd 命令行添加一个新成员：$ etcdctl member add infra1 http://10.0.1.11:2380# 第二步：为这个成员节点生成对应的启动参数，并启动它：$ etcd --data-dir=/var/etcd/data --name=infra1 --initial-advertise-peer-urls=http://10.0.1.11:2380 --listen-peer-urls=http://0.0.0.0:2380 --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://10.0.1.11:2379 --initial-cluster=infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380 --initial-cluster-state=existing Etcd Operator Informer跟所有的自定义控制器一样，Etcd Operator 的启动流程也是围绕着 Informer 展开的，如下所示： 12345678910111213141516171819202122func (c *Controller) Start() error &#123; for &#123; err := c.initResource() ... time.Sleep(initRetryWaitTime) &#125; c.run()&#125;func (c *Controller) run() &#123; ... _, informer := cache.NewIndexerInformer(source, &amp;api.EtcdCluster&#123;&#125;, 0, cache.ResourceEventHandlerFuncs&#123; AddFunc: c.onAddEtcdClus, UpdateFunc: c.onUpdateEtcdClus, DeleteFunc: c.onDeleteEtcdClus, &#125;, cache.Indexers&#123;&#125;) ctx := context.TODO() // TODO: use workqueue to avoid blocking informer.Run(ctx.Done())&#125; 可以看到，Etcd Operator 启动要做的第一件事（ c.initResource），是创建 EtcdCluster 对象所需要的 CRD，即：前面提到的etcdclusters.etcd.database.coreos.com。这样 Kubernetes 就能够“认识”EtcdCluster 这个自定义 API 资源了。 而接下来，Etcd Operator 会定义一个 EtcdCluster 对象的 Informer。不过，需要注意的是，由于 Etcd Operator 的完成时间相对较早，所以它里面有些代码的编写方式会跟我们之前讲解的最新的编写方式不太一样。在具体实践的时候，你还是应该以我讲解的模板为主。 比如，在上面的代码最后，你会看到有这样一句注释： 12// TODO: use workqueue to avoid blocking... 具体来讲，我们在控制循环里执行的业务逻辑，往往是比较耗时间的。比如，创建一个真实的 Etcd 集群。而 Informer 的 WATCH 机制对 API 对象变化的响应，则非常迅速。所以，控制器里的业务逻辑就很可能会拖慢 Informer 的执行周期，甚至可能 Block 它。而要协调这样两个快、慢任务的一个典型解决方法，就是引入一个工作队列。 由于 Etcd Operator 里没有工作队列，那么在它的 EventHandler 部分，就不会有什么入队操作，而直接就是每种事件对应的具体的业务逻辑了。 可以看到，Etcd Operator 的特殊之处在于，它为每一个 EtcdCluster 对象，都启动了一个控制循环，“并发”地响应这些对象的变化。显然，这种做法不仅可以简化 Etcd Operator 的代码实现，还有助于提高它的响应速度。 启动Bootstrap其中，第一个工作只在该 Cluster 对象第一次被创建的时候才会执行。这个工作，就是我们前面提到过的 Bootstrap，即：创建一个单节点的种子集群。 由于种子集群只有一个节点，所以这一步直接就会生成一个 Etcd 的 Pod 对象。这个 Pod 里有一个 InitContainer，负责检查 Pod 的 DNS 记录是否正常。如果检查通过，用户容器也就是 Etcd 容器就会启动起来。 启动命令 1234567891011/usr/local/bin/etcd --data-dir=/var/etcd/data --name=example-etcd-cluster-mbzlg6sd56 --initial-advertise-peer-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380 --listen-peer-urls=http://0.0.0.0:2380 --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2379 --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380 --initial-cluster-state=new --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8 可以看到，在这些启动参数（比如：initial-cluster）里，Etcd Operator 只会使用 Pod 的 DNS 记录，而不是它的 IP 地址。 这当然是因为，在 Operator 生成上述启动命令的时候，Etcd 的 Pod 还没有被创建出来，它的 IP 地址自然也无从谈起。 这也就意味着，每个 Cluster 对象，都会事先创建一个与该 EtcdCluster 同名的 Headless Service。这样，Etcd Operator 在接下来的所有创建 Pod 的步骤里，就都可以使用 Pod 的 DNS 记录来代替它的 IP 地址了。 启动该集群所对应的控制循环Cluster 对象的第二个工作，则是启动该集群所对应的控制循环。 这个控制循环每隔一定时间，就会执行一次下面的 Diff 流程。 首先，控制循环要获取到所有正在运行的、属于这个 Cluster 的 Pod 数量，也就是该 Etcd 集群的“实际状态”。 而这个 Etcd 集群的“期望状态”，正是用户在 EtcdCluster 对象里定义的 size。 如果实际的 Pod 数量不够，那么控制循环就会执行一个添加成员节点的操作（即：上述流程图中的 addOneMember 方法）；反之，就执行删除成员节点的操作（即：上述流程图中的 removeOneMember 方法） 以 addOneMember 方法为例，它执行的流程如下所示： 生成一个新节点的 Pod 的名字，比如：example-etcd-cluster-v6v6s6stxd； 调用 Etcd Client，执行前面提到过的 etcdctl member add example-etcd-cluster-v6v6s6stxd 命令； 使用这个 Pod 名字，和已经存在的所有节点列表，组合成一个新的 initial-cluster 字段的值； 使用这个 initial-cluster 的值，生成这个 Pod 里 Etcd 容器的启动命令。如下所示： 12345678910/usr/local/bin/etcd --data-dir=/var/etcd/data --name=example-etcd-cluster-v6v6s6stxd --initial-advertise-peer-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380 --listen-peer-urls=http://0.0.0.0:2380 --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2379 --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380,example-etcd-cluster-v6v6s6stxd=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380 --initial-cluster-state=existing 对比StatefulSet 可能有的两个疑问问题1在 StatefulSet 里，它为 Pod 创建的名字是带编号的，这样就把整个集群的拓扑状态固定了下来（比如：一个三节点的集群一定是由名叫 web-0、web-1 和 web-2 的三个 Pod 组成）。可是，在 Etcd Operator 里，为什么我们使用随机名字就可以了呢？ 答：这是因为，Etcd Operator 在每次添加 Etcd 节点的时候，都会先执行 etcdctl member add ；每次删除节点的时候，则会执行 etcdctl member remove 。这些操作，其实就会更新 Etcd 内部维护的拓扑信息，所以 Etcd Operator 无需在集群外部通过编号来固定这个拓扑关系。 问题2为什么我没有在 EtcdCluster 对象里声明 Persistent Volume？难道，我们不担心节点宕机之后 Etcd 的数据会丢失吗？ 答：在有了 Operator 机制之后，上述 Etcd 的备份操作，是由一个单独的 Etcd Backup Operator 负责完成的。 创建和使用这个 Operator 的流程，如下所示： 123456789101112131415161718192021222324252627282930313233# 首先，创建etcd-backup-operator$ kubectl create -f example/etcd-backup-operator/deployment.yaml# 确认etcd-backup-operator已经在正常运行$ kubectl get podNAME READY STATUS RESTARTS AGEetcd-backup-operator-1102130733-hhgt7 1/1 Running 0 3s# 可以看到，Backup Operator会创建一个叫etcdbackups的CRD$ kubectl get crdNAME KINDetcdbackups.etcd.database.coreos.com CustomResourceDefinition.v1beta1.apiextensions.k8s.io# 我们这里要使用AWS S3来存储备份，需要将S3的授权信息配置在文件里$ cat $AWS_DIR/credentials[default]aws_access_key_id = XXXaws_secret_access_key = XXX$ cat $AWS_DIR/config[default]region = &lt;region&gt;# 然后，将上述授权信息制作成一个Secret$ kubectl create secret generic aws --from-file=$AWS_DIR/credentials --from-file=$AWS_DIR/config# 使用上述S3的访问信息，创建一个EtcdBackup对象$ sed -e 's|&lt;full-s3-path&gt;|mybucket/etcd.backup|g' \ -e 's|&lt;aws-secret&gt;|aws|g' \ -e 's|&lt;etcd-cluster-endpoints&gt;|"http://example-etcd-cluster-client:2379"|g' \ example/etcd-backup-operator/backup_cr.yaml \ | kubectl create -f - 需要注意的是，每当你创建一个 EtcdBackup 对象（backup_cr.yaml），就相当于为它所指定的 Etcd 集群做了一次备份。EtcdBackup 对象的 etcdEndpoints 字段，会指定它要备份的 Etcd 集群的访问地址。所以，在实际的环境里，我建议你把最后这个备份操作，编写成一个 Kubernetes 的 CronJob 以便定时运行。 而当 Etcd 集群发生了故障之后，你就可以通过创建一个 EtcdRestore 对象来完成恢复操作。当然，这就意味着你也需要事先启动 Etcd Restore Operator。 1234567891011121314# 创建etcd-restore-operator$ kubectl create -f example/etcd-restore-operator/deployment.yaml# 确认它已经正常运行$ kubectl get podsNAME READY STATUS RESTARTS AGEetcd-restore-operator-4203122180-npn3g 1/1 Running 0 7s# 创建一个EtcdRestore对象，来帮助Etcd Operator恢复数据，记得替换模板里的S3的访问信息$ sed -e 's|&lt;full-s3-path&gt;|mybucket/etcd.backup|g' \ -e 's|&lt;aws-secret&gt;|aws|g' \ example/etcd-restore-operator/restore_cr.yaml \ | kubectl create -f - 上面例子里的 EtcdRestore 对象（restore_cr.yaml），会指定它要恢复的 Etcd 集群的名字和备份数据所在的 S3 存储的访问信息。而当一个 EtcdRestore 对象成功创建后，Etcd Restore Operator 就会通过上述信息，恢复出一个全新的 Etcd 集群。然后，Etcd Operator 会把这个新集群直接接管过来，从而重新进入可用的状态。EtcdBackup 和 EtcdRestore 这两个 Operator 的工作原理，与 Etcd Operator 的实现方式非常类似。 再谈PVC、PV、StorageClassPV12345678910111213apiVersion: v1kind: PersistentVolumemetadata: name: nfsspec: storageClassName: manual capacity: storage: 1Gi accessModes: - ReadWriteMany nfs: server: 10.244.1.4 path: &quot;/&quot; PVC1234567891011apiVersion: v1kind: PersistentVolumeClaimmetadata: name: nfsspec: accessModes: - ReadWriteMany storageClassName: manual resources: requests: storage: 1Gi 在Pod中使用PVC12345678910111213141516171819apiVersion: v1kind: Podmetadata: labels: role: web-frontendspec: containers: - name: web image: nginx ports: - name: web containerPort: 80 volumeMounts: - name: nfs mountPath: &quot;/usr/share/nginx/html&quot; volumes: - name: nfs persistentVolumeClaim: claimName: nfs 可以看到，Pod 需要做的，就是在 volumes 字段里声明自己要使用的 PVC 名字。接下来，等这个 Pod 创建之后，kubelet 就会把这个 PVC 所对应的 PV，也就是一个 NFS 类型的 Volume，挂载在这个 Pod 容器内的目录上。 Volume Controller在 Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器,这个 Volume Controller 维护着多个控制循环。 PersistentVolumeController其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。它的名字叫作 PersistentVolumeController。 PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态，从而结束“单身”之旅。 而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。 AttachDetachController其中，“第一阶段”的 Attach（以及 Dettach）操作，是由 Volume Controller 负责维护的，这个控制循环的名字叫作：AttachDetachController。而它的作用，就是不断地检查每一个 Pod 对应的 PV，和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作。 需要注意，作为一个 Kubernetes 内置的控制器，Volume Controller 自然是 kube-controller-manager 的一部分。所以，AttachDetachController 也一定是运行在 Master 节点上的。当然，Attach 操作只需要调用公有云或者具体存储项目的 API，并不需要在具体的宿主机上执行操作，所以这个设计没有任何问题。 VolumeManagerReconciler而“第二阶段”的 Mount（以及 Unmount）操作，必须发生在 Pod 对应的宿主机上，所以它必须是 kubelet 组件的一部分。这个控制循环的名字，叫作：VolumeManagerReconciler，它运行起来之后，是一个独立于 kubelet 主循环的 Goroutine。通过这样将 Volume 的处理同 kubelet 的主循环解耦，Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环，进而导致 Pod 的创建效率大幅下降的问题。实际上，kubelet 的一个主要设计原则，就是它的主控制循环绝对不可以被 block。这个思想，我在后续的讲述容器运行时的时候还会提到。 PV对象是如何变成容器里的一个持久化存储的呢？我在前面讲解容器基础的时候，已经为你详细剖析了容器 Volume 的挂载机制。 用一句话总结，所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。 而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”。即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。 显然，我们前面使用的 hostPath 和 emptyDir 类型的 Volume 并不具备这个特征：它们既有可能被 kubelet 清理掉，也不能被“迁移”到其他节点上。 所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。 而 Kubernetes 需要做的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载时使用。而所谓“持久化”，指的是容器在这个目录里写入的文件，都会保存在远程存储中，从而使得这个目录具备了“持久性”。 当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径： 1/var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt; 这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。 “第一阶段”（Attach）接下来，kubelet 要做的操作就取决于你的 Volume 类型了。 如果你的 Volume 类型是远程块存储，即提供远程磁盘服务。 比如 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么 kubelet 就需要先调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。这相当于执行： 1$ gcloud compute instances attach-disk &lt;虚拟机名字&gt; --disk &lt;远程磁盘名字&gt; 这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach。 而如果你的 Volume 类型是远程文件存储（比如 NFS）的话，kubelet 的处理过程就会更简单一些。因为在这种情况下，kubelet 可以跳过“第一阶段”（Attach）的操作，这是因为一般来说，远程文件存储并没有一个“存储设备”需要挂载在宿主机上。 “第二阶段”（Mount） 如果你的 Volume 类型是远程块存储，即提供远程磁盘服务。 Attach 阶段完成后，为了能够使用这个远程磁盘，kubelet 还要进行第二个操作，即：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上。不难理解，这个挂载点，正是我在前面反复提到的 Volume 的宿主机目录。所以，这一步相当于执行： 123456# 通过lsblk命令获取磁盘设备ID$ sudo lsblk# 格式化成ext4格式$ sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/&lt;磁盘设备ID&gt;# 挂载到挂载点$ sudo mkdir -p /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt; 这个将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。 Mount 阶段完成后，这个 Volume 的宿主机目录就是一个“持久化”的目录了，容器在它里面写入的内容，会保存在 Google Cloud 的远程磁盘中。 而如果你的 Volume 类型是远程文件存储（比如 NFS）的话。 kubelet 会直接从“第二阶段”（Mount）开始准备宿主机上的 Volume 目录。 kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令： 1$ mount -t nfs &lt;NFS服务器地址&gt;:/ /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt; 而经过了“两阶段处理”，我们就得到了一个“持久化”的 Volume 宿主机目录。所以，接下来，kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令： 1$ docker run -v /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;:/&lt;容器内的目标目录&gt; 我的镜像 ... Dynamic Provisioning前面人工管理 PV 的方式就叫作 Static Provisioning。 Kubernetes 为我们提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。 StorageClass具体地说，StorageClass 对象会定义如下两个部分内容： 第一，PV 的属性。比如，存储类型、Volume 的大小等等。 第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。 有了这样两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 StorageClass 声明的存储插件，创建出需要的 PV。 举个例子,假如我们的 Volume 的类型是 GCE 的 Persistent Disk 的话 12345678apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: block-serviceprovisioner: kubernetes.io/gce-pdparameters: type: pd-ssd 这个 StorageClass 的 provisioner 字段的值是：kubernetes.io/gce-pd，这正是 Kubernetes 内置的 GCE PD 存储插件的名字。 在举个例子，使用Rook存储服务的话 12345678910111213141516171819apiVersion: ceph.rook.io/v1beta1kind: Poolmetadata: name: replicapool namespace: rook-cephspec: replicated: size: 3---apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: block-serviceprovisioner: ceph.rook.io/blockparameters: pool: replicapool #The value of &quot;clusterNamespace&quot; MUST be the same as the one in which your rook cluster exist clusterNamespace: rook-ceph 有了 StorageClass 的 YAML 文件之后，运维人员就可以在 Kubernetes 里创建这个 StorageClass 了： 12$ kubectl create -f sc.yaml StorageClass 并不是专门为了 Dynamic Provisioning 而设计的。 比如，在本篇一开始的例子里，我在 PV 和 PVC 里都声明了 storageClassName=manual。而我的集群里，实际上并没有一个名叫 manual 的 StorageClass 对象。这完全没有问题，这个时候 Kubernetes 进行的是 Static Provisioning，但在做绑定决策的时候，它依然会考虑 PV 和 PVC 的 StorageClass 定义。 在Pvc中使用这时候，作为应用开发者，我们只需要在 PVC 里指定要使用的 StorageClass 名字即可，如下所示： 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: claim1spec: accessModes: - ReadWriteOnce storageClassName: block-service resources: requests: storage: 30Gi 这里，你可能会有疑问，我在之前讲解 StatefulSet 存储状态的例子时，好像并没有声明 StorageClass 啊？实际上，如果你的集群已经开启了名叫 DefaultStorageClass 的 Admission Plugin，它就会为 PVC 和 PV 自动添加一个默认的 StorageClass；否则，PVC 的 storageClassName 的值就是“”，这也意味着它只能够跟 storageClassName 也是“”的 PV 进行绑定。 原理 从图中我们可以看到，在这个体系中： PVC 描述的，是 Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。 PV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。 而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。 当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。 总结容器持久化存储涉及的概念比较多，试着总结一下整体流程。 当用户创建了一个 PVC 之后，为你创建出对应的 PV。 PersistentVolumeController帮PVC和PV配对。 用户提交请求创建pod，Kubernetes发现这个pod声明使用了PVC，找到对应的PV，新创建的PV，还只是一个API 对象，需要经过“两阶段处理”变成宿主机上的“持久化 Volume”才真正有用：第一阶段由运行在master上的AttachDetachController负责，为这个PV完成 Attach 操作，为宿主机挂载远程磁盘；第二阶段是运行在每个节点上kubelet组件的内部，把第一步attach的远程磁盘 mount 到宿主机目录。这个控制循环叫VolumeManagerReconciler，运行在独立的Goroutine，不会阻塞kubelet主循环。 完成这两步，PV对应的“持久化 Volume”就准备好了，POD可以正常启动，将“持久化 Volume”挂载在容器内指定的路径。 Local Persistent Volume出现的意义在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。也就是说，用户希望 Kubernetes 能够直接使用宿主机上的本地磁盘目录，而不依赖于远程存储服务，来提供“持久化”的容器 Volume。这样做的好处很明显，由于这个 Volume 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多。这个需求对本地物理服务器部署的私有 Kubernetes 集群来说，非常常见。所以，Kubernetes 在 v1.10 之后，就逐渐依靠 PV、PVC 体系实现了这个特性。这个特性的名字叫作：Local Persistent Volume。 LPV，不就等同于 hostPath 加 NodeAffinity 吗？比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了。那么，我只需要再给这个 Pod 加上一个 nodeAffinity=nodeA，不就可以使用这个 Volume 了吗？ 事实上，你绝不应该把一个宿主机上的目录当作 PV 使用。这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。 所以，一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。这个原则，我们可以称为“一个 PV 一块盘”。 适用哪些应用它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。 实现难点一如何把本地磁盘抽象成 PV 在公有云上，这个操作等同于给虚拟机额外挂载一个磁盘，比如 GCE 的 Local SSD 类型的磁盘就是一个典型例子。 而在我们部署的私有环境中，你有两种办法来完成这个步骤。 第一种，当然就是给你的宿主机挂载并格式化一个可用的本地磁盘，这也是最常规的操作； 第二种，对于实验环境，你其实可以在宿主机上挂载几个 RAM Disk（内存盘）来模拟本地磁盘。 我们使用第二种模拟 1234567# 在node-1上执行$ mkdir /mnt/disks$ for vol in vol1 vol2 vol3; do mkdir /mnt/disks/$vol mount -t tmpfs $vol /mnt/disks/$voldone 需要注意的是，如果你希望其他节点也能支持 Local Persistent Volume 的话，那就需要为它们也执行上述操作，并且确保这些磁盘的名字（vol1、vol2 等）都不重复。 实现难点二调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？ 造成这个问题的原因在于，对于常规的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后，再通过“两阶段处理”来“持久化”这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。 可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。 这个原则，我们可以称为“在调度的时候考虑 Volume 分布”。在 Kubernetes 的调度器里，有一个叫作 VolumeBindingChecker 的过滤条件专门负责这个事情。在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。 模拟Local PVPV12345678910111213141516171819202122apiVersion: v1kind: PersistentVolumemetadata: name: example-pvspec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/vol1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node-1 可以看到，这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。 当然了，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 node-1 上。 所以，在这个 PV 的定义里，需要有一个 nodeAffinity 字段指定 node-1 这个节点的名字。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。这正是 Kubernetes 实现“在调度的时候就考虑 Volume 分布”的主要方法。 接下来，我们就可以使用 kubect create 来创建这个 PV，如下所示： 123456$ kubectl create -f local-pv.yaml persistentvolume/example-pv created$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEexample-pv 5Gi RWO Delete Available local-storage 16s 而正如我在上一篇文章里所建议的那样，使用 PV 和 PVC 的最佳实践，是你要创建一个 StorageClass 来描述这个 PV，如下所示： StorageClass123456kind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: local-storageprovisioner: kubernetes.io/no-provisionervolumeBindingMode: WaitForFirstConsumer 123$ kubectl create -f local-sc.yaml storageclass.storage.k8s.io/local-storage created 这个 StorageClass 的名字，叫作 local-storage。需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。也就是说，我们前面创建 PV 的操作，是不可以省略的。 与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：延迟绑定[^1]。 PVC123456789101112kind: PersistentVolumeClaimapiVersion: v1metadata: name: example-local-claimspec: accessModes: - ReadWriteOnce resources: requests: storage: 5Gi storageClassName: local-storage 1234567$ kubectl create -f local-pvc.yaml persistentvolumeclaim/example-local-claim created$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEexample-local-claim Pending local-storage 7s 可以看到，尽管这个时候，Kubernetes 里已经存在了一个可以与 PVC 匹配的 PV，但这个 PVC 依然处于 Pending 状态，也就是等待绑定的状态。 Pod123456789101112131415161718kind: PodapiVersion: v1metadata: name: example-pv-podspec: volumes: - name: example-pv-storage persistentVolumeClaim: claimName: example-local-claim containers: - name: example-pv-container image: nginx ports: - containerPort: 80 name: &quot;http-server&quot; volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: example-pv-storage 而我们一旦使用 kubectl create 创建这个 Pod，就会发现，我们前面定义的 PVC，会立刻变成 Bound 状态，与前面定义的 PV 绑定在了一起，如下所示： 1234567$ kubectl create -f local-pod.yaml pod/example-pv-pod created$ kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEexample-local-claim Bound example-pv 5Gi RWO local-storage 6h 也就是说，在我们创建的 Pod 进入调度器之后，“绑定”操作才开始进行。这时候，我们可以尝试在这个 Pod 的 Volume 目录里，创建一个测试文件，比如： 1234$ kubectl exec -it example-pv-pod -- /bin/sh# cd /usr/share/nginx/html# touch test.txt 然后，登录到 node-1 这台机器上，查看一下它的 /mnt/disks/vol1 目录下的内容，你就可以看到刚刚创建的这个文件： 1234# 在node-1上$ ls /mnt/disks/vol1test.txt 而如果你重新创建这个 Pod 的话，就会发现，我们之前创建的测试文件，依然被保存在这个持久化 Volume 当中： 1234567$ kubectl delete -f local-pod.yaml $ kubectl create -f local-pod.yaml $ kubectl exec -it example-pv-pod -- /bin/sh# ls /usr/share/nginx/html# touch test.txt 为什么需要延迟绑定我们知道，当你提交了 PV 和 PVC 的 YAML 文件之后，Kubernetes 就会根据它们俩的属性，以及它们指定的 StorageClass 来进行绑定。只有绑定成功后，Pod 才能通过声明这个 PVC 来使用对应的 PV。可是，如果你使用的是 Local Persistent Volume 的话，就会发现，这个流程根本行不通。比如，现在你有一个 Pod，它声明使用的 PVC 叫作 pvc-1。并且，我们规定，这个 Pod 只能运行在 node-2 上。而在 Kubernetes 集群中，有两个属性（比如：大小、读写权限）相同的 Local 类型的 PV。其中，第一个 PV 的名字叫作 pv-1，它对应的磁盘所在的节点是 node-1。而第二个 PV 的名字叫作 pv-2，它对应的磁盘所在的节点是 node-2。假设现在，Kubernetes 的 Volume 控制循环里，首先检查到了 pvc-1 和 pv-1 的属性是匹配的，于是就将它们俩绑定在一起。然后，你用 kubectl create 创建了这个 Pod。这时候，问题就出现了。调度器看到，这个 Pod 所声明的 pvc-1 已经绑定了 pv-1，而 pv-1 所在的节点是 node-1，根据“调度器必须在调度的时候考虑 Volume 分布”的原则，这个 Pod 自然会被调度到 node-1 上。可是，我们前面已经规定过，这个 Pod 根本不允许运行在 node-1 上。所以。最后的结果就是，这个 Pod 的调度必然会失败。这就是为什么，在使用 Local Persistent Volume 的时候，我们必须想办法推迟这个“绑定”操作。 延迟到什么时候答案是：推迟到调度的时候。 所以说，StorageClass 里的 volumeBindingMode=WaitForFirstConsumer 的含义，就是告诉 Kubernetes 里的 Volume 控制循环（“红娘”）：虽然你已经发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但请不要现在就执行绑定操作（即：设置 PVC 的 VolumeName 字段）。 而要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV 所在的节点位置，来统一决定，这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。 这样，在上面的例子里，由于这个 Pod 不允许运行在 pv-1 所在的节点 node-1，所以它的 PVC 最后会跟 pv-2 绑定，并且 Pod 也会被调度到 node-2 上。所以，通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。 编写自己的存储插件FlexVolumeflexVolume插件只负责attach和mount，使用简单 所以，如果场景简单，不需要Dynamic Provisioning，则可以使用flexVolume； 可以看到，在上述体系下，无论是 FlexVolume，还是 Kubernetes 内置的其他存储插件，它们实际上担任的角色，仅仅是 Volume 管理中的“Attach 阶段”和“Mount 阶段”的具体执行者。而像 Dynamic Provisioning 这样的功能，就不是存储插件的责任，而是 Kubernetes 本身存储管理功能的一部分。 CSI而CSI插件包括了一部分原来kubernetes中存储管理的功能，实现、部署起来比较复杂。 如果场景复杂，需要支持Dynamic Provisioning，则用CSI插件。]]></content>
  </entry>
  <entry>
    <title><![CDATA[note]]></title>
    <url>%2F2020%2F02%2F08%2Flinux_note%2F</url>
    <content type="text"><![CDATA[获取linux是ubuntu or centos1awk -F= &apos;/^NAME/&#123;print $2&#125;&apos; /etc/os-release 获取ubuntu version-a```12345678```outputOUTPUTNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 18.04 LTSRelease: 18.04Codename: bionic 获取docker容器的mount id和镜像层/var/lib/docker/image/aufs/layerdb/mounts/$(docker inspect --format &#123;&#123;.Id&#125;&#125; )/mount-id)```123456789101112131415161718192021222324![image-20200210125625341](/images/image-20200210125625341.png)有两种方式1. cat /var/lib/docker/aufs/layers/&lt;mount-id&gt; &lt;img src=&quot;/images/image-20200210125530118.png&quot; alt=&quot;image-20200210125530118&quot; style=&quot;zoom:150%;&quot; /&gt;2. Cat /proc/mounts |grep &lt;mount-id&gt; 获取si=xxxxxxxxxxx cat /sys/fs/aufs/si_xxxxxxxxxxxx/br[0-9]* ![image-20200210125431975](/images/image-20200210125431975.png) kubeadm join 202.182.103.180:6443 --token wqk2zv.v873dv5se20c0z0o \ --discovery-token-ca-cert-hash sha256:39c09a5a814b67e0a02d7c1656e714487c8a780e046feef300b61c66789b4f4f### ssh如果遇到 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!Someone could be eavesdropping on you right now (man-in-the-middle attack)!It is also possible that a host key has just been changed.The fingerprint for the ECDSA key sent by the remote host isSHA256:oE6RlmdY9tbK5nwlSoHSb3MsmezZNxC7FatOOoiuXow.Please contact your system administrator.Add correct host key in /Users/mobike/.ssh/known_hosts to get rid of this message.Offending ECDSA key in /Users/mobike/.ssh/known_hosts:18ECDSA host key for 202.182.103.180 has changed and you have requested strict checking.Host key verification failed.12解决方法 ssh-keygen -R 202.182.103.180`]]></content>
  </entry>
  <entry>
    <title><![CDATA[2019年度总结·调整心态]]></title>
    <url>%2F2020%2F01%2F10%2F2019%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%C2%B7%E8%B0%83%E6%95%B4%E5%BF%83%E6%80%81%2F</url>
    <content type="text"><![CDATA[学习不是努力读更多的书，盲目追求阅读的速度和数量，这会让人产生低层次的勤奋和成长的感觉，这只是在使蛮力。要思辨，要践行，要总结和归纳，否则，你只是在机械地重复某件事，而不会有质的成长的。 从今年开始，我会写年度总结，对比去年的变化，本年的收获和不足，并制定下一年的的计划。 2019年感觉对我是比较关键的一年，工作方面晋升到下一等级，个人成长方面一直在不断努力，虽然收效甚微，但是万事开头难，做总是比不做强，我相信坚持下去总会有收获。 2019年总结主要是心态的转变，下面是心态转变的几个阶段。 盲目追新名词2017-2019，好多新名词层出不穷，人工智能、区块链、k8s容器化，然后自己的内心也跟着着急起来。想着与时俱进，自己想着赶紧学点啥，然后购买了极客时间的《区块链课程》，看的时候感觉太难了，各种没听过的算法，中途放弃，进入自我怀疑的心态。然后受某个大佬的博客启发，不要盲目追新技术，新技术万变不离其宗，都是基础知识加上新的理念构成。文中有这么一句话，基础扎实的人可能很快掌握这种新技术，但是为了学新名词的菜鸟可能得花上几个月的时间，所以菜鸟要追新技术太难了，成本太高了。不如回归基础。 莫以浮沙筑高楼受到上述启发后，制定了一些针对基础知识的专项训练。包括计算机网络、算法、linux操作系统、网络编程等方面的计划。2019年基本完成了计算机网络的学习，对于TCP协议有了清晰的认识，由当时只了解为了应付面试的三次握手四次挥手，到了解了重传策略、流控制、拥塞控制、keepalive，并用wireshark分析报文。 这个过程的不足 贪多这个阶段效率确实比较低，因为我学新知识点有个毛病，总是想把关于这个知识点的所有书籍和专栏都看一遍。所以关于计算机网络，我看了极客时间的专栏《趣谈网络协议》和正在看的专栏《Web协议详解与抓包实战》，看了书籍《网络是怎样连接的》、《图解TCP/IP》。所以这个基础知识大概进行了5个月，目前还未完结。 缺少好的学习习惯学的过程不爱记笔记总结，所以基本看完一个专栏或书籍，就忘了，然后就有个重新在读一遍的念头闪现出来，然后整个人就开始浮躁，感觉进入了一怪圈里边，心里想着放弃吧，又不甘心。所以学习过程比较痛苦，这也是效率不高的一个原因。 浮躁这一年自己一直以一种特别浮躁的心态来学东西，总是感觉要学的东西太多了，所以比较着急。举个例子，因为技术好多层次都是相关的，比如今天看到一个知识点，里边有另一个领域的一个知识点，我就会顺藤摸瓜的去学另外一个知识点，然后又发现另外一个知识点…，这样层层追溯下去，自己把自己逼得放弃了，会很苦恼。时间花了不少，因为学习过程是一个不断自我否定的过程，最后自己放弃了。 以上不足归结到一点还是浮躁。虽然每次都想到放弃，但是值得欣慰的一点是自己2019年没有放弃。因为听到过一句话，大概是这样「平庸的人和优秀的人都会遇到类似问题，但是优秀的人会去总结，然后调整策略继续投入进去，平庸的人到这里就结束了。」 摆正心态根据上述不足，自己复盘反思了下，觉得自己并不是学习意念不够强烈，而是心态不对。 不浮躁：学习是一个循序渐进的过程，有一个好心态，能事半功倍 学习方法论：养成好的学习习惯，学完一个知识点，要整理学习笔记，不要怕花时间，经常反思，要借鉴别人的学习方法。 不急功近利：从2017年到2019年经历了立志-放弃-立志-放弃的n次循环后，往往都是不断的自我怀疑和自我否定占据上风，这种想法一直在提醒我，你差的太多了，补起来无从下手，在这种思想压力下自己索性就放弃了。2019年我坚持下来了，因为当你前面敌人太多的时候，你总要挑一个下手，收拾完一个，就会有信心收拾另外一个，而不是因为敌人多而停止脚步。 2020年展望作为一个码农，技术是安身立命之根本，所以优先从技术层面开始。因为自己达不到程序员的标准，所以姑且认为自己就是个码农吧。 技术编程能力由于上级人事变动，新来的leader更关注写代码的能力，而自己之前的观点是「能完成功能是主要的，代码可以适当抽象，不必过分较真」。但是这个观点是错误的，质量好的代码，能避免好多问题。 编程能力是软件工程师最重要的能力，很多工程师在讨论分布式、高可用系统时都能把别人说的一愣一愣的，张嘴闭嘴就是五个九的可用性，异地多活，但是一旦落到编程能力上，实现一个小功能都能漏洞百出，更不用说写出真正高可用的系统了。 今年要提升代码抽象能力，计划阅读相关书籍《Clean Code》和《Head First 设计模式》，多看成熟框架的源码。工作中的编码任务，要仔细推敲，好好抽象，不能为了完成任务而糊弄。 架构能力2019年阅读了极客时间的专栏《从0开始学架构》、《从0开始学微服务》、《微服务架构核心20讲》、《微服务架构实战160讲》和书籍《大型网站技术架构》，因为没有实际参与过架构设计，但是看了这些专栏和书籍，对架构和微服务有了一些浅显的认知。2020年希望能结合公司目前的架构设计，加深对架构的理解。 技术栈2019年基本没有接触其他技术栈。2020年还是以巩固基础能力为主，其他技术栈也可以适当了解。 2020年会看《Java核心技术 卷一》等Java基础书籍，关于新技术栈目前对容齐比较感兴趣，希望系统的了解下Kubernetes领域。 软能力沟通一个好的程序员，需要有好的学习能力，这样你才能为成为技术专家，但是，你还要有好的沟通能力，不然你的技术能力发挥不出来。就像一棵大树一样，学习能力能让你根越来越深，无论遇到什么狂风暴雨，你都可以屹立不倒，而沟通能力则是树干和树叶，它们能让你延伸到更远的天空。 自己沟通能力欠佳，逻辑有时会混乱，容易激动，语速加快，不能很准确的表达出心里的想法。 2020年会看下《简单的逻辑学》和《金字塔理论》，并在沟通的时候有意的注意下以上问题。 写作能力今年感觉自己基本没有写作能力和画技术流程图、架构图的能力，把技术方案输出为文档能力很差。 2020年会着重注意这个问题。 英语搜索尽量google，经常看下medium里的文章。 总结2020年，自己会调整好心态，一步一个脚印，积少成多。浮躁的时候，要多想想这句话「学习是一辈子的事，没有速成」。带着浮躁的心态学习，过程很痛苦，而且最后收获很少。所以2020会从以下几个方面入手，在实践中总结出适合自己的学习方法 端正学习态度，摆正心态 坚持学习 一定要动手实践 不要乱买书，一定要买某个技术中最经典的，评价最高的]]></content>
  </entry>
  <entry>
    <title><![CDATA[TCP协议详解]]></title>
    <url>%2F2020%2F01%2F10%2FTCP%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[TCP首部格式 数据偏移：表示TCP所传输的数据部分应该从TCP包的哪个位开始计算，当然也可以看成TCP首部的长度。该字段长4位，单位4字节 如果没有Options，TCP头部占20字节，Options最多40字节(数据偏移量占4个bit位代表4个字节，即头部最多15*4=60个字节) TCP选项 TCP的特点与其目的TCP通过校验和、序列号、确认应答、重发控制、连接管理、流控制以及拥塞控制等机制实现可靠传输。 通过序列号与确认应答提高可靠性丢包的情况 确认应答丢失的情况 序列号上述这些确认应答处理、重发控制以及重复控制等功能都可以通过序列号实现。 序列号是按照顺序给发送数据的每一个字节（8位字节）都标上号码的编码。接收端查询接受数据TCP首部中的序列号和数据长度，将自己下一步应该接受的序列号作为确认应该返送回去。这样通过序列号和确认应答号，TCP可以实现可靠传输。 序号不能都从1开始，因为A发1/2/3，三个包给B，中途3丢了，这时A掉线了重新连上B，如果序号又从1开始，发送1/2，这时之前丢的3绕回来了，B就认为这个3是下一个包，于是发生了错误。 因此每个连接都有不同的序号这个起始序号是随着时间变化的可以看成一个32位的计数器 序列号回绕序列号的范围是0-2^32 也就是0-4G，如果一个连接累计发送超过4G的流量，那么序列号就开始复用了。比如你发送0G-4G流量的时候丢了一个包，当序列号开始复用的时候这个丢的包恰好到了接收方，那么就有问题了。如何解决这个问题呢？ 解决序列号回绕使用TCP options中的时间戳。 重发时间如何确定RTO(Retransmission TimeOut )，重传超时时间。 RTT(Round-Trip Time)，报文往返时间。 重发超时时间(RTO)是指在重发数据之前，等待确认应答到来的那个特定时间间隔(RTT)，如果超过这个时间仍然未收到确认应答，发送端将进行数据重发。 RTT如何计算TCP不论在何种网络环境下都要提供高性能通信，并且无论网络拥堵情况发生了何种变化，都必须保持这一特性。为此，它在每次发包时都会计算往返时间及其偏差。将这个时间和偏差相加，重发超时的时间就是比这个总和要稍微大一点的值。 重发的计算既要考虑往返时间又要考虑偏差是有原因的。如图所示，根据网络环境的不同往返时间可能会产生大幅度的摇摆，之所以发生这种情况是因为数据包的分段是经过不同线路到达的。 重发策略数据被重发之后若还是收不到确认应答，则进行再次发送。此时，等待确认应答的时间将会以2倍、4倍的指数函数延长。此外数据也不会被无限的、反复地重发。达到一定重发次数后，如果仍没有任何确认应答返回，就会判断为网络或对端主机发生了异常，强行关闭连接，并且通知应用通信异常强行终止。 linux设置RTO间隔和次数，Linux TCP_RTO_MIN, TCP_RTO_MAX and the tcp_retries2 sysctl 连接管理三次握手目标 建立连接 TCP是面向连接的，所以在数据通信之前，需要先建立连接。保证双方的包有去有回连接就建立成功了。 同步 Sequence 序列号 初始序列号 ISN(Initial Sequence Number) 交换 TCP 通讯参数如 MSS、窗口比例因子、选择性确认、指定校验和算法 状态变迁 存在的性能和安全问题三次握手示例流程 性能 应用层 connect 超时时间调整 操作系统内核限制调整 服务器端 SYN_RCV 状态 net.ipv4.tcp_max_syn_backlog:SYN_RCVD 状态连接的最大个数 net.ipv4.tcp_synack_retries:被动建立连接时，发SYN/ACK的重试次数 net.ipv4.tcp_abort_on_overflow:超出处理能力时，对新来的 SYN 直接回包 RST，丢弃连接 客户端 SYN_SENT 状态 net.ipv4.tcp_syn_retries = 6 主动建立连接时，发 SYN 的重试次数 net.ipv4.ip_local_port_range = 32768 60999 建立连接时的本地端口可用范围 ACCEPT/backlog队列设置当应用创建一个sock的时候，需要制定这个参数。How TCP backlog works in Linux Fast Open降低时延net.ipv4.tcp_fastopen:系统开启 TFO 功能 0:关闭 1:作为客户端时可以使用 TFO 2:作为服务器时可以使用 TFO 3:无论作为客户端还是服务器，都可以使用 TFO TCP_DEFER_ACCEPT设置到server端收到ack后不会进入established，而是丢掉ack，等到接收到client发送的数据才会进入established状态。既然server端忽略ACK，那么client可不可以不发送ACK，而直接发送数据呢，这样就节省了一次传输？答案是可以的：TCP_DEFER_ACCEPT设置在client端的socket fd上，就能达到这个效果。 安全，如何应对SYN攻击SYN攻击属于DOS攻击的一种，它利用TCP协议缺陷，伪造IP通过发送大量的半连接请求，服务器回复确认包（进入SYN_RECV），并等待客户的确认，由于源地址是不存在的，服务器需要不断的重发直至超时，这些伪造的SYN包将长时间占用未连接队列(syn_backlog)，正常的SYN请求被丢弃，目标系统运行缓慢，严重者引起网络堵塞甚至系统瘫痪。使服务器不能为正常用户服务。 tcp_syncookies net.ipv4.tcp_syncookies = 1 当 SYN 队列满后，新的 SYN 不进入队列，计算出 cookie 再 以 SYN+ACK 中的序列号返回客户端，正常客户端发报文时， 服务器根据报文中携带的 cookie 重新恢复连接。 由于 cookie 占用序列号空间，导致此时所有 TCP 可选 功能失效，例如扩充窗口、时间戳等四次挥手 为什么建立连接是三次握手，关闭连接确是四次挥手呢？ 建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 TCP的三次握手与四次挥手（详解+动图） 优化关闭连接时的TIME-WAIT状态 MSL(Maximum Segment Lifetime)报文最大生存时间 维持 2MSL 时长的 TIME-WAIT 状态 保证至少一次报文的往返时间内 端口是不可复用 TIME-WAIT的作用?这要从两个方面来说。 首先，这样做是为了确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。 TCP 在设计的时候，做了充分的容错性设计，比如，TCP 假设报文会出错，需要重传。在这里，如果图中主机 1 的 ACK 报文没有传输成功，那么主机 2 就会重新发送 FIN 报文。如果主机 1 没有维护 TIME_WAIT 状态或者维时间太短，而直接进入 CLOSED 状态，它就失去了当前状态的上下文，只能回复一个 RST 操作，从而导致被动关闭方出现错误。现在主机 1 知道自己处于 TIME_WAIT 的状态，就可以在接收到 FIN 报文之后，重新发出一个 ACK 报文，使得主机 2 可以进入正常的 CLOSED 状态。 第二个理由和连接“化身”和报文迷走有关系，为了让旧连接的重复分节在网络中自然消失。 我们知道，在网络中，经常会发生报文经过一段时间才能到达目的地的情况，产生的原因是多种多样的，如路由器重启，链路突然出现故障等。如果迷走报文到达时，发现 TCP 连接四元组（源 IP，源端口，目的 IP，目的端口）所代表的连接不复存在，那么很简单，这个报文自然丢弃。我们考虑这样一个场景，在原连接中断后，又重新创建了一个原连接的“化身”，说是化身其实是因为这个连接和原先的连接四元组完全相同，如果迷失报文经过一段时间也到达，那么这个报文会被误认为是连接“化身”的一个 TCP 分节，这样就会对 TCP 通信产生影响。 所以，TCP 就设计出了这么一个机制，经过 2MSL 这个时间，足以让两个方向上的分组都被丢弃，使得原来连接的分组在网络中都自然消失，再出现的分组一定都是新化身所产生的。 TIME_WAIT 的危害过多的 TIME_WAIT 的主要危害有两种。 第一是内存资源占用，这个目前看来不是太严重，基本可以忽略。 第二是对端口资源的占用，一个 TCP 连接至少消耗一个本地端口。要知道，端口资源也是有限的，一般可以开启的端口为 32768～61000 ，也可以通过net.ipv4.ip_local_port_range指定，如果 TIME_WAIT 状态过多，会导致无法创建新连接。这个也是我们在一开始讲到的那个例子。 如何优化TIME_WAITnet.ipv4.tcp_max_tw_buckets一个暴力的方法是通过 sysctl 命令，将系统值调小。这个值默认为 18000，当系统中处于 TIME_WAIT 的连接一旦超过这个值时，系统就会将所有的 TIME_WAIT 连接状态重置，并且只打印出警告信息。这个方法过于暴力，而且治标不治本，带来的问题远比解决的问题多，不推荐使用。 net.ipv4.tcp_tw_reuse那么 Linux 有没有提供更安全的选择呢？当然有。这就是net.ipv4.tcp_tw_reuse选项。 Linux 系统对于net.ipv4.tcp_tw_reuse的解释如下: 1Allow to reuse TIME-WAIT sockets for new connections when it is safe from protocol viewpoint. Default value is 0.It should not be changed without advice/request of technical experts. 这段话的大意是从协议角度理解如果是安全可控的，可以复用处于 TIME_WAIT 的套接字为新的连接所用。 那么什么是协议角度理解的安全可控呢？主要有两点： 只适用于连接发起方（C/S 模型中的客户端）； 对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用。 使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即net.ipv4.tcp_timestamps=1（默认即为 1）。 要知道，TCP 协议也在与时俱进，RFC 1323 中实现了 TCP 拓展规范，以便保证 TCP 的高可用，并引入了新的 TCP 选项，两个 4 字节的时间戳字段，用于记录 TCP 发送方的当前时间戳和从对端接收到的最新时间戳。由于引入了时间戳，我们在前面提到的 2MSL 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。 net.ipv4.tcp_tw_recycle 开启后，同时作为客户端和服务器都可以使用 TIME-WAIT 状态的端口，不安全，容易造成端口接收数据混乱，无法避免报文延迟、重复等给新连接造成混乱。 TCP以段为单位发送数据 定义:仅指 TCP 承载数据，不包含 TCP 头部的大小，参见 RFC879 MSS 选择目的 尽量每个 Segment 报文段携带更多的数据，以减少头部空间占用比率 防止 Segment 被某个设备的 IP 层基于 MTU 拆分 默认 MSS:536 字节(默认 MTU576 字节，20 字节 IP 头部，20 字节 TCP 头部) 握手阶段协商 MSS两端主机在发出建立连接的请求时，会在TCP首部中写入MSS选项，然后会在两者之间选择一个较小的值使用 MSS 分类 发送方最大报文段 SMSS:SENDER MAXIMUM SEGMENT SIZE 接收方最大报文段 RMSS:RECEIVER MAXIMUM SEGMENT SIZE 窗口控制利用窗口控制提高速度TCP以一个段为单位，每发一个段进行一次确认应答的处理。这样的传输方式有个缺点。那就是包的往返时间越长通信性能就越低。 为了解决这个问题，TCP引入了窗口这个概念。即使在往返时间较长的情况下，他也能控制网络性能的下降。 缓存发送端的缓存第一部分：发送并已经确认的 第二部分：发送还并且尚未确认 第三部分：没有发送，但是已经等待发送 第四部分：没有发送，并且暂时不会发送 TCP里接收端会给发送端报一个窗口大小，叫做AdvertisedWindow,窗口大小=第二部分+第三部分 接收端的缓存第一部分：接收已确认,之后是已经接收了，但是还没被应用层读取的 第二部分：等待接收，也就是能承受的最大工作量。也就是AdvertisedWindow 第三部分：还没接收，无法接受，因为超过了最大承受量 其中第二部分里面，由于收到的包可能不是顺序的，会出现空挡，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。 窗口控制与重发策略丢包怎么办某些ack未能返回 某个报文段丢失(快速重传) 流控制发送端根据自己的实际情况发送数据。但是接收端可能收到的是一个毫无关系的数据包又可能会在处理问题其他问题上花费一些时间。因此在为这个数据包做其他处理时会耗费一些时间，甚至在高负荷的情况下无法接受任何数据。如此一来，如果接收端将本应该接收的数据丢弃的话，就又会触发重发机制，从而导致网络流量的无端浪费。为了防止这种现象的发生，TCP提供一种机制可以让发送端根据接收端的实际接收能力控制发送的数据量。这就是所谓的流控 窗口滑动与流量控制对于包的确认中，同时会携带一个窗口大小。我们假设窗口不变，始终为9，4的ACK来的时候会右移一个，这个时候第13个包也可以发送了 这个时候如果发送端发送过猛，会将10、11、12、13都发送了，之后停止发送，因为未发送可发送部分为0。 这时候又收到了5的ACK，窗口又会往右滑动一格，这时才可以有更多包可以发送 如果接收方处理的太慢，导致缓存中没有空间，可以通过确认信息修改窗口大小，甚至可以设置为0，让发送方定制发送。 我们假设一个极端情况，接收端的应用一直不读取缓存里的数据，当数据包6确认后，窗口大小不能在是9了，就要缩小一个变成8。 这个新窗口8通过6的确认消息，到达发送端的时候，你会发现发送端窗口没有右移，而只是左边的移动了，窗口大小由9变成8 如果接收端的应用还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，甚至变为0 当窗口大小通过14包的ACK到达发送端时，发送端的窗口也变为0。 如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。 操作系统缓冲区和滑动窗口的关系 应用层没有及时读取缓存 收紧缓冲区导致的丢包解决办法：先收缩窗口在减少缓冲区 拥塞控制有了TCP的窗口控制，收发主机之间即使不再以一个数据段为单位发送确认应答，也能够连续发送大量的数据包。然后，如果在通信刚开始时就发送大量的数据，也可能会引发其他问题。 那发送方怎么判断网络是不是满呢？这其实是个挺难的事情，因为对于 TCP 协议来讲，他压根不知道整个网络路径都会经历什么，对他来讲就是一个黑盒。 假设tcp发送的速度超过了带宽，中间设备处理不了的包就会被丢弃， 为了解决这个问题中间经过的设备会会加上缓存，处理不过来的在队列里排着，虽然避免了丢包，但是造成了超时重传 于是TCP的拥塞控制住要来解决这两种现象，包丢失和超时重传，一旦出现了这些现象就说明太快了，要慢一点。TCP 的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽 拥塞窗口cwnd(congestion window) 通告窗口rwnd(receiver‘s advertised window) 发送窗口swnd = min(cwnd，rwnd) 拥塞控制历史 以丢包作为依据 • New Reno:RFC6582 • BIC:Linux2.6.8 – 2.6.18 • CUBIC(RFC8312):Linux2.6.19 以探测带宽作为依据• BBR:Linux4.9 以丢包为依据的CUBIC慢启动每收到一个ACK，cwnd扩充一倍，呈指数级增长。 有了上述这些机制，就可以有效地减少通信开始时的连续发包导致的网络拥堵，还可以避免网络拥塞情况的发生。 拥塞避免不过随着每次往返，拥塞窗口也会以指数函数的增长，拥堵状况激增甚至导致网络拥塞的发生。为了防止这些，引入慢启动阀值 ssthresh(slow start threshold)的概念。只要拥塞窗口超过这个阀值，在没收到一次确认应答时，只允许以下面这种比例放大拥塞窗口。达到 ssthresh 后，以线性方式增加 cwnd += SMSS*SMSS/cwnd 超时重传但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。 拥塞的一种表现形式是丢包，需要超时重传，这个时候，将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。 快速重传与快速恢复前面讲过快速重传算法。当接收端发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速的重传，不必等待超时再重传。TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd 减半为 cwnd/2，然后 sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。 TCP窗口变化图 从丢包到测量驱动的拥塞控制算法上述拥塞策略有什么问题 第一个问题是是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。 第二个问题是 TCP 的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实 TCP 只要填满管道就可以了，不应该接着填，直到连缓存也填满。 为了优化这两个问题出来了TCP拥塞 BBR算法。它企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。 提高网络利用率SACK 与选择性重传算法仅重传丢失段 保守乐观累积确认 Sequence 序号的问题 如果part3没有收到，ack只会通知server端它没有收到part3，但是不会告知他收到了part4. 在这张图中这个算法工作的特别好，因为只重传了丢失的part3。但是如果part4页丢失了呢，part3经过重发已经收到了了，那么client又开始通知server端part4也没有收到，效率比较低。 仅重传丢失段:保守乐观 ，大量丢包时效率低下 重传所有段 –积极悲观 重传所有段:积极悲观 ，可能浪费带宽 引入SACK( Selective Acknowledgment) wireshark抓包 如何减少小报文提高网络效率?糊涂窗口综合症SWS 糊涂窗口综合症（Silly window syndrome），亦称愚蠢窗口综合症、愚笨窗口综合症，是TCP流量控制实现不良导致的一种计算机网络问题。当发送程序缓慢地创建数据，接收程序缓慢地消耗数据，或者两者同时存在时，滑动窗口运作会出现严重问题。如果一个服务器无法处理所有传入的数据而存在此问题，它会要求客户端减少一次发送的数据量（TCP网络数据包的滑动窗口）。如果服务器仍然无法处理所有传入的数据，窗口会随此问题越来越小，有时甚至将使传输数据小于数据包头，使数据传输变得极为低效。这个问题的名字取自窗口缩小到“愚蠢”的值。 SWS 避免算法发送方• Nagle 算法:TCP_NODELAY 用于关闭 Nagle 算法 Nagle解决方案要求发送方发送第一个段，即使它是小的，然后就等待直至收到一个ACK，或者累积到最大大小段（MSS）。 接收方 • David D Clark 算法:Clark的解决方案是关闭窗口，直到接收到最大段大小（MSS）的另一个段，或者缓冲区为半空。 TCP delayed acknowledgment 延迟确认接收数据的主机如果每次都立即回复确认的话，可能会返回一个较小的窗口。那是因为刚接受完数据，应用还没有读取。当某个发送端收到这个小窗口后，会以它为上限发送数据，从而降低网络利用率。因此引入一个方法，那就是收到数据不立即返回确认，而是延时一段时间。 在没有收到2✖️最大长度的数据为止不做确认应答(根据操作系统的不同，有时也有不论数据大小，只要收到两个包就即刻返回确认应答的情况。) 其他情况下，最大延迟0.5秒发送确认应答( 很多操作系统设置为0.2秒左右) (PiggyBack Acknowleggement)捎带应答TCP的确认应答和回执数据可以通过一个包发送。这种方式叫做捎带应答。通过这种机制，可以使收发的数据量减少。 另外，接收数据以后如果立刻返回确认应答，就无法实现捎带应答。而是将所接收的数据传给应用处理生成返回数据以后再进行发送请求为止，必须一直等待确认应答的发送。也就是说，如果没有启用延迟确认应答就无法实现捎带应答。 Linux 上更为激进的“Nagle”：TCP_CORKNagle 算法与 TCP socket 选项 TCP_CORK Nagle VS delayed ACKNagle和delayed ACK不能同时打开 • 关闭 delayed ACK:TCP_QUICKACK • 关闭 Nagle:TCP_NODELAY TCP的keep-alive功能 Linux 的 tcp keepalive • 发送心跳周期 • Linux: net.ipv4.tcp_keepalive_time = 7200 • 探测包发送间隔 • net.ipv4.tcp_keepalive_intvl = 75 • 探测包重试次数 • net.ipv4.tcp_keepalive_probes = 9]]></content>
      <tags>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《趣谈网络协议》云计算中的网络24-28]]></title>
    <url>%2F2019%2F11%2F01%2F%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C24-28%2F</url>
    <content type="text"><![CDATA[虚拟网卡的原理 虚拟机互联在物理机上，应该有一个虚拟的交换机，在 Linux 上有一个命令叫作 brctl，可以创建虚拟的网桥 brctl addbr br0。创建出来以后，将两个虚拟机的虚拟网卡，都连接到虚拟网桥 brctl addif br0 tap0 上，这样将两个虚拟机配置相同的子网网段，两台虚拟机就能够相互通信了。 这里面，host-only 的网络对应的，其实就是上面两个虚拟机连到一个 br0 虚拟网桥上，而且不考虑访问外部的场景，只要虚拟机之间能够相互访问就可以了。 虚拟机连接外网 桥接 虚拟交换机就是br0 如果使用桥接网络，当你登录虚拟机里看 IP 地址的时候会发现，你的虚拟机的地址和你的笔记本电脑的，以及你旁边的同事的电脑的网段是一个网段。这是为什么呢？这其实相当于将物理机和虚拟机放在同一个网桥上，相当于这个网桥上有三台机器，是一个网段的，全部打平了。我将图画成下面的样子你就好理解了。 在数据中心里面，采取的也是类似的技术，只不过都是 Linux，在每台机器上都创建网桥 br0，虚拟机的网卡都连到 br0 上，物理网卡也连到 br0 上，所有的 br0 都通过物理网卡出来连接到物理交换机上。 同样我们换一个角度看待这个拓扑图。同样是将网络打平，虚拟机会和你的物理网络具有相同的网段。 在这种方式下，不但解决了同一台机器的互通问题，也解决了跨物理机的互通问题，因为都在一个二层网络里面，彼此用相同的网段访问就可以了。但是当规模很大的时候，会存在问题。你还记得吗？在一个二层网络里面，最大的问题是广播。 NAT 在这种方式下，你登录到虚拟机里面查看 IP 地址，会发现虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。虚拟机要想访问物理机的时候，需要将地址 NAT 成为物理机的地址。 除此之外，它还会在你的笔记本电脑里内置一个 DHCP 服务器，为笔记本电脑上的虚拟机动态分配 IP 地址。因为虚拟机的网络自成体系，需要进行 IP 管理。为什么桥接方式不需要呢？因为桥接将网络打平了，虚拟机的 IP 地址应该由物理网络的 DHCP 服务器分配。 如果是你自己登录到物理机上做个简单配置，你可以简化一下。例如将虚拟机所在网络的网关的地址直接配置到 br0 上，不用 DHCP Server，手动配置每台虚拟机的 IP 地址，通过命令 iptables -t nat -A POSTROUTING -o ethX -j MASQUERADE，直接在物理网卡 ethX 上进行 NAT，所有从这个网卡出去的包都 NAT 成这个网卡的地址。通过设置 net.ipv4.ip_forward = 1，开启物理机的转发功能，直接做路由器，而不用单独的路由器，这样虚拟机就能直接上网了 隔离问题如果一台机器上的两个虚拟机不属于同一个用户，怎么办呢？ 好在 brctl 创建的网桥也是支持 VLAN 功能的，可以设置两个虚拟机的 tag，这样在这个虚拟网桥上，两个虚拟机是不互通的。 但是如何跨物理机互通，并且实现 VLAN 的隔离呢？由于 brctl 创建的网桥上面的 tag 是没办法在网桥之外的范围内起作用的，于是我们需要寻找其他的方式。有一个命令vconfig可以基于物理网卡 eth0 创建带 VLAN 的虚拟网卡，所有从这个虚拟网卡出去的包，都带这个 VLAN，如果这样，跨物理机的互通和隔离就可以通过这个网卡来实现。 首先为每个用户分配不同的 VLAN，例如有一个用户 VLAN 10，一个用户 VLAN 20。 在一台物理机上，基于物理网卡，为每个用户用 vconfig 创建一个带 VLAN 的网卡。 不同的用户使用不同的虚拟网桥，带 VLAN 的虚拟网卡也连接到虚拟网桥上。 不同的用户由于网桥不通，不能相互通信，一旦出了网桥，由于 VLAN 不同，也不会将包转发到另一个网桥上。另外，出了物理机，也是带着 VLAN ID 的。只要物理交换机也是支持 VLAN 的，到达另一台物理机的时候，VLAN ID 依然在，它只会将包转发给相同 VLAN 的网卡和网桥，所以跨物理机，不同的 VLAN 也不会相互通信。 缺点： 使用 brctl 创建出来的网桥功能是简单的，基于 VLAN 的虚拟网卡也能实现简单的隔离。但是这都不是大规模云平台能够满足的，一个是 VLAN 的隔离，数目太少。前面我们学过，VLAN ID 只有 4096 个，明显不够用。另外一点是这个配置不够灵活。谁和谁通，谁和谁不通，流量的隔离也没有实现，还有大量改进的空间。]]></content>
  </entry>
  <entry>
    <title><![CDATA[《趣谈网络协议》第14-23讲笔记]]></title>
    <url>%2F2019%2F10%2F25%2F%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E7%AC%AC14-23%E8%AE%B2%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[HTTP 协议虽然很常用，也很复杂，重点记住 GET、POST、 PUT、DELETE 这几个方法，以及重要的首部字段； HTTP2.0HTTP 2.0 通过头压缩、分帧、二进制编码、多路复用等技术提升性能； 相比HTTP1.1的优点HTTP 2.0 会对 HTTP 的头进行一定的压缩，将原来每次都要携带的大量 key value 在两端建立一个索引表，对相同的头只发送索引表中的索引 HTTP 2.0 协议将一个 TCP 的连接中，切分成多个流，每个流都有自己的 ID，而且流可以是客户端发往服务端，也可以是服务端发往客户端。它其实只是一个虚拟的通道。流是有优先级的 HTTP 2.0 还将所有的传输信息分割为更小的消息和帧，并对它们采用二进制格式编码。常见的帧有Header 帧，用于传输 Header 内容，并且会开启一个新的流。再就是Data 帧，用来传输正文实体。多个 Data 帧属于同一个流。 通过以上两种机制，HTTP 2.0 的客户端可以将多个请求分到不同的流中，然后将请求内容拆成帧，进行二进制传输。这些帧可以打散乱序发送， 然后根据每个帧首部的流标识符重新组装，并且可以根据优先级，决定优先处理哪个流的数据。 我们来举一个例子。假设我们的一个页面要发送三个独立的请求，一个获取 css，一个获取 js，一个获取图片 jpg。如果使用 HTTP 1.1 就是串行的，但是如果使用 HTTP 2.0，就可以在一个连接里，客户端和服务端都可以同时发送多个请求或回应，而且不用按照顺序一对一对应。 HTTP 2.0 其实是将三个请求变成三个流，将数据分成帧，乱序发送到一个 TCP 连接中。 HTTP 2.0 成功解决了 HTTP 1.1 的队首阻塞问题，同时，也不需要通过 HTTP 1.x 的 pipeline 机制用多条 TCP 连接来实现并行请求与响应，减少了 TCP 连接数对服务器性能的影响。同时将页面的多个数据 css、js、 jpg 等通过一个数据链接进行传输，能够加快页面组件的传输速度。 不足HTTP 2.0 虽然大大增加了并发性，但还是有问题的。因为 HTTP 2.0 也是基于 TCP 协议的，TCP 协议在处理包时是有严格顺序的。当其中一个数据包遇到问题，TCP 连接需要等待这个包完成重传之后才能继续进行。虽然 HTTP 2.0 通过多个 stream，使得逻辑上一个 TCP 连接上的并行内容，进行多路数据的传输，并且这些数据没有关联。一前一后，前面 stream 2 的帧没有收到，后面 stream 1 的帧也会因此阻塞。 QUIC于是，就又到了从 TCP 切换到 UDP。这就是 Google 的 QUIC 协议。 QUIC 协议通过基于 UDP 自定义的类似 TCP 的连接、重试、多路复用、流量控制技术，进一步提升性能。 HTTPS加密分对称加密和非对称加密。对称加密效率高，但是解决不了密钥传输问题；非对称加密可以解决这个问题，但是效率不高。 非对称加密需要通过证书和权威机构来验证公钥的合法性。 HTTPS 是综合了对称加密和非对称加密算法的 HTTP 协议。既保证传输安全，也保证传输效率。 FTP下载FTP 采用两个 TCP 连接来传输一个文件。 控制连接：服务器以被动的方式，打开众所周知用于 FTP 的端口 21，客户端则主动发起连接。该连接将命令从客户端传给服务器，并传回服务器的应答。常用的命令有：list——获取文件目录；reter——取一个文件；store——存一个文件。 数据连接：每当一个文件在客户端与服务器之间传输时，就创建一个数据连接。 FTP两种工作模式 主动模式(PORT) 主动模式是FTP客户端向FTP服务器发送数据传输需要的端口，FTP服务端去连接FTP客户端的端口。 被动模式(PASV) 被动模式是FTP服务器返回数据传输需要的端口，FTP客户端去连接FTP服务端。绝大部分的互联网应用(比如Web/Http)，都是客户端向服务端发起连接。换句话说，绝大部分互联网应用都是被动模式。 需要注意的是，被动模式和主动模式的登录过程，都是FTP客户端去连接FTP服务器。 为什么绝大部分互联网应用都是被动模式因为大部分客户端都是在路由器后面，没有独立的公网IP地址，服务器想要主动连接客户端，难度太大，在现在真实的互联网环境里面几乎是不可能完成的任务。 P2P种子文件(.torrent)由两部分组成，分别是：announce（tracker URL）和文件信息。 文件信息里面有这些内容。 info 区：这里指定的是该种子有几个文件、文件有多长、目录结构，以及目录和文件的名字。 Name 字段：指定顶层目录名字。 每个段的大小：BitTorrent（简称 BT）协议把一个文件分成很多个小段，然后分段下载。 段哈希值：将整个种子中，每个段的 SHA-1 哈希值拼在一起 下载时，BT 客户端首先解析.torrent 文件，得到 tracker 地址，然后连接 tracker 服务器。tracker 服务器回应下载者的请求，将其他下载者（包括发布者）的 IP 提供给下载者。 虽然下载的过程是非中心化的，但是加入这个 P2P 网络的时候，都需要借助 tracker 中心服务器，这个服务器是用来登记有哪些用户在请求哪些资源。所以，这种工作方式有一个弊端，一旦 tracker 服务器出现故障或者线路遭到屏蔽，BT 工具就无法正常工作了。能不能彻底非中心化呢？请看DHT 去中心化网络(DHT)DHT(Distributed Hash Table) 有一种著名的 DHT 协议，叫 Kademlia 协议，接下来要介绍的就是这个协议。 任何一个BitTorrent启动后，都有两个角色 peer角色 监听TCP端口，用来上传下载文件。 DHT node 监听一个UDP端口，通过这个角色加入DHT网络 存储和定位一个DHT网络里，每一个DHT Node，都有一个id，每个DHT都保存有一些索引，哪些文件保存在哪个节点上。 算法规定，如果有文件的哈希值和DHT node的id一致，则这个DHT node知道从哪里下载，除了一模一样的DHT node知道，和哈希值接近的N个DHT node也知道。什么叫和哈希值接近呢？例如只修改了最后一位，就很接近；修改了倒数 2 位，也不远；修改了倒数 3 位，也可以接受。总之，凑齐了规定的 N 这个数就行。 如何找到对应的NODE呢分层 如果一个节点的 ID，前面所有位数相同，从倒数第 i 位开始不同，这样的节点只有 2^(i-1) 个，与基础节点的距离范围为 [2^(i-1), 2^i)；对于 01010 而言，这样的节点归为“k-bucket i”。 查找判断目标节点C与自己从倒数第i几位开始不同，然后从“k-bucket i”中去找，这一层所有NODE的第i位肯定和A的第i位不一样 如果有，那就找到了； 如果没有，在k-bucket i里随便找一个B节点（注意任意B节点，它的第i位肯定与C相同，即它与C的距离小于 2^(i-1)，距离缩短了一半，Kademlia 的这种查询机制，是通过折半查找的方式来收缩范围，对于总的节点数目为 N，最多只需要查询 log2(N) 次，就能够找到。） Kademlia 算法中，每个节点只有 4 个指令。 PING：测试一个节点是否在线，还活着没，相当于打个电话，看还能打通不。 STORE：要求一个节点存储一份数据，既然加入了组织，有义务保存一份数据。 FIND_NODE：根据节点 ID 查找一个节点，就是给一个 160 位的 ID，通过上面朋友圈的方式找到那个节点。 FIND_VALUE：根据 KEY 查找一个数据，实则上跟 FIND_NODE 非常类似。KEY 就是文件对应的 160 位的 ID，就是要找到保存了文件的节点。 k-bucket的维护及更新机制 每个bucket里的节点都按照最后一次接触的时间倒序排列(按最后一次接触时间从末尾往开头排序) 每次执行四个指令的任何一个都会触发更新 当一个节点与自己接触时，检查它是否在K-bucket中 –如果在，那么将它挪到k-bucket列表最底层（最新） –如果不在，PING一下列表最上面（最旧）的一个节点 –a）如果PING通，将旧节点挪到列表最底，并丢弃新节点 –b）如果PING不通，删除旧节点，并将新节点加入列表 该机制保证了任意节点加入和离开都不影响整体网络 易懂分布式 | Kademlia算法 DNS 传统的 DNS 有很多问题，例如解析慢、更新不及时。因为缓存、转发、NAT 问题导致客户端误会自己所在的位置和运营商，从而影响流量的调度。 HTTPDNS 通过客户端 SDK 和服务端，通过 HTTP 直接调用解析 DNS 的方式，绕过了传统 DNS 的这些缺点，实现了智能的调度 CDN CDN 和电商系统的分布式仓储系统一样，分为中心节点、区域节点、边缘节点，而数据缓存在离用户最近的位置。 CDN 最擅长的是缓存静态数据，除此之外还可以缓存流媒体数据，这时候要注意使用防盗链。它也支持动态数据的缓存，一种是边缘计算的生鲜超市模式，另一种是链路优化的冷链运输模式。 VPN连接多个数据中心的方式： 走公网，最简单但不安全 专线连接，成本高昂，效率高 VPN 连接，简单便宜，保证私密性，性能稍差 IPSec VPN「Virtual Private Network」工作原理：将要发送的 IP 包「乘客协议」加密后加上 IPSec 包头「隧道协议」后再放入另一个 IP 包「承载协议」中发送 IPSec VPN 是基于 IP 协议的安全隧道协议，采用一些机制保证安全性 私密性：加密数据 完整性：对数据进行 hash 运算产生数据摘要 真实性：通过身份认证保证对端身份的真实性 IPSec VPN 协议簇包括： 两种协议： AH「Authentication Header」：只能进行数据摘要，不能实现数据加密 ESP「Encapsulating Security Payload」：能够进行数据加密和数据摘要 两种算法： 加密算法 摘要算法 两大组件： IKE「Internet Key Exchange Key Management」：用于交换对称秘钥 SA「Security Association」：进行连接维护 IPsec VPN 的建立过程： 建立 IKE 自己的 SA，算出对称秘钥 K 使用对称秘钥 K 建立 IPSec SA，在 SA 中生成随机对称秘钥 M，使用 M 进行双方接下来的通信 扩展IP 协议：不是面向连接的，是尽力而为的协议，每个 IP 包自由选择路径，依赖于上一层 TCP 的重发来保证可靠性 优点：一条道路崩溃时，可以自动换其他路 缺点：不断的路由查找，效率低下 IPSec VPN 的缺点：由于 IPSec VPN 是基于 IP 协议的，所以速度慢 ATM 协议：这种协议是面向连接的，并且和 IP 是同一个层次，ATM 是在传输之前先建立一个连接，形成一个虚拟的通路 优点：速度快，因为按照指定路径传输 缺点：当某个节点故障，连接就会中断，无法传输数据 多协议标签交换「MPLS，Multi-Protocol Label Switching」结合了 IP 和 ATM 协议的优点 需要标签交换路由器「LSR，Label Switching Router」的支持 如何动态生成标签 LDP「Label Distribution Protocol」 将 MPLS 和 VPN 结合起来可以提高 VPN 的效率需要解决的问题有： BGP 协议如何处理地址空间重叠的 VPN 的路由 路由表怎么区分重复的网段 移动网络手机是通过收发无线信号来通信的，专业名称是 Mobile Station，简称 MS，需要嵌入 SIM。手机是客户端，而无线信号的服务端，就是基站子系统（BSS，Base Station SubsystemBSS） 2G2G 时代，上网使用的不是 IP 网络，而是电话网络，走模拟信号，专业名称为公共交换电话网（PSTN，Public Switched Telephone Network）。 基站子系统分两部分，一部分对外提供无线通信，叫作基站收发信台（BTS，Base Transceiver Station），另一部分对内连接有线网络，叫作基站控制器（BSC，Base Station Controller）。基站收发信台通过无线收到数据后，转发给基站控制器。这部分属于无线的部分，统称为无线接入网（RAN，Radio Access Network）。 基站控制器通过有线网络，连接到提供手机业务的运营商的数据中心，这部分称为核心网（CN，Core Network）。核心网还没有真的进入互联网，这部分还是主要提供手机业务，是手机业务的有线部分。 首先接待基站来的数据的是移动业务交换中心（MSC，Mobile Service Switching Center），它是进入核心网的入口。 鉴权中心（AUC，Authentication Center）和设备识别寄存器（EIR，Equipment Identity Register）主要是负责安全性的 另外，需要看你是本地的号，还是外地的号，这个牵扯到计费的问题，异地收费还是很贵的。访问位置寄存器（VLR，Visit Location Register）是看你目前在的地方，归属位置寄存器（HLR，Home Location Register）是看你的号码归属地。 网关移动交换中心（GMSC ，Gateway Mobile Switching Center）是一个网关，连接核心网和真正的互联网。 数据中心里面的这些模块统称为网络子系统（NSS，Network and Switching Subsystem） 因而 2G 时代的上网如图所示，我们总结一下，有这几个核心点： 手机通过无线信号连接基站； 基站一面朝前接无线，一面朝后接核心网； 核心网一面朝前接到基站请求，一是判断你是否合法，二是判断你是不是本地号，还有没有钱，一面通过网关连接电话网络。 2.5G 相对2G网络的变化： 原来电路交换的基础上，加入了分组交换业务，支持 Packet 的转发，从而支持 IP 网络。** 在上述网络的基础上，基站一面朝前接无线，一面朝后接核心网。在朝后的组件中，多了一个分组控制单元（PCU，Packet Control Unit），用以提供分组交换通道。 在核心网里面，有个朝前的接待员（SGSN，Service GPRS Supported Node）和朝后连接 IP 网络的网关型 GPRS 支持节点（GGSN，Gateway GPRS Supported Node）。 3G 到了 3G 时代，主要是无线通信技术有了改进，大大增加了无线的带宽。以 W-CDMA 为例，理论最高 2M 的下行速度。 改变： 基站改变了，一面朝外的是 Node B，一面朝内连接核心网的是无线网络控制器（RNC，Radio Network Controller）。 核心网以及连接的 IP 网络没有什么变化。 4G 4G 网络，基站为 eNodeB，包含了原来 Node B 和 RNC 的功能，下行速度向百兆级别迈进。另外，核心网实现了控制面和数据面的分离。 在前面的核心网里面，有接待员 MSC 或者 SGSN，你会发现检查是否合法是它负责，转发数据也是它负责，也即控制面和数据面是合二为一的，这样灵活性比较差，因为控制面主要是指令，多是小包，往往需要高的及时性；数据面主要是流量，多是大包，往往需要吞吐量。 HSS 用于存储用户签约信息的数据库，其实就是你这个号码归属地是哪里的，以及一些认证信息。MME 是核心控制网元，是控制面的核心，当手机通过 eNodeB 连上的时候，MME 会根据 HSS 的信息，判断你是否合法。如果允许连上来，MME 不负责具体的数据的流量，而是 MME 会选择数据面的SGW 和 PGW，然后告诉 eNodeB，我允许你连上来了，你连接它们吧。于是手机直接通过 eNodeB 连接 SGW，连上核心网，SGW 相当于数据面的接待员，并通过 PGW 连到 IP 网络。PGW 就是出口网关。在出口网关，有一个组件 PCRF，称为策略和计费控制单元，用来控制上网策略和流量的计费。 4G网络协议解析 控制面协议 eNodeB 还是两面派，朝前对接无线网络，朝后对接核心网络，在控制面对接的是 MME。 eNodeB 和 MME 之间的连接就是很正常的 IP 网络，但是这里面在 IP 层之上，却既不是 TCP，也不是 UDP，而是 SCTP。这也是传输层的协议，也是面向连接的，但是更加适合移动网络。 它继承了 TCP 较为完善的拥塞控制并改进 TCP 的一些不足之处。 SCTP优势： 多宿主（Multi-homing） 多流（Multi-streaming） 初始化保护（Initiation protection） 消息分帧（Message framing） 可配置的无序发送（Configurable unordered delivery） 平滑关闭（Graceful shutdown） 参考链接使用 SCTP 优化网络 当 MME 通过认证鉴权，同意这个手机上网的时候，需要建立一个数据面的数据通路。建立通路的过程还是控制面的事情，因而使用的是控制面的协议 GTP-C。GTP-C是基于UDP的，GTP有序列号，所以不用 TCP，GTP-C 自己就可以实现可靠性，为每个输出信令消息分配一个依次递增的序列号，以确保信令消息的按序传递，并便于检测重复包。对于每个输出信令消息启动定时器，在定时器超时前未接收到响应消息则进行重发。 建设的数据通路分两段路，其实是两个隧道。 一段是从 eNodeB 到 SGW，这个数据通路由 MME 通过 S1-MME 协议告诉 eNodeB，它是隧道的一端，通过 S11 告诉 SGW，它是隧道的另一端。 第二端是从 SGW 到 PGW，SGW 通过 S11 协议知道自己是其中一端，并主动通过 S5 协议，告诉 PGW 它是隧道的另一端。 数据面协议当两个隧道都打通，接在一起的时候，PGW 会给手机分配一个 IP 地址，这个 IP 地址是隧道内部的 IP 地址，可以类比为 IPsec 协议里面的 IP 地址。这个 IP 地址是归手机运营商管理的。然后，手机可以使用这个 IP 地址，连接 eNodeB，从 eNodeB 经过 S1-U 协议，通过第一段隧道到达 SGW，再从 SGW 经过 S8 协议，通过第二段隧道到达 PGW，然后通过 PGW 连接到互联网。数据面的协议都是通过 GTP-U，如图所示。 手机每发出的一个包，都由 GTP-U 隧道协议封装起来，格式如下。 手机上网流程 手机开机以后，在附近寻找基站 eNodeB，找到后给 eNodeB 发送 Attach Request，说“我来啦，我要上网”。 eNodeB 将请求发给 MME，说“有个手机要上网”。 MME 去请求手机，一是认证，二是鉴权，还会请求 HSS 看看有没有钱，看看是在哪里上网。 当 MME 通过了手机的认证之后，开始分配隧道，先告诉 SGW，说要创建一个会话（Create Session）。在这里面，会给 SGW 分配一个隧道 ID t1，并且请求 SGW 给自己（MME）也分配一个隧道 ID。 SGW 转头向 PGW 请求建立一个会话，为 PGW 的控制面分配一个隧道 ID t2，也给 PGW 的数据面分配一个隧道 ID t3，并且请求 PGW 给自己（SGW）的控制面和数据面分配隧道 ID。 PGW 回复 SGW 说“创建会话成功”，使用自己的控制面隧道 ID t2，回复里面携带着给 SGW 控制面分配的隧道 ID t4 和控制面的隧道 ID t5，至此 SGW 和 PGW 直接的隧道建设完成。双方请求对方，都要带着对方给自己分配的隧道 ID，从而标志是这个手机的请求。 接下来 SGW 回复 MME 说“创建会话成功”，使用自己的隧道 ID t1 访问 MME，回复里面有给 MME 分配隧道 ID t6，也有 SGW 给 eNodeB 分配的隧道 ID t7。 当 MME 发现后面的隧道都建设成功之后，就告诉 eNodeB，“后面的隧道已经建设完毕，SGW 给你分配的隧道 ID 是 t7，你可以开始连上来了，但是你也要给 SGW 分配一个隧道 ID”。 eNodeB 告诉 MME 自己给 SGW 分配一个隧道，ID 为 t8。 MME 将 eNodeB 给 SGW 分配的隧道 ID t8 告知 SGW，从而前面的隧道也建设完毕 这样，手机就可以通过建立的隧道成功上网了。 为什么你的手机在国外上不了facebook为什么要分 SGW 和 PGW 呢，一个 GW 不可以吗？SGW 是你本地的运营商的设备，而 PGW 是你所属的运营商的设备。 如果你在巴塞罗那，一下飞机，手机开机，周围搜寻到的肯定是巴塞罗那的 eNodeB。通过 MME 去查寻国内运营商的 HSS，看你是否合法，是否还有钱。如果允许上网，你的手机和巴塞罗那的 SGW 会建立一个隧道，然后巴塞罗那的 SGW 和国内运营商的 PGW 建立一个隧道，然后通过国内运营商的 PGW 上网。 这样判断你是否能上网的在国内运营商的 HSS，控制你上网策略的是国内运营商的 PCRF，给手机分配的 IP 地址也是国内运营商的 PGW 负责的，给手机分配的 IP 地址也是国内运营商里统计的。运营商由于是在 PGW 里面统计的，这样你的上网流量全部通过国内运营商即可，只不过巴塞罗那运营商也要和国内运营商进行流量结算。由于你的上网策略是由国内运营商在 PCRF 中控制的，因而你还是上不了脸书。]]></content>
  </entry>
  <entry>
    <title><![CDATA[《图解TCP/IP》笔记]]></title>
    <url>%2F2019%2F09%2F19%2F%E3%80%8A%E5%9B%BE%E8%A7%A3TCP-IP%E3%80%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[中继器到底能否完成不同媒介之间的转接工作？《图解TCP/IP》中说道“一般，中继器两端连接的是相同的通信媒介，但有的中继器也可以完成不同媒介之间的转接工作。如，可以在同轴电缆与光缆之间调整信号。然而，这种情况下，中继器也只是单纯负责信号在0和1比特流之间的替换，并不负责判断数据是否有错误。同时，它只负责将电信号转换为光信号，因此不能在传输速度不同的媒介之间转发” 请问是否前后矛盾，不能理解 这句话有问题，所以不正确也正确。 比如光网络可以跑万兆以上 但是你的光猫转换器是1000M的电口，符合你这个说法，因为光和电 2个媒介是传输速度不同的媒介。 但是 如果你的光口进来的数据的速率是100M的 那么这时候你的电口是可以满足的 这时候你这句话就是错误的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[知识盲点]]></title>
    <url>%2F2019%2F09%2F11%2F%E7%9F%A5%E8%AF%86%E7%9B%B2%E7%82%B9%2F</url>
    <content type="text"><![CDATA[git pull&amp;git pull rebase 字符编码笔记：ASCII，Unicode 和 UTF-8 长连接相关TCP Keepalive机制与应用层心跳Heartbeat 如何使用 Linux 中的 TCP keepalive? 随手记之TCP Keepalive笔记 高效保活长连接：手把手教你实现 自适应的心跳保活机制 nginx的so_keepalive和timeout相关小计 NATNAT大致分为两大类：圆锥形和对称型 圆锥形又分为三种 Full Cone：计算机A链接公网计算机M后，NAT打开一个端口，以后公网上任何发送到这个端口的数据（不限于M）都可以访问到A，这个时候任何从公网上发过来的数据都可以通过该端口到达内网计算机A，不限制M、端口和IP Address Restricted Cone：内网计算机A通过路由器链接了外网计算机M，NAT打开一个端口，这个时候外网计算机M（只限于M）可以通过任何端口和内网计算A进行通信。限制了ip地址，没有限制端口。 Port Restricted Cone：内网计算机A通过路由器链接了外网计算机M，NAT打开一个端口，M可以通过这个端口跟A进行通信，这种即限制了ip地址又限制了端口。 Symmetric NAT（对称形） 对称型NAT和圆锥形不同的地方在于： 圆锥形NAT对于同一台内网计算机，无论与那一台的外网服务器通信，NAT所分配的端口不变。 对称型NAT对于同一台内网计算机与不同的外网计算机通讯会分配不同的端口号，对称型NAT一般不能用于p2p软件。 NAT穿透介绍 SNAT DNAT linux命令netstat ss 网络相关HTTP/2: the difference between HTTP/1.1, benefits and how to use it 正向代理和反向代理 你真的掌握lvs工作原理吗？ consul 架构 阻塞IO和非阻塞IO Java NIO浅析 原码 反码 补码https://cloud.tencent.com/developer/article/1353672 IaaS, Paas, Saas的区别 http://www.ruanyifeng.com/blog/2017/07/iaas-paas-saas.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[gc案例]]></title>
    <url>%2F2019%2F09%2F11%2Fgc%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[2019-09-11 频繁cms gc jvm参数 1-Xmx6g -Xms6g -XX:SurvivorRatio=8 -XX:NewRatio=2 -XX:PermSize=128m -XX:MaxPermSize=512m -XX:+DisableExplicitGC -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintCommandLineFlags -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:ParallelCMSThreads=4 -XX:+CMSClassUnloadingEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=1 -XX:CMSInitiatingOccupancyFraction=50 现象：老年代远远没有达到配置的50%阀值就开始进行了频繁cms gc 解决办法：增加jvm参数 -XX:+UseCMSInitiatingOccupancyOnly，加上后效果明显 如果不配置 -XX:+UseCMSInitiatingOccupancyOnly cms会动态决定什么时候执行cms gc，详情观看下文 参考链接 https://mp.weixin.qq.com/s/Mu-Xz4CLgdxJhcMJ7aKAHg]]></content>
  </entry>
  <entry>
    <title><![CDATA[《网络是怎么连接的》第二章笔记]]></title>
    <url>%2F2019%2F09%2F10%2F%E3%80%8A%E7%BD%91%E7%BB%9C%E6%98%AF%E6%80%8E%E4%B9%88%E8%BF%9E%E6%8E%A5%E7%9A%84%E3%80%8B%E7%AC%AC%E4%BA%8C%E7%AB%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[用电信号传输TCP/IP数据—–探索协议栈和网卡创建套接字 浏览器、邮件等应用程序收发数据时一般使用TCP;DNS查询等收发较短的控制数据时使用UDP; 套接字是什么？协议栈内部有一块用于存放控制信息的内存空间，这里记录了用于控制通信操作的控制信息，例如通信对象的IP地址、端口号、通信操作的进行状态等。本来套接字只是一个概念而已，并不存在实体，如果一定要赋予它一个实体，我们可以说这些控制信息就是套接字的实体，或者说存放控制信息的内存空间就是套接字的实体。协议栈在执行操作时需要参阅这些控制信息。例如发送数据时需要看一看套接字中的通信对象的IP地址和端口号。发送数据后 套接字中必须要记录是否已经收到响应，以及发送数据后经过多长时间，才能根据这些信息按照需要进行重发操作。 协议栈是根据套接字中记录的控制信息来工作的。 可以在操作系统里输入netstat命令来显示套接字内容。每一行相当于一个套接字，当创建套接字时，就会在这里增加一行新的控制信息。 调用socket时的操作看过套接字的样子后，我们看一看当浏览器调用socket、connect等Socket库中的程序组件时，协议栈内部时如何工作的。 创建套接字的阶段 如图2.3(1)所示，应用程序调用socket申请创建套接字，协议栈根据应用程序的申请执行创建套接字的操作。在这个过程中协议栈首先会分配用于存放一个套接字所需的内存空间，相当于为控制信息准备一个容器。但是光有容器并没有什么用，还需要往里边存入控制信息。套接字刚刚创建时，数据收发操作还没有开始，因此需要在套接字的内存空间写入表示这一初始状态的控制信息。 接下来，需要将这个套接字的描述符告知应用程序。描述符相当于用来区分协议栈中的多个套接字的号码牌。 收到描述符之后，应用程序在向协议栈进行收发数据委托时就需要提供这个描述符。由于套接字中记录了通信双方的信息以及通信处于怎样的状态，所以只要通过描述符确定了相应的套接字，协议栈就能获取所有相关信息，这样一来，应用程序就不需要每次都告诉协议栈应该和谁进行通信了。 连接服务器连接实际上是通信双方交换控制信息 2.3(1)步骤套接字刚刚创建完成的时候，套接字里写的只是初始数据，并不知道通信的对方是谁，在这个状态下，即便是应用程序要求发送数据，协议栈也不知道数据该发给谁。因为调用socket来创建套接字时，这些信息并没有传递给协议栈。因此我们在connect时需要把服务器的IP地址和端口号等信息告诉协议帧，这是连接操作的目的之一。 服务器那边又是怎样的情况呢？服务器上也会创建套接字，服务器上的协议栈和客户端上的一样，只创建套接字是不知道和谁进行通信的。而且和客户端不同的是，在服务器上连应用程序也不知道通信的对象是谁，于是需要客户端告知服务器必要的信息，这也是连接操作的目的之一。 双方交换的的控制信息，除了IP和端口，还有其他。 此外，当执行数据收发操作的时候，我们还需要一块用于临时存放要收发的数据的内存空间，这块内存空间称为缓冲区，它也是在连接操作的过程中分配的。 通信操作中使用的控制信息分为两类。 （1）头部中记录的信息 （2）套接字（协议栈中的内存空间）中记录的信息 连接操作的实际过程这个过程是从应用程序调用Socket库的connect开始的(图2.3(2))。 connect(&lt;描述符&gt;,&lt;服务器IP地址和端口号&gt;) 上面的提供的服务器IP地址和端口号，会传递给协议栈中的TCP模块。然后TCP模块会与该IP地址对应的对象(服务器TCP模块)交换控制信息，这一交互过程包括下面几个步骤。 客户端先创建一个包含表示开始数据收发操作的控制信息的头部。重点关注的是发送方和接收方的端口号，可以通过端口号准确的找到服务器的套接字。然后将头部控制位的SYN比特值设置为1 当TCP头创建好了后，TCP模块把信息传递给IP模块并委托他进行发送。IP模块执行发送网络包操作，网络包到达服务器，服务器IP模块会把收到的数据传递给TCP模块，服务器根据头部的端口号找到对应的套接字，找到后写入相应的信息，并将状态改为正在连接。返回响应，SYN、ACK比特位设置为1 然后网络包到达客户端，通过IP模块到达TCP模块，并通过TCP头部的信息确认连接服务器的操作是否成功。如果SYN为1表示连接成功，这时会向套接字中写入服务器的IP地址、端口号等信息，同时将状态改为连接完毕。客户端返回ACK，ACK比特设置为1 收发数据协议栈并不关心应用程序传来的数据是什么内容。应用程序在调用write时会指定发送数据的长度，在协议栈看来，要发送的数据就是一定长度的二进制字节码而已。 协议栈并不是一收到数据就会马上发出去，而是将数据放在内部的发送缓冲区中，并等待应用程序的下一段数据，因此需要在数据累计到一定的量在发出去，这样做是为了防止网络效率下降 关于协议栈发送数据的时机有两个要素决定 第一个判断要素是每个网络包能容纳的数据长度，协议栈会根据一个叫做MTU的参数来进行判断。MTU表示一个网络包的最大长度，在以太网中一般是1500字节。MTU是包含头部的总长度，因此需要减去头部，TCP和IP头部加起来一共40字节，所以MTU减去这个长度就是一个网络包中能容纳的最大数据长度，这一长度叫做MSS。当协议栈收到的数据长度超过或者接近MSS时再发出去，就可以避免发送大量小包的问题了。 另一个判断要素是时间。当应用程序发送数据频率不高的时候，如果每次都等到接近MSS时在发送，可能会因为等待时间太长而造成发送延迟，这种情况下即便缓冲区中的数据长度没有达到MSS，也应该果断发送出去。为此协议栈内部有一个计时器，当经过一定时间后，就把网络包发送出去。 这两个要素其实是相互矛盾的。长度优先，效率会提高，但是会产生延迟；想反时间优先，延迟降低，网络效率降低。TCP协议规格中并没有告诉我们怎么才能平衡，因此如果判断是由协议栈的开发者来决定的，正是这个原因，不同种类和版本的操作系统在相关操作上也就存在差异。仅靠协议栈来判断发送时机也会带来问题，因此协议栈给应用程序保留了控制发送时机的余地，应用程序可以指定一些选项。 对较大的数据进行拆分：当HTTP消息长度超过一个网络包所能容纳的数据量时，就需要进行拆分了，如图所示。 使用ACK号确认网络包已收到：TCP模块在拆分数据时，会先算好每一块数据相当于从头开始的第几个字节，这个值就是“序号”，然后长度也需要告知接收方，不过这个不是放在TCP头部中，而是用数据包长度减去头部的长度就可以得到数据的长度。接收方回复的ACK号代表，到第XX字节前的数据我都已经收到了，所以已收到多少字节加1作为ACK号。实际通信中序号并不是从1开始的，而是需要随机计算出一个初始值，防止能预测序号发动攻击，如果序号是随机的，为了让对方清楚初始值，所以在建立连接的过程中，需要把序号告诉对方。 通过“序号”和“ACK号”可以确认接收方是否收到了网络包。TCP采用这样的方式确认对方是否收到数据，在得到对方确认之前，发送过的包都会保存在发送缓冲区，如果没有返回某些包对应的ACK号，那么就重新发送这些包。这一机制非常强大，有了这一机制，我们就不需要在其他地方对错误进行补救了。因此网卡、集线器、路由器都没有错误补偿机制，一旦检测到错误就直接丢弃相应的包。应用程序和IP模块也一样只管自顾自的发送数据就好了。 根据网络包平均往返时间调整ACK号等待的时间，等待时间需要设置一个合适的值，不能太长也不能太短，太长会延迟，太短会增加重传对于拥堵的网络无疑是雪上加霜，因此，TCP采用了动态调整等待时间的做法，这个等待时间是根据ACK号返回所需要的时间来判断的。具体来说，TCP会在发送数据的过程中持续测量ACK号的返回时间，如果ACK号返回变慢，则相应延长等待时间；相对地，如果ACK号马上就能返回，则相应缩短等待时间。 每发送一个包就等待一个ACK号的方式是最简单的也是最容易理解的，但是在等ACK号的这段时间内，如果什么也不做就太浪费了。为了减少这样的浪费，TCP采用滑动窗口方式来管理数据发送和ACK号的操作。所谓滑动窗口，就是在发送完一个包之后，不等ACK号返回，而是直接发送后续一系列的包。如图所示 但是滑动窗口的方式发送的数据接收方处理不过来怎么办呢？ 接收方的TCP模块收到包后，会先将数据存放到接收缓存区中。接收方需要计算ACK号，将数据块组装起来还原成原本的数据并传递给应用程序，如果数据到达的速率比处理这些数据并传递给应用程序的速率还快，那么缓存区中的数据就会越堆越多，最后就会溢出。 怎么解决呢？ 首先接收方要告诉发送方自己最多能接收多少数据，然后发送方根据这个值对数据发送操作进行控制，这就是滑动窗口方式的基本思路。 ACK与窗口的合并 要想提高发送数据的效率，还要考虑另一个问题，就是返回ACK号和更新窗口时机。 当收到的数据刚刚填入缓冲区时，其实没必要每次都像发送方更新窗口大小，发送方可以自己算出来。 更新窗口的时机是，接收方从缓冲区中取出数据传递给应用程序的时候。这时缓冲区容量增加，发送方是不知道的，所以需要告诉发送方。 ACK号的发送时机是，接收方收到数据时，确认内容没有问题，就会向发送方返回ACK号，因此我们可以认为收到数据之后马上就进行这一操作。 如果将前面两个因素结合起来看，每收到一个包接收方需要发送两个包，数据到达接收方发ACK包，数据传递给应用程序需要通知发送方更新窗口大小。这样一来，导致网络效率下降。 因此，接收方发送ACK号和窗口更新时，并不会立马发送，而是等待一段时间，这个过程可能会出现其他通知操作，这样就可以把多个通知合并在一个包里发送了。 举个大例子，在等待ACK号的时候正好需要更新窗口，这样就可以把ACK和窗口更新放在一个包里发送了。 当需要连续发送多个ACK号时，也可以减少包的数量，这是因为ACK号表示的是已经收到的数据量，也就是说，它是告诉发送方目前已接收的数据的最后的位置在哪里，因此当需要发送多个ACK号时，只要发送最后一个ACK就行了，中间的可以省略。 同样当需要连续发送多个窗口更新时也可以减少包的数量，这种情况和ACK号一样，可以省略中间过程，只要发送最终结果就可以了。 协议栈接收数据的操作过程 首先，协议栈会检查收到的数据块和TCP头部的内容，判断数据是否有丢失，如果没有问题返回ACK号。 然后，协议栈将数据块暂存到接收缓冲区中，并将数据块按顺序连接起来还原出原始数据 最后，将数据交给应用程序，具体来说，协议栈会将接收到的数据复制到应用程序指定的内存地址中，然后将控制流程交给应用程序。将数据交给应用程序后，协议栈还需要找到合适的时机向发送方发送窗口更新。 从服务器断开并删除套接字客户端和服务器都可以发起断开过程，这里我们以服务器一方发起断开过程为例进行讲解。 断开过程 首先服务器一方的应用程序会调用Socket库的close程序。然后，服务器的协议栈生成包含断开信息的TCP头部，就是将控制位中FIN比特设置为1。接下来协议栈委托IP模块向客户端发送数据。同时服务器套接字中也会记录下断开操作的相关信息。 当客户端收到服务器发来的FIN为1的TCP头部时，客户端的协议栈会将自己的套接字标记进入断开操作状态。然后告知服务器已经收到FIN为1的包，客户端会向服务器返回一个ACK号。这些操作完成后，协议栈就等着应用程序来取数据了。过了一会儿，应用程序就会调用read来读取数据。这时，协议栈不会向应用程序传递数据，而是告知应用程序来自服务器的数据已经全部收到了。 根据规则，服务器返回请求之后，web通信操作就全部结束了，因此只要收到服务器返回的所有数据，客户端的操作也就随之结束了。因此客户端应用程序会调用close来结束数据收发操作，这时客户端的协议栈也会和服务器一样，生成一个FIN比特为1的TCP包，然后委托IP模块发送给服务器。 一段时间后服务器就会返回ACK号，到这里，客户端和服务器的通信就全部结束了。 删除套接字 通信结束后，按理说用来通信的套接字不会再用了，这时我们就可以删除这个套接字了。不过实际上不会立马删除，而是等待一段时间然后删除。 等待这段时间是为了防止误操作，引发误操作的原因有好多，这里举一个最容易理解的例子。 假设客户端先发起断开，则断开的操作顺序如下。 （1）客户端发送FIN （2）服务器返回ACK号 （3）服务器发送FIN （4）客户端返回ACK号 假设最后客户端返回的ACK号丢了，这时服务器没有收到ACK号，可能会重发一次FIN。如果这时客户端的套接字被删了，那套接字保存的控制信息也就跟着消失了，套接字对应的端口就会被释放出来。这时如果别的应用程序要创建套接字，新套接字碰巧又被分配给了同一个接口，而服务器重发的FIN刚好到达，于是这个FIN就会错误的跑到新套接字里面，新套接字就开始执行断开操作了。之所以不马上删除套接字，就是为了防止这样的误操作。 那么应该等待多长时间呢，这和包重传的操作方式有关。网络包丢失之后会进行重传，这个操作通常要持续几分钟。如果重传了几分钟之后依然无效，则停止重传。但在这段时间内，网络中可能存在重传的包，因此需要等待到重传完全结束。协议中对于这个等待时间没有明确规定，一般来说会等待几分钟之后在删除套接字。 数据收发操作小结 IP与以太网的包收发操作包的基本结构 发送方的网路设备会负责创建包，接下来，包会被发往最近的网络转发设备。当到达最近的网络转发设备之后，转发设备会根据头部中的信息判断接下来应该发往哪里。这个过程需要一张表，这张表里记录了每一个地址对应的发送方向，也就是按照头部里记录的目的地址在表里进行查询，并根据查到的信息判断接下来发往哪个方向。接下来，包在向目的地移动的过程中，又会到达下一个转发设备。这样经过多个转发设备的接力以后，包就到达最终的设备。 （1）路由器根据目标地址判断下一个路由器的位置。 （2）集线器在子网中将网络包传输到下一个路由。 实际上，集线器是按照以太网规则传输包的设备，而路由器是按照IP规则传输包的设备，因此我们可以作如下理解。 （1）IP协议根据目标地址判断下一个IP转发设备的位置。 （2）子网中的以太网协议将包传输到下一个转发设备 TCP/IP包包含如下两个头部。 （a）MAC头部（用于以太网协议） （b）IP头部（用于IP协议） 传输过程中ip头目标ip地址不变，mac头目标地址不断变化位下一个路由器的mac地址，更准确的说，收到包的时候MAC头会被舍弃，而当再次发送的时候又会加上包含新MAC地址的新MAC头部。如图所示 包收发概览包收发操作的起点是TCP模块委托IP模块发送包的操作（如下图所示）。这个委托的过程就是TCP模块在数据块的前面加上TCP头部，然后整个传递给IP模块，这部分就是网络包的内容。收到委托后IP模块负责添加MAC头部和IP头部，接下来，封装好的包会被交给网卡。传递给网卡的网络包是由一连串0和1组成的数字信息，网卡将这些数字信息转换为电信号或光信号，并通过网线发送出去，然后这些信号就会到达集线器、路由器等转发设备，再由转发设备一步一步地送达接收方。 生成IP头部IP模块接收TCP模块的委托负责包的收发操作，它会生成IP头部并附加在TCP头部前。最重要的是接收方IP地址，这个地址是TCP模块告知的，而TCP又是在执行连接操作时从应用程序那里获得的。发送方IP地址需要判断发送所使用的网卡，并填写该网卡的IP地址。 生成以太网用的MAC头部IP头部中的接收方IP地址表示网络包的目的地，通过这个地址我们就可以判断将包发送到哪里，但在以太网的世界里，TCP/IP这个思路是行不通的。以太网判断网络包的目的地时和TCP/IP的方式不同，因此必须采用相匹配的方式才能在以太网中将包发往目的地，而MAC头部就是干这个用的。 IP模块根据路由表Gateway栏的内容判断应该把包发给谁，解释看书内章节2.5.3 通过ARP查询目标路由器的MAC地址在以太网中，有一种叫作广播的方法，可以把包发给连接在同一以太网的所有设备。ARP就是利用了广播。 为了防止每次发送包都用ARP查询一次，网络中就会增加很多ARP包，因此我们会将查询结果放到一块叫做ARP缓存的内存空间中留着以后用。在发送包时，先查下缓存，虽然能减少ARP包的数量，但是如果IP地址发生变化时，ARP缓存的内容就会和现实产生差异。为了防止这种问题的发生，ARP缓存中的值过一段时间就会被删除。 UDP 以太网基本知识（建议看原文2.5.6-2.5.11）]]></content>
  </entry>
  <entry>
    <title><![CDATA[《网络是怎么连接的》第一章笔记]]></title>
    <url>%2F2019%2F08%2F30%2F%E3%80%8A%E7%BD%91%E7%BB%9C%E6%98%AF%E6%80%8E%E4%B9%88%E8%BF%9E%E6%8E%A5%E7%9A%84%E3%80%8B%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[浏览器生成消息生成HTTP请求 一条请求消息中只能写一个URI。如果需要获取多个文件，必须对每个文件单独发送1条请求。向DNS服务器查询Web服务器的IP地址域名和IP地址并用的理由问：为啥不直接使用IP地址，而是要用域名 答：这样其实也是能正常工作的，想想日常上网，你查某个网站肯定是输入域名比如baidu,tianmao之类的，你肯定记不住一长串的IP地址 问：那干脆不用IP地址，用域名来确定通信对象不好吗 答：应该也能实现，但是从运行效率上来看，这并不能算是一个好主意。互联网中存在无数的路由器，他们之间相互配合，根据IP地址来判断应该把数据传送到什么地方。IP地址的长度为32bit，也就是4个字节，相对的域名最短也要几十个字节，增加了路由器的负担。 DNS解析器查询DNS其实是想DNS服务器发送根据域名查询IP的请求，对于DNS服务器我们计算机上的有DNS客户端，相当于DNS客户端的部分我们称为DNS解析器 解析器其实就是一段程序，它包含在操作系统的Socket库中，Socket库是用于调用网络功能的程序组件集合。 根据域名查询IP地址时，浏览器会使用Socket库中的解析器。 解析器内部原理图 向DNS服务器发送消息时，我们当然需要知道DNS服务器的IP地址。这个是事先设置好的。如图window系统 全世界DNS服务器的大接力DNS查询消息包含3种消息 域名：服务器、邮件服务器 Class：最早设计DNS方案时，DNS在互联网以外的其他网络中的应用也被考虑到了，而Class就是用来识别网络的信息。不过如今除了互联网没有其他网络了，因此Class的值永远代表互联网的IN 记录类型：表示域名对应何种类型的记录。例如当类型为A时，表示域名对应的是IP地址;当类型为MX时，表示域名对应的是邮件服务器。不同的记录类型，服务器返回给客户端的信息也会不同。 DNS服务器会从域名与IP地址的对照表中查找相应的记录，并返回IP地址 域名的层次结构问题1： 互联网中存在不计其数的IP地址，将这些IP地址保存到一台DNS服务器是不可能的，因此肯定会存在在DNS服务器中找不到查询信息的情况。 解决方案1： DNS服务器按照域名以分层次的结构来保存到多个DNS服务器上。 比如域名www.lab.glasscom.com,这里的点号代表了不同层级之间的界限。在域名中**越靠右的位置表示层级越高**，其中，每一层级的部分称为域。因此com域的下一层是glasscom域,在下一层是lab域,在下面才是www这个名字。 负责下级域的DNS服务器的IP地址注册到他们上级DNS服务器中，然后上级在注册到更上级的DNS服务器中，以此类推。 根域：似乎com、net这些域就是最顶层了，他们各自负责保存下级DNS服务器的信息，但实际上他们上层还有一级域，称为根域，可以认为www.lab.glasscom.com. 最后那个.代表根域但是一般不写最后的点，因此根域往往被忽略，根域的DNS服务器保存着com、net等DNS服务器的信息。所以我们从根域开始一路顺藤摸瓜找到任意域的DNS服务器，除此之外还需要完成另一项工作，那就是将根域的DNS服务器信息保存在互联网中所有的DNS服务器中。这样一来客户端找到任意一台DNS服务器，就可以通过他找到根域DNS服务器，然后在顺藤摸瓜找到位于下层的目标服务器。 委托协议栈发送消息收发数据操作之前双方需要建立管道。建立管道的关键在于管道两端的数据入口，这些入口称为套接字。 我们需要先创建套接字，然后将套接字连接起来形成管道。 创建套接字阶段 &lt;描述符&gt; = socket(&lt;使用IPV4&gt;,&lt;流模式&gt;, ….); 创建套接字调用Socket库中的socket程序组件就可以了，套接字创建完成后，协议栈会返回一个描述符，应用程序会将收到的描述符放在内存中。描述符是用来识别不同的套接字的。应用程序是通过“描述符”这一类号码牌的东西来识别套接字的 连接阶段:把管道接上去 connect(&lt;描述符&gt;,&lt;服务器的IP地址和端口号&gt;,…); 关于端口号 可能大家还有疑问，既然确定连接对象的套接字需要使用端口号，那么服务器也得知道客户端套接字的端口号才行吧？这个问题怎么解决的呢？首先客户端在创建套接字时，协议栈会为这个套接字随便分配一个端口号。接下来协议栈执行连接操作时，会将这个随便分配的端口号通知给服务器。总之当连接成功后，协议栈会将对方的IP地址和端口号等信息保存在套接字中，这样我们可以开始收发数据了 描述符：应用程序用来识别套接字的机制 IP地址和端口号：客户端和服务器之间用来识别对方套接字的机制 通信阶段：传递消息 断开连接：收发数据结束 本笔记中出现Socket、socket、套接字(英文也是socket)等看起来非常容易混淆的词，其中小写的socket表示程序组件的名称，大写字母开头的Socket表示库，而汉字“套接字”则表示管道两端的接口。]]></content>
      <tags>
        <tag>网络是怎么连接的</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《趣谈网络协议》第10-13讲笔记]]></title>
    <url>%2F2019%2F08%2F19%2F%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E7%AC%AC10-13%E8%AE%B2%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[TCP和UDP的区别TCP面向连接，UDP无连接，所谓的建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，用这样的数据结构来保证所谓的面向连接的特性。 TCP提供可靠交付，通过 TCP 连接传输的数据，无差错、不丢失、不重复、并且按序到达。UDP不保证不丢失，不保证顺序 TCP 是面向字节流的。发送的时候发的是一个流，没头没尾。IP 包可不是一个流，而是一个个的 IP 包。之所以变成了流，这也是 TCP 自己的状态维护做的事情。而UDP 继承了 IP 的特性，基于数据报的，一个一个地发，一个一个地收。 TCP是有拥塞控制的。当它意识到包丢弃了或者网络不好，就会根据情况调整自己的行为，决定是不是要发慢点。 UDP就不会，应用让我发我就发。 TCP是一个有状态的服务，里面精确的精确的记着发送了没有，接收到没有，发送到哪个了，应该接收哪个了，错一点就不行。而UDP是无状态的服务。 我们可以这样比喻，如果 MAC 层定义了本地局域网的传输行为，IP 层定义了整个网络端到端的传输行为，这两层基本定义了这样的基因：网络传输是以包为单位的，二层叫帧，网络层叫包，传输层叫段。我们笼统地称为包。包单独传输，自行选路，在不同的设备封装解封装，不保证到达。基于这个基因，生下来的孩子 UDP 完全继承了这些特性，几乎没有自己的思想。 UDP包头发送的时候，我知道我发的是一个 UDP 的包，收到的那台机器咋知道的呢？所以在 IP 头里面有个 8 位协议，这里会存放，数据里面到底是 TCP 还是 UDP，当然这里是 UDP。 当我们看到 UDP 包头的时候，发现的确有端口号，有源端口号和目标端口号。因为是两端通信嘛，这很好理解。但是你还会发现，UDP 除了端口号，再没有其他的了。和下两节要讲的 TCP 头比起来，这个简直简单得一塌糊涂啊！ UDP三大适用场景 需要资源少，网络情况比较好的内网，或者丢包不敏感的应用。 不需要一对一沟通，建立连接，而是可以广播的应用。 需要处理速度快，时延低，可以容忍少量丢包，但是要求即使网络拥塞也毫不退缩，一往无前的时候。 TCP包头 序号是解决乱序的问题，哪个包先发哪个包后发 确认序号是解决不丢包的问题，发出去的包得有确认，没有确认得重新发送直到送达。 状态位SYN表示发起一个连接，ACK是回复，RST是重新连接，FIN是结束连接。 窗口大小TCP要做流量控制，通信双方各声明一个窗口，标示当前自己能处理的能力，别发送的太快，也别发送的太慢 总结下TCP 顺序问题，稳重不乱 丢包问题，承诺靠谱 连接维护，有始有终 流量控制，把握分寸 拥塞控制，知进知退 TCP三次握手 只要保证双方的包有去有回就行 三次握手除了建立连接外，主要还是为了沟通一件事，就是TCP包的序号的问题。 TCP序号的问题序号不能都从1开始，因为A发1/2/3，三个包给B，中途B丢了，这时A掉线了重新连上B，如果序号又从1开始，发送1/2，这时之前丢的3绕回来了，B就认为这个3是下一个包，于是发生了错误。 因此每个连接都有不同的序号这个起始序号是随着时间变化的可以看成一个32位的计数器 TCP四次挥手为什么建立连接是三次握手，关闭连接确是四次挥手呢？ 建立连接的时候， 服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。举个例子：A 和 B 打电话，通话即将结束后，A 说“我没啥要说的了”，B回答“我知道了”，但是 B 可能还会有要说的话，A 不能要求 B 跟着自己的节奏结束通话，于是 B 可能又巴拉巴拉说了一通，最后 B 说“我说完了”，A 回答“知道了”，这样通话才算结束。 TCP的三次握手与四次挥手（详解+动图） TCP状态机 在这个图中，加黑加粗的部分，是上面说到的主要流程，其中阿拉伯数字的序号，是连接过程中的顺序，而大写中文数字的序号，是连接断开过程中的顺序。加粗的实线是客户端 A 的状态变迁，加粗的虚线是服务端 B 的状态变迁。 TCP缓存发送端的缓存第一部分：发送并已经确认的 第二部分：发送还并且尚未确认 第三部分：没有发送，但是已经等待发送 第四部分：没有发送，并且暂时不会发送 TCP里接收端会给发送端报一个窗口大小，叫做AdvertisedWindow,窗口大小=第二部分+第三部分 接收端的缓存第一部分：接收已确认,之后是已经接收了，但是还没被应用层读取的 第二部分：等待接收，也就是能承受的最大工作量。也就是AdvertisedWindow 第三部分：还没接收，无法接受，因为超过了最大承受量 其中第二部分里面，由于收到的包可能不是顺序的，会出现空挡，只有和第一部分连续的，可以马上进行回复，中间空着的部分需要等待，哪怕后面的已经来了。 顺序与丢包问题分析刚才接受端和发送端的图 状态如下： 1、2、3没有问题达成一致 4、5接收方说ACK了，但是发送方还没有收到，有可能丢了，有可能在路上 6、7、8、9肯定都发了，但是8、9已经到了，但是6、7没到出现乱序，没办法ACK 根据这个例子，知道丢包和顺序问题都可能发生。 假设4的ACK收到了，5的ACK丢了，6、7的数据包丢了，咋办呢 超时重试，那超时时间如何控制呢。需要自适应重传算法（Adaptive+Retransmission+Algorithm）[^1]，如果过一段时间5、6、7都超时了，发送端会重发5、6、7的包，接收端发现5收到过就会丢弃5,不回ACK[^2]，6收到回ACK，不幸的是7又丢了，当再次超时的时候，TCP的策略是超时间隔加倍，每当遇到一次超时重传的时候，都会将下一次超时时间设置为先前两倍。两次超时说明网络环境差，不宜频繁反复发送,超时重传的问题是超时周期可能相对较长，是不是有更快的方式，有一个可以快速重传的机制，当接收方收到一个序号大于下一个所期望的报文段时，就检测到了数据流中的一个间格，于是发送三个冗余的 ACK，客户端收到后，就在定时器过期之前，重传丢失的报文段。例如，接收方发现 6、8、9 都已经接收了，就是 7 没来，那肯定是丢了，于是发送三个 6 的 ACK，要求下一个是 7。客户端收到 3 个，就会发现 7 的确又丢了，不等超时，马上重发。 Selective Acknowledgment(SAK) 这种方式需要在 TCP 头里加一个 SACK 的东西，可以将缓存的地图发送给发送方。例如可以发送 ACK6、SACK8、SACK9，有了地图，发送方一下子就能看出来是 7 丢了。 流量控制对于包的确认中，同时会携带一个窗口大小。我们假设窗口不变，始终为9，4的ACK来的时候会右移一个，这个时候第13个包也可以发送了 这个时候如果发送端发送过猛，会将10、11、12、13都发送了，之后停止发送，因为未发送可发送部分为0。 这时候又收到了5的ACK，窗口又会往右滑动一格，这时才可以有更多包可以发送 如果接收方处理的太慢，导致缓存中没有空间，可以通过确认信息修改窗口大小，甚至可以设置为0，让发送方定制发送。 我们假设一个极端情况，接收端的应用一直不读取缓存里的数据，当数据包6确认后，窗口大小不能在是9了，就要缩小一个变成8。 这个新窗口8通过6的确认消息，到达发送端的时候，你会发现发送端窗口没有右移，而只是左边的移动了，窗口大小由9变成8 如果接收端的应用还是一直不处理数据，则随着确认的包越来越多，窗口越来越小，甚至变为0 当窗口大小通过14包的ACK到达发送端时，发送端的窗口也变为0。 如果这样的话，发送方会定时发送窗口探测数据包，看是否有机会调整窗口的大小。当接收方比较慢的时候，要防止低能窗口综合征，别空出一个字节来就赶快告诉发送方，然后马上又填满了，可以当窗口太小的时候，不更新窗口，直到达到一定大小，或者缓冲区一半为空，才更新窗口。 拥塞控制解决什么问题最后，我们看一下拥塞控制的问题，也是通过窗口的大小来控制的，前面的滑动窗口 rwnd 是怕发送方把接收方缓存塞满，而拥塞窗口 cwnd，是怕把网络塞满。 这里有一个公式 LastByteSent - LastByteAcked &lt;= min {cwnd, rwnd} ，是拥塞窗口和滑动窗口共同控制发送的速度。 那发送方怎么判断网络是不是满呢？这其实是个挺难的事情，因为对于 TCP 协议来讲，他压根不知道整个网络路径都会经历什么，对他来讲就是一个黑盒。 假设tcp发送的速度超过了带宽，中间设备处理不了的包就会被丢弃， 为了解决这个问题中间经过的设备会会加上缓存，处理不过来的在队列里排着，虽然避免了丢包，但是造成了超时重传 于是TCP的拥塞控制住要来解决这两种现象，包丢失和超时重传，一旦出现了这些现象就说明太快了，要慢一点。TCP 的拥塞控制就是在不堵塞，不丢包的情况下，尽量发挥带宽 如何解决如果我们通过漏斗往瓶子里灌水，我们就知道，不能一桶水一下子倒进去，肯定会溅出来，要一开始慢慢的倒，然后发现总能够倒进去，就可以越倒越快。这叫作慢启动。 一条 TCP 连接开始，cwnd 设置为一个报文段，一次只能发送一个；当收到这一个确认的时候，cwnd 加一，于是一次能够发送两个；当这两个的确认到来的时候，每个确认 cwnd 加一，两个确认 cwnd 加二，于是一次能够发送四个；当这四个的确认到来的时候，每个确认 cwnd 加一，四个确认 cwnd 加四，于是一次能够发送八个。可以看出这是指数增长，增长到啥时候是个头呢。 涨到什么时候是个头呢？有一个值 ssthresh 为 65535 个字节，当超过这个值的时候，就要小心一点了，不能倒这么快了，可能快满了，再慢下来。 每收到一个确认后，cwnd 增加 1/cwnd，我们接着上面的过程来，一次发送八个，当八个确认到来的时候，每个确认增加 1/8，八个确认一共 cwnd 增加 1，于是一次能够发送九个，变成了线性增长。 但是线性增长还是增长，还是越来越多，直到有一天，水满则溢，出现了拥塞，这时候一般就会一下子降低倒水的速度，等待溢出的水慢慢渗下去。 拥塞的一种表现形式是丢包，需要超时重传，这个时候，将 sshresh 设为 cwnd/2，将 cwnd 设为 1，重新开始慢启动。这真是一旦超时重传，马上回到解放前。但是这种方式太激进了，将一个高速的传输速度一下子停了下来，会造成网络卡顿。 前面讲过快速重传算法。当接收端发现丢了一个中间包的时候，发送三次前一个包的 ACK，于是发送端就会快速的重传，不必等待超时再重传。TCP 认为这种情况不严重，因为大部分没丢，只丢了一小部分，cwnd 减半为 cwnd/2，然后 sshthresh = cwnd，当三个包返回的时候，cwnd = sshthresh + 3，也就是没有一夜回到解放前，而是还在比较高的值，呈线性增长。 有什么问题第一个问题是是丢包并不代表着通道满了，也可能是管子本来就漏水。例如公网上带宽不满也会丢包，这个时候就认为拥塞了，退缩了，其实是不对的。 第二个问题是 TCP 的拥塞控制要等到将中间设备都填充满了，才发生丢包，从而降低速度，这时候已经晚了。其实 TCP 只要填满管道就可以了，不应该接着填，直到连缓存也填满。 为了优化这两个问题出来了TCP拥塞 BBR算法。它企图找到一个平衡点，就是通过不断的加快发送速度，将管道填满，但是不要填满中间设备的缓存，因为这样时延会增加，在这个平衡点可以很好的达到高带宽和低时延的平衡。 基于TCP协议Socket函数调用过程 服务端调用accept函数，拿出一个已经完成的连接进行处理。如果没有完成就需要等待，在服务器等待的时候，客户端可以通过connect函数发起连接。先在参数中指明要连接的IP地址和端口号，然后开始发起三次握手。内核会给客户端分配一个临时端口。一旦握手成功，服务端的accept就会返回另一个Socket。 这是经常考的知识点，就是监听的Socket和真正用来传数据的Socket是两个，一个叫做监听Socket，一个叫做已连接Socket。 基于UDP协议Socket函数调用过程 对于 UDP 来讲，过程有些不一样。UDP 是没有连接的，所以不需要三次握手，也就不需要调用 listen 和 connect，但是，UDP 的的交互仍然需要 IP 和端口号，因而也需要 bind。UDP 是没有维护连接状态的，因而不需要每对连接建立一组 Socket，而是只要有一个 Socket，就能够和多个客户端通信。也正是因为没有连接状态，每次通信的时候，都调用 sendto 和 recvfrom，都可以传入 IP 地址和端口。 [^1]:估计往返时间，需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个值，而且这个值还是要不断变化的，因为网络状况不断的变化。除了采样 RTT，还要采样 RTT 的波动范围，计算出一个估计的超时时间。由于重传时间是不断变化的，我们称为[^2]:不会有问题。接收方的 ACK 应答，不是和发送方的数据包一一对应的，也不是一个个回复。当某个包做了 ACK 应答，表示它之前的所有包都收到了。所以这里的例子里，5 虽然没有 ACK 应答，但是之后收到的 6、7、8，只需要 ACK 应答 8 就表示之前的包都收到并确认了。]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《趣谈网络协议》第3-7讲笔记]]></title>
    <url>%2F2019%2F08%2F14%2F%E3%80%8A%E8%B6%A3%E8%B0%88%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E3%80%8B%E7%AC%AC3-7%E8%AE%B2%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[ifconfig 和 ip addr的区别这是有关net-tools 和 iproute2的故事，后续回答这个问题 IP是什么ip地址是一个网卡在网络世界的通信地址，相当于我们现实世界的地址。 IPv6出现原因这个地址被分为四个部分，每个部分8个bit，所以一共32位，所以IP地址的数量很快就不够用了。当时设计IP地址时，哪里知道会有这么多计算机，因为不够用所以，就有了IPv6，这个有128位。 IP分类本来 32 位的 IP 地址就不够，还被分成了 5 类。现在想想，当时分配地址的时候，真是太奢侈了。 A/B/C类分网络号和主机号两部分 IP A/B/C 类包含的主机数量 类别 IP地址范围 最大主机数 私有IP地址范围 A 0.0.0.0-127.255.255.255 16,777,214 10.0.0.0-10.255.255.255 B 128.0.0.0-191.255.255.255 65534 172.16.0.0-172.31.255.255 C 192.0.0.0-223.255.255.255 254 192.168.0.0-192.168.255.255 C类主机数太少了只有254个，一般大网吧都满足不了。B类又太多了，6 万多台机器放在一个网络下面，一般的企业基本达不到这个规模，闲着的地址就是浪费。为了解决C类主机数太少，B类主机数太多的窘状，出现了CIDR 无类型域间选路（CIDR）这种方式打破了原来设计的分类方案，将原来的ip地址一分为二，前边是网络号，后边是主机号 从哪里分呢？你如果注意观察的话可以看到，10.100.122.2/24，这个 IP 地址中有一个斜杠，斜杠后面有个数字 24。这种地址表示形式，就是 CIDR。后面 24 的意思是，32 位中，前 24 位是网络号，后 8 位是主机号。 CIDR里有几个概念 广播地址: 10.100.122.255。如果发送这个地址，所有 10.100.122 网络里面的机器都可以收到 子网掩码: 255.255.255.0 子网掩码&amp;IP地址=网络号: 10.100.122.2&amp;255.255.255.0=10.100.122.0 CIDR 可以用来判断是不是本地人，网络号一样就是本地人 公有IP和私有IP平时我们看到的数据中心里，办公室、家里或学校的 IP 地址，一般都是私有 IP 地址段。因为这些地址允许组织内部的 IT 人员自己管理、自己分配，而且可以重复。因此，你学校的某个私有 IP 地址段和我学校的可以是一样的。 公有 IP 地址有个组织统一分配，你需要去买。如果你搭建一个网站，给你学校的人使用，让你们学校的 IT 人员给你一个 IP 地址就行。但是假如你要做一个类似网易 163 这样的网站，就需要有公有 IP 地址，这样全世界的人才能访问。 表格中的 192.168.0.x 是最常用的私有 IP 地址。你家里有 Wi-Fi，对应就会有一个 IP 地址。一般你家里地上网设备不会超过 256 个，所以 /24 基本就够了。有时候我们也能见到 /16 的 CIDR，这两种是最常见的，也是最容易理解的。 不需要将十进制转换为二进制 32 位，就能明显看出 192.168.0 是网络号，后面是主机号。而整个网络里面的第一个地址 192.168.0.1，往往就是你这个私有网络的出口地址。例如，你家里的电脑连接 Wi-Fi，Wi-Fi 路由器的地址就是 192.168.0.1，而 192.168.0.255 就是广播地址。一旦发送这个地址，整个 192.168.0 网络里面的所有机器都能收到。 但是也不总都是这样的情况。因此，其他情况往往就会很难理解，还容易出错。 ip addr 和ifconfig 查ip信息内容分析12345678910111213root@test:~# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff inet 10.100.122.2/24 brd 10.100.122.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fec7:7975/64 scope link valid_lft forever preferred_lft forever scope在 IP 地址的后面有个 scope，对于 eth0 这张网卡来讲，是 global，说明这张网卡是可以对外的，可以接收来自各个地方的包。对于 lo 来讲，是 host，说明这张网卡仅仅可以供本机相互通信。 lo 全称是loopback，又称环回接口，往往会被分配到 127.0.0.1 这个地址。这个地址用于本机通信，经过内核处理后直接返回，不会在任何网络中出现。 MAC地址在 IP 地址的上一行是 link/ether fa:16:3e:c7:79:75 brd ff:ff:ff:ff:ff:ff，这个被称为MAC 地址，是一个网卡的物理地址，用十六进制，6 个 byte 表示。 MAC 地址是一个很容易让人“误解”的地址。因为 MAC 地址号称全局唯一，不会有两个网卡有相同的 MAC 地址，而且网卡自生产出来，就带着这个地址。 很多人看到这里就会想，既然这样，整个互联网的通信，全部用 MAC 地址好了，只要知道了对方的 MAC 地址，就可以把信息传过去。 这样当然是不行的。 一个网络包要从一个地方传到另一个地方，除了要有确定的地址，还需要有定位功能。 而有门牌号码属性的 IP 地址，才是有远程定位功能的。 例如，你去杭州市网商路 599 号 B 楼 6 层找刘超，你在路上问路，可能被问的人不知道 B 楼是哪个，但是可以给你指网商路怎么去。但是如果你问一个人，你知道这个身份证号的人在哪里吗？可想而知，没有人知道。 MAC 地址是有一定定位功能的，只不过范围非常有限。你可以根据 IP 地址，找到杭州市网商路 599 号 B 楼 6 层，但是依然找不到我，你就可以靠吼了，大声喊身份证 XXXX 的是哪位？我听到了，我就会站起来说，是我啊。但是如果你在上海，到处喊身份证 XXXX 的是哪位，我不在现场，当然不会回答，因为我在杭州不在上海。 所以，MAC 地址的通信范围比较小，局限在一个子网里面。例如，从 192.168.0.2/24 访问 192.168.0.3/24 是可以用 MAC 地址的。一旦跨子网，即从 192.168.0.2/24 到 192.168.1.2/24，MAC 地址就不行了，需要 IP 地址起作用了。 MAC 地址更像是身份证，是一个唯一的标识。它的唯一性设计是为了组网的时候，不同的网卡放在一个网络里面的时候，可以不用担心冲突。从硬件角度，保证不同的网卡有不同的标识。 网络设备的状态标识解析完了 MAC 地址，我们再来看 &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; 是干什么的？这个叫作net_device flags，网络设备的状态标识。 UP 表示网卡处于启动的状态；BROADCAST 表示这个网卡有广播地址，可以发送广播包；MULTICAST 表示网卡可以发送多播包；LOWER_UP 表示 L1 是启动的，也即网线插着呢。 mtuMTU1500 是指什么意思呢？是哪一层的概念呢？最大传输单元 MTU 为 1500，这是以太网的默认值。网络包是层层封装的。MTU 是二层 MAC 层的概念。MAC 层有 MAC 的头，以太网规定连 MAC 头带正文合起来，不允许超过 1500 个字节。正文里面有 IP 的头、TCP 的头、HTTP 的头。如果放不下，就需要分片来传输。 qdiscqdisc pfifo_fast 是什么意思呢？qdisc 全称是queueing discipline，中文叫排队规则。内核如果需要通过某个网络接口发送数据包，它都需要按照为这个接口配置的 qdisc（排队规则）把数据包加入队列。 最简单的 qdisc 是 pfifo，它不对进入的数据包做任何的处理，数据包采用先入先出的方式通过队列。 pfifo_fast 稍微复杂一些，它的队列包括三个波段（band）。在每个波段里面，使用先进先出规则。 三个波段（band）的优先级也不相同。band 0 的优先级最高，band 2 的最低。如果 band 0 里面有数据包，系统就不会处理 band 1 里面的数据包，band 1 和 band 2 之间也是一样。 数据包是按照服务类型（Type of Service，TOS）被分配到三个波段（band）里面的。TOS 是 IP 头里面的一个字段，代表了当前的包是高优先级的，还是低优先级的。 队列是个好东西，后面我们讲云计算中的网络的时候，会有很多用户共享一个网络出口的情况，这个时候如何排队，每个队列有多粗，队列处理速度应该怎么提升，我都会详细为你讲解。 如何手动设计IP地址使用net-tools: 12$ sudo ifconfig eth1 10.0.0.1/24$ sudo ifconfig eth1 up 使用iproute2: 12$ sudo ip addr add 10.0.0.1/24 dev eth1$ sudo ip link set up eth1 如果身边的人的IP都是192.68.1.x，我设置成 172.10.168.2会怎么样包发不出去，因为linux会首先判断这个ip和本机是一个网段的吗？只有是一个网段才会发送ARP协议获取MAC地址。如果判断不是同一网段，Linux 默认的逻辑是，如果这是一个跨网段的调用，它便不会直接将包发送到网络上，而是企图将包发送到网关。所以如果你要配置ip一定要问好你的网络管理员，人少还行，如果每个人都去问那管理员会疯的，所以有了DHCP 动态主机配置协议（DHCP）Dynamic Host Configuration Protocol 有了这个协议，网络管理员就轻松多了。他只需要配置一段共享的 IP 地址。每一台新接入的机器都通过 DHCP 协议，来这个共享的 IP 地址里申请，然后自动配置好就可以了。等人走了，或者用完了，还回去，这样其他的机器也能用。 DHCP工作模式第一步（DHCP Discover）新来的机器使用 IP 地址 0.0.0.0 发送了一个广播包，目的 IP 地址为 255.255.255.255。广播包封装了 UDP，UDP 封装了 BOOTP。其实 DHCP 是 BOOTP 的增强版，但是如果你去抓包的话，很可能看到的名称还是 BOOTP 协议。 在这个广播包里面，新人大声喊：我是新来的（Boot request），我的 MAC 地址是这个，我还没有 IP，谁能给租给我个 IP 地址！ 第二步（DHCP Offer）如果配置了DHCP server，他会立马知道来了个新人，是通过MAC地址判断的，所以唯一的MAC地址是多么重要。他会立马回DHCP Offer，DHCP Server 仍然使用广播地址作为目的地址，因为，此时请求分配 IP 的新人还没有自己的 IP。DHCP Server 回复说，我分配了一个可用的 IP 给你，你看如何？除此之外，服务器还发送了子网掩码、网关和 IP 地址租用期等信息。 第三步（选一个Offer）新来的机器很开心，它的广播得到了回复，并且有人愿意租给它一个 IP 地址了，这意味着它可以在网络上立足了。 当然更令人开心的是，如果有多个 DHCP Server，这台新机器会收到多个 IP 地址，简直受宠若惊。 它会选择其中一个 DHCP Offer，一般是最先到达的那个，并且会向网络发送一个 DHCP Request 广播数据包，包中包含客户端的 MAC 地址、接受的租约中的 IP 地址、提供此租约的 DHCP 服务器地址等，并告诉所有 DHCP Server 它将接受哪一台服务器提供的 IP 地址，告诉其他 DHCP 服务器，谢谢你们的接纳，并请求撤销它们提供的 IP 地址，以便提供给下一个 IP 租用请求者。 此时，由于还没有得到 DHCP Server 的最后确认，客户端仍然使用 0.0.0.0 为源 IP 地址、255.255.255.255 为目标地址进行广播。在 BOOTP 里面，接受某个 DHCP Server 的分配的 IP。 第四步（DHCP Server Ack）当 DHCP Server 接收到客户机的 DHCP request 之后，会广播返回给客户机一个 DHCP ACK 消息包，表明已经接受客户机的选择，并将这一 IP 地址的合法租用信息和其他的配置信息都放入该广播包，发给客户机，欢迎它加入网络大家庭。 最终租约达成的时候，还是需要广播一下，让大家都知道一下。 IP地址的收回与续租既然是租房子，就是有租期的。租期到了，管理员就要将 IP 收回。 如果不用的话，收回就收回了。就像你租房子一样，如果还要续租的话，不能到了时间再续租，而是要提前一段时间给房东说。DHCP 也是这样。 客户机会在租期过去 50% 的时候，直接向为其提供 IP 地址的 DHCP Server 发送 DHCP request 消息包。客户机接收到该服务器回应的 DHCP ACK 消息包，会根据包中所提供的新的租期以及其他已经更新的 TCP/IP 参数，更新自己的配置。这样，IP 租用更新就完成了。 HUB有一个叫作Hub的东西，也就是集线器。这种设备有多个口，可以将宿舍里的多台电脑连接起来。但是，和交换机不同，集线器没有大脑，它完全在物理层工作。它会将自己收到的每一个字节，都复制到其他端口上去。这是第一层物理层联通的方案。 你可能已经发现问题了。Hub 采取的是广播的模式，如果每一台电脑发出的包，宿舍的每个电脑都能收到，那就麻烦了。这就需要解决几个问题：包是发给谁的，顺序，包是否完整，这几个问题都是MAC层需要解决的 MAC层如何解决上述问题根据ip如何获取MAC地址根据IP地址获取MAC地址 ####第一个问题，发给谁，谁接收？ 这里用到一个物理地址，叫作链路层地址。但是因为第二层主要解决媒体接入控制的问题，所以它常被称为MAC 地址。 解决第一个问题就牵扯到第二层的网络包格式。对于以太网，第二层的最开始，就是目标的 MAC 地址和源的 MAC 地址。 第二个问题，顺序问题MAC的全称是Medium Access Control，即媒体访问控制。控制什么呢？其实就是控制在往媒体上发数据的时候，谁先发、谁后发的问题。防止发生混乱。这解决的是第二个问题。这个问题中的规则，学名叫多路访问。有很多算法可以解决这个问题。就像车管所管束马路上跑的车，能想的办法都想过了。 方式一：分多个车道。每个车一个车道，你走你的，我走我的。这在计算机网络里叫作信道划分； 方式二：今天单号出行，明天双号出行，轮着来。这在计算机网络里叫作轮流协议； 方式三：不管三七二十一，有事儿先出门，发现特堵，就回去。错过高峰再出。我们叫作随机接入协议。著名的以太网，用的就是这个方式。 第三个问题，如何校验包是否完整对于以太网，第二层的最后面是CRC，也就是循环冗余检测。通过 XOR 异或的算法，来计算整个包是否在发送的过程中出现了错误，主要解决第三个问题。 交换机一旦机器数目增多，问题就出现了。因为 Hub 是广播的，不管某个接口是否需要，所有的 Bit 都会被发送出去，然后让主机来判断是不是需要。这种方式路上的车少就没问题，车一多，产生冲突的概率就提高了。而且把不需要的包转发过去，纯属浪费。看来 Hub 这种不管三七二十一都转发的设备是不行了，需要点儿智能的。因为每个口都只连接一台电脑，这台电脑又不怎么换 IP 和 MAC 地址，只要记住这台电脑的 MAC 地址，如果目标 MAC 地址不是这台电脑的，这个口就不用转发了。 谁能知道目标 MAC 地址是否就是连接某个口的电脑的 MAC 地址呢？这就需要一个能把 MAC 头拿下来，检查一下目标 MAC 地址，然后根据策略转发的设备，这个设备是个二层设备，我们称为交换机。 交换机是有 MAC 地址学习能力的，学完了它就知道谁在哪儿了，不用广播了。 拓扑结构怎么形成的当机器变得很多的时候，一个交换机肯定不够用，需要多台交换机，交换机之间连接起来，就形成一个稍微复杂的拓扑结构。 如何解决拓扑结构中的环路问题 需要STP协议来解决 STP协议在数据结构中，有一个方法叫作最小生成树。有环的我们常称为图。将图中的环破了，就生成了树。在计算机网络中，生成树的算法叫作STP，全称Spanning Tree Protocol。 如何解决广播问题和安全问题？毕竟机器多了，交换机也多了，就算交换机比 Hub 智能一些，但是还是难免有广播的问题，一大波机器，相关的部门、不相关的部门，广播一大堆，性能就下来了。就像一家公司，创业的时候，一二十个人，坐在一个会议室，有事情大家讨论一下，非常方便。但是如果变成了 50 个人，全在一个会议室里面吵吵，就会乱的不得了。有两种解决方法，物理隔离、虚拟隔离 物理隔离每个部门设一个单独的会议室，对应到网络方面，就是每个部门有单独的交换机，配置单独的子网，这样部门之间的沟通就需要路由器了。路由器咱们还没讲到，以后再说。这样的问题在于，有的部门人多，有的部门人少。人少的部门慢慢人会变多，人多的部门也可能人越变越少。如果每个部门有单独的交换机，口多了浪费，少了又不够用。 虚拟隔离就是用我们常说的VLAN，或者叫虚拟局域网。使用 VLAN，一个交换机上会连属于多个局域网的机器，那交换机怎么区分哪个机器属于哪个局域网呢？ 我们只需要在原来的二层的头上加一个 TAG，里面有一个 VLAN ID，一共 12 位。为什么是 12 位呢？因为 12 位可以划分 4096 个 VLAN。这样是不是还不够啊。 现在的情况证明，目前云计算厂商里面绝对不止 4096 个用户。当然每个用户需要一个 VLAN 了啊，怎么办呢，这个我们在后面的章节再说。 如果我们买的交换机是支持 VLAN 的，当这个交换机把二层的头取下来的时候，就能够识别这个 VLAN ID。这样只有相同 VLAN 的包，才会互相转发，不同 VLAN 的包，是看不到的。这样广播问题和安全问题就都能够解决了。 有人会问交换机之间怎么连接呢？将两个交换机连接起来的口应该设置成什么 VLAN 呢？对于支持 VLAN 的交换机，有一种口叫作Trunk 口。它可以转发属于任何 VLAN 的口。交换机之间可以通过这种口相互连接。 ICMP协议ICMP全称Internet Control Message Protocol，就是互联网控制报文协议。这里面的关键词是“控制”，那具体是怎么控制的呢。 ICMP 报文是封装在 IP 包里面的。因为传输指令的时候，肯定需要源地址和目标地址。它本身非常简单。因为作为侦查兵，要轻装上阵，不能携带大量的包袱。 ICMP 报文有很多的类型，不同的类型有不同的代码。最常用的类型是主动请求为 8,主动请求的应答为0 查询报文类型常用的ping 就是查询报文，是一种主动请求，并且获得主动应答的 ICMP 协议。所以，ping 发的包也是符合 ICMP 协议格式的，只不过它在后面增加了自己的格式。 差错报文类型Traceroute 使用差错报文,详细请看专栏 路由器网关往往是一个路由器，是一个三层转发的设备。啥叫三层设备？前面也说过了，就是把 MAC 头和 IP 头都取下来，然后根据里面的内容，看看接下来把包往哪里转发的设备。]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程下hashmap死循环]]></title>
    <url>%2F2019%2F02%2F16%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8Bhashmap%E6%AD%BB%E5%BE%AA%E7%8E%AF%2F</url>
    <content type="text"><![CDATA[问题由于HashMap并非是线程安全的，所以在高并发的情况下必然会出现问题，可能发生死循环，导致cpu100%，服务重启之后，问题消失，过段时间可能又复现了。这是为什么 原因分析在了解来龙去脉之前，我们先看看HashMap的数据结构。在内部，HashMap使用一个Entry数组保存key、value数据，当一对key、value被加入时，会通过一个hash算法得到数组的下标index，算法很简单，根据key的hash值，对数组的大小取模 hash &amp; (length-1)，并把结果插入数组该位置，如果该位置上已经有元素了，就说明存在hash冲突，这样会在index位置生成链表。如果存在hash冲突，最惨的情况，就是所有元素都定位到同一个位置，形成一个长长的链表，这样get一个值时，最坏情况需要遍历所有节点，性能变成了O(n)，所以元素的hash值算法和HashMap的初始化大小很重要。当插入一个新的节点时，如果不存在相同的key，则会判断当前内部元素是否已经达到阈值（默认是数组大小的0.75），如果已经达到阈值，会对数组进行扩容，也会对链表中的元素进行rehash。 实现HashMap的put方法实现： 判断key是否已经存在123456789101112131415161718192021public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); // 如果key已经存在，则替换value，并返回旧值 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // key不存在，则插入新的元素 addEntry(hash, key, value, i); return null;&#125; 检查容量是否达到阈值threshold123456789void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125; 复制代码如果元素个数已经达到阈值，则扩容，并把原来的元素移动过去。 扩容实现1234567891011void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; ... Entry[] newTable = new Entry[newCapacity]; ... transfer(newTable, rehash); table = newTable; threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 这里会新建一个更大的数组，并通过transfer方法，移动元素。123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 移动的逻辑也很清晰，遍历原来table中每个位置的链表，并对每个元素进行重新hash，在新的newTable找到归宿，并插入。 案例分析假设HashMap初始化大小为4，插入个3节点，不巧的是，这3个节点都hash到同一个位置，如果按照默认的负载因子的话，插入第3个节点就会扩容，为了验证效果，假设负载因子是1.123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 以上是节点移动的相关逻辑。 两个线程同时rehash插入第4个节点时，发生rehash，假设现在有两个线程同时进行，线程1和线程2，两个线程都会新建新的数组。 线程2Block线程1继续执行假设 线程2 在执行到Entry&lt;K,V&gt; next = e.next;之后，cpu时间片用完了，这时变量e指向节点a，变量next指向节点b。线程1继续执行，很不巧，a、b、c节点rehash之后又是在同一个位置7，开始移动节点第一步，移动节点a 第二步，移动节点b 注意，这里的顺序是反过来的，继续移动节点c 线程1Block线程2开始执行这个时候 线程1 的时间片用完，内部的table还没有设置成新的newTable， 线程2 开始执行，这时内部的引用关系如下： 线程2继续循环剩余逻辑这时，在 线程2 中，变量e指向节点a，变量next指向节点b，开始执行循环体的剩余逻辑。12345Entry&lt;K,V&gt; next = e.next;int i = indexFor(e.hash, newCapacity);e.next = newTable[i];newTable[i] = e;e = next; 执行之后的引用关系如下图执行后，变量e指向节点b，因为e不是null，则继续执行循环体 线程2第二次循环12345Entry&lt;K,V&gt; next = e.next;int i = indexFor(e.hash, newCapacity);e.next = newTable[i];newTable[i] = e;e = next; 执行后的引用关系 变量e又重新指回节点a，只能继续执行循环体 线程2第三次循环(出现环)12345678// 目前节点a没有next，所以变量next指向null；Entry&lt;K,V&gt; next = e.next;// 其中 newTable[i] 指向节点b，那就是把a的next指向了节点b，这样a和b就相互引用了，形成了一个环e.next = newTable[i]; // 把节点a放到了数组i位置；newTable[i] = e // 把变量e赋值为null，因为第一步中变量next就是指向null；e = next; 所以最终的引用关系是这样的： 节点a和b互相引用，形成了一个环，当在数组该位置get寻找对应的key时，就发生了死循环。另外，如果线程2把newTable设置成到内部的table，节点c的数据就丢了，看来还有数据遗失的问题。 总结所以在并发的情况，发生扩容时，可能会产生循环链表，在执行get的时候，会触发死循环，引起CPU的100%问题，所以一定要避免在并发环境下使用HashMap。曾经有人把这个问题报给了Sun，不过Sun不认为这是一个bug，因为在HashMap本来就不支持多线程使用，要并发就用ConcurrentHashmap。 文章参考 占小狼和coolshell的博客]]></content>
      <tags>
        <tag>java基础</tag>
        <tag>java并发编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal]]></title>
    <url>%2F2019%2F02%2F13%2FThreadLocal%2F</url>
    <content type="text"><![CDATA[ThreadLocal是什么jdk文档介绍1234567891011121314/** * This class provides thread-local variables. These variables differ from * their normal counterparts in that each thread that accesses one (via its * &#123;@code get&#125; or &#123;@code set&#125; method) has its own, independently initialized * copy of the variable. &#123;@code ThreadLocal&#125; instances are typically private * static fields in classes that wish to associate state with a thread (e.g., * a user ID or Transaction ID). * * &lt;p&gt;For example, the class below generates unique identifiers local to each * thread. * A thread's id is assigned the first time it invokes &#123;@code ThreadId.get()&#125; * and remains unchanged on subsequent calls. */ 结合我的总结可以这样理解：ThreadLocal提供了线程的局部变量，每个线程都可以通过set()和get()来对这个局部变量进行操作，但不会和其他线程的局部变量进行冲突，实现了线程的数据隔离～。简要言之：往ThreadLocal中填充的变量属于当前线程，该变量对其他线程而言是隔离的。 作用是什么从上面可以得出：ThreadLocal可以让我们拥有当前线程的变量，那这个作用有什么用呢？？？最常见的ThreadLocal使用场景为用来解决 数据库连接、Session管理、避免一些参数传递等例如12345678910private static ThreadLocal&lt;Connection&gt; connectionHolder= new ThreadLocal&lt;Connection&gt;() &#123;public Connection initialValue() &#123; return DriverManager.getConnection(DB_URL);&#125;&#125;; public static Connection getConnection() &#123;return connectionHolder.get();&#125; 1234567891011121314private static final ThreadLocal threadSession = new ThreadLocal(); public static Session getSession() throws InfrastructureException &#123; Session s = (Session) threadSession.get(); try &#123; if (s == null) &#123; s = getSessionFactory().openSession(); threadSession.set(s); &#125; &#125; catch (HibernateException ex) &#123; throw new InfrastructureException(ex); &#125; return s;&#125; 实现原理Thread、ThreadLocal、ThreadLocalMap、Entry之间的关系 ThreadLocal的实现是这样的：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 是 ThreadLocal实例本身，value 是真正需要存储的 Object。 也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，弱引用的对象在 GC 时会被回收。 ThreadLocalMap由一个个Entry对象构成，Entry的代码如下：12345678static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125;&#125; Entry继承自WeakReference&lt;ThreadLocal&lt;?&gt;&gt;，一个Entry由ThreadLocal对象和Object构成。由此可见，Entry的key是ThreadLocal对象，并且是一个弱引用。当没指向key的强引用后，该key就会被垃圾收集器回收。 那么，ThreadLocal是如何工作的呢？下面来看set和get方法。1234567891011121314151617181920212223242526public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value);&#125;public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue();&#125;ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; ThreadLocal为什么会内存泄漏ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话(比如线程池线程循环利用)，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。 其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。 但是这些被动的预防措施并不能保证不会内存泄漏： 使用static的ThreadLocal有外部强引用，延长了ThreadLocal的生命周期，可能导致的内存泄漏 分配使用了ThreadLocal又不再调用get(),set(),remove()方法，那么就会导致内存泄漏。(虽然ThreadLocal回收了但是value没有被回收) 为什么使用弱引用从表面上看内存泄漏的根源在于使用了弱引用。网上的文章大多着重分析ThreadLocal使用了弱引用会导致内存泄漏，但是另一个问题也同样值得思考：为什么使用弱引用而不是强引用？ 我们先来看看官方文档的说法： To help deal with very large and long-lived usages, the hash table entries use WeakReferences for keys.为了应对非常大和长时间的用途，哈希表使用弱引用的 key。 下面我们分两种情况讨论： key 使用强引用：引用的ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。 key 使用弱引用：引用的ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的弱引用，即使没有手动删除，ThreadLocal也会被回收。value在下一次ThreadLocalMap调用set,get，remove的时候会被清除。 比较两种情况，我们可以发现：由于ThreadLocalMap的生命周期跟Thread一样长，如果都没有手动删除对应key，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用ThreadLocal不会内存泄漏，对应的value在下一次ThreadLocalMap调用set,get,remove的时候会被清除。 因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 避免内存泄漏综合上面的分析，我们可以理解ThreadLocal内存泄漏的前因后果，那么怎么避免内存泄漏呢？ 每次使用完ThreadLocal，都调用它的remove()方法，清除数据。在使用线程池的情况下，thread的生命周期很长的情况下，没有及时清理ThreadLocal，导致内存泄漏随着应用运行的时间越来越长会导致内存溢出。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表]]></title>
    <url>%2F2019%2F01%2F29%2F%E9%93%BE%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[相比数组，链表是一种稍微复杂一点的数据结构。对于初学者来说，掌握起来也要比数组稍难一些。这两个非常基础、非常常用的数据结构，我们常常将会放到一块儿来比较。所以我们先来看，这两者有什么区别。 链表和数组的区别为了直观地对比，我画了一张图。从图中我们看到，数组需要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个 100MB 大小的数组，当内存中没有连续的、足够大的存储空间时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。而链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。 链表结构五花八门，今天我重点给你介绍三种最常见的链表结构，它们分别是：单链表、双向链表和循环链表。 单链表我们刚刚讲到，链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点的地址。如图所示，我们把这个记录下个结点地址的指针叫作“后继指针” 从我画的单链表图中，你应该可以发现，其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针不是指向下一个结点，而是指向一个空地址NULL，表示这是链表上最后一个结点。 与数组一样，链表也支持数据的查找、插入和删除操作。 我们知道，在进行数组的插入、删除操作时，为了保持内存数据的连续性，需要做大量的数据搬移，所以时间复杂度是 O(n)。而在链表中插入或者删除一个数据，我们并不需要为了保持内存的连续性而搬移结点，因为链表的存储空间本身就不是连续的。所以，在链表中插入和删除一个数据是非常快速的。 为了方便你理解，我画了一张图，从图中我们可以看出，针对链表的插入和删除操作，我们只需要考虑相邻结点的指针改变，所以对应的时间复杂度是 O(1)。 但是，有利就有弊。链表要想随机访问第 k 个元素，就没有数组那么高效了。因为链表中的数据并非连续存储的，所以无法像数组那样，根据首地址和下标，通过寻址公式就能直接计算出对应的内存地址，而是需要根据指针一个结点一个结点地依次遍历，直到找到相应的结点。 你可以把链表想象成一个队伍，队伍中的每个人都只知道自己后面的人是谁，所以当我们希望知道排在第 k 位的人是谁的时候，我们就需要从第一个人开始，一个一个地往下数。所以，链表随机访问的性能没有数组好，需要 O(n) 的时间复杂度。 循环链表循环链表是一种特殊的单链表。实际上，循环链表也很简单。它跟单链表唯一的区别就在尾结点。我们知道，单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。从我画的循环链表图中，你应该可以看出来，它像一个环一样首尾相连，所以叫作“循环”链表。和单链表相比，循环列表的优点是是从链尾到链头比较方便。当要处理的数据具有环型结构特点时，就特别适合采用循环链表。比如著名的约瑟夫问题。尽管用单链表也可以实现，但是用循环链表实现的话，代码就会简洁很多。 双向链表单向链表只有一个方向，结点只有一个后继指针 next 指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。单向链表只有一个方向，结点只有一个后继指针 next 指向后面的结点。而双向链表，顾名思义，它支持两个方向，每个结点不止有一个后继指针 next 指向后面的结点，还有一个前驱指针 prev 指向前面的结点。 从结构上来看，双向链表可以支持 O(1) 时间复杂度的情况下找到前驱结点，正是这样的特点，也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效。 你可能会说，我刚讲到单链表的插入、删除操作的时间复杂度已经是 O(1) 了，双向链表还能再怎么高效呢？别着急，刚刚的分析比较偏理论，很多数据结构和算法书籍中都会这么讲，但是这种说法实际上是不准确的，或者说是有先决条件的。我再来带你分析一下链表的两个操作。 删除操作在实际的软件开发中，从链表中删除一个数据无外乎这两种情况： 删除结点中“值等于某个给定值”的结点；这种情况，不管是单链表还是双向链表，为了查找到值等于给定值的结点，都需要从头结点开始一个一个依次遍历对比，直到找到值等于给定值的结点，然后再通过我前面讲的指针操作将其删除。 尽管单纯的删除操作时间复杂度是 O(1)，但遍历查找的时间是主要的耗时点，对应的时间复杂度为 O(n)。根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的总时间复杂度为 O(n)。 删除给定指针指向的结点。对于这种种情况，我们已经找到了要删除的结点，但是删除某个结点 q 需要知道其前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表，直到 p-&gt;next=q，说明 p 是 q 的前驱结点。 但是对于双向链表来说，这种情况就比较有优势了。因为双向链表中的结点已经保存了前驱结点的指针，不需要像单链表那样遍历。所以，针对第二种情况，单链表删除操作需要 O(n) 的时间复杂度，而双向链表只需要在 O(1) 的时间复杂度内就搞定了！ 同理，如果我们希望在链表的某个指定结点前面插入一个结点，双向链表比单链表有很大的优势。双向链表可以在 O(1) 时间复杂度搞定，而单向链表需要 O(n) 的时间复杂度。你可以参照我刚刚讲过的删除操作自己分析一下。 除了插入、删除操作有优势之外，对于一个有序链表，双向链表的按值查询的效率也要比单链表高一些。因为，我们可以记录上次查找的位置 p，每次查询时，根据要查找的值与 p 的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据。 现在，你有没有觉得双向链表要比单链表更加高效呢？这就是为什么在实际的软件开发中，双向链表尽管比较费内存，但还是比单链表的应用更加广泛的原因。如果你熟悉 Java 语言，你肯定用过 LinkedHashMap 这个容器。如果你深入研究 LinkedHashMap 的实现原理，就会发现其中就用到了双向链表这种数据结构。 实际上，这里有一个更加重要的知识点需要你掌握，那就是用空间换时间的设计思想。当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路。 双向循环链表 数组vs链表性能比拼通过前面内容的学习，你应该已经知道，数组和链表是两种截然不同的内存组织方式。正是因为内存存储的区别，它们插入、删除、随机访问操作的时间复杂度正好相反。 开发中如何选择不过，数组和链表的对比，并不能局限于时间复杂度。而且，在实际的软件开发中，不能仅仅利用复杂度分析就决定使用哪个数据结构来存储数据。 数组简单易用，在实现上使用的是连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对 CPU 缓存不友好，没办法有效预读。 数组的缺点是大小固定，一经声明就要占用整块连续内存空间。如果声明的数组过大，系统可能没有足够的连续内存空间分配给它，导致“内存不足（out of memory）”。如果声明的数组过小，则可能出现不够用的情况。这时只能再申请一个更大的内存空间，把原数组拷贝进去，非常费时。链表本身没有大小的限制，天然地支持动态扩容，我觉得这也是它与数组最大的区别。 我举一个稍微极端的例子。如果我们用 ArrayList 存储了了 1GB 大小的数据，这个时候已经没有空闲空间了，当我们再插入数据的时候，ArrayList 会申请一个 1.5GB 大小的存储空间，并且把原来那 1GB 的数据拷贝到新申请的空间上。听起来是不是就很耗时？ 除此之外，如果你的代码对内存的使用非常苛刻，那数组就更适合你。因为链表中的每个结点都需要消耗额外的存储空间去存储一份指向下一个结点的指针，所以内存消耗会翻倍。而且，对链表进行频繁的插入、删除操作，还会导致频繁的内存申请和释放，容易造成内存碎片，如果是 Java 语言，就有可能会导致频繁的 GC（Garbage Collection，垃圾回收）。 所以，在我们实际的开发中，针对不同类型的项目，要根据具体情况，权衡究竟是选择数组还是链表。 链表实现LRU缓存我的思路是这样的：我们维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从链表头开始顺序遍历链表。 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。 如果次数据不在链表里 链表满了，删除尾结点，该数据插入头结点 链表还有空间，插入头结点 代码地址现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍链表，所以这种基于链表的实现思路，缓存访问的时间复杂度为 O(n)。 实际上，我们可以继续优化这个实现思路，比如引入散列表（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到 O(1)。因为要涉及我们还没有讲到的数据结构，所以这个优化方案，我现在就不详细说了，等讲到散列表的时候，我会再拿出来讲。 问题数组实现LRU缓存除了基于链表的实现思路，实际上还可以用数组来实现 LRU 缓存淘汰策略。如何利用数组实现 LRU 缓存淘汰策略呢？我把这个问题留给你思考。我的思路是这样：维护一个数组，越靠近尾部的结点是越早之前访问的。当有一个新的数据被访问时，我们从数组第一个元素开始遍历 如果此数据已经被缓存，我们遍历得到这个数据的下标，并将前面的所有数据都往右移动一位，然后在把该元素放到数据下标0的位置 如果数组不在数组里 数组满了，把最后一个元素之前的元素向右平移一位，最后一个元素自然也就丢弃了，然后把该数据放到下标为0的位置 数组还有空间，把数组里所有元素向右平移一位，然后把该数据放到下标为0的位置代码地址现在我们来看下缓存访问的时间复杂度是多少。因为不管缓存有没有满，我们都需要遍历一遍数组，所以这种基于数组的实现思路，缓存访问的时间复杂度为 O(n)。 如何判断一个字符串是回文串如何判断一个字符串是否是回文字符串的问题，我想你应该听过，我们今天的思题目就是基于这个问题的改造版本。如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？你有什么好的解决思路呢？相应的时间空间复杂度又是多少呢？我的思路是：使用快慢两个指针找到链表中点，慢指针每次前进一步，快指针每次前进两步。在慢指针前进的过程中，同时修改其 next 指针，使得链表前半部分反序。最后比较中点两侧的链表是否相等。代码地址 常见单链表编程题 单链表反转 链表中环的检测 两个有序链表合并 删除倒数第n个代码 求链表中间结点代码地址]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组]]></title>
    <url>%2F2019%2F01%2F29%2F%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[如何实现随机访问什么是数组？数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。 这个定义里有几个关键词 线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。 而与他对立的是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。 连续的内存空间和相同类型的数据正是有了这个特性，他才有了随机访问这个“杀手锏”。但是有利就有弊，比如删除添加为了保证连续性，必须要做数据搬移工作。说到数据的访问，那你知道数组是如何实现根据下标随机访问数组元素的吗？我们拿一个长度为 10 的 int 类型的数组 int[] a = new int[10] 来举例。在我画的这个图中，计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。 我们知道，计算机会给每个内存单元分配一个地址，计算机通过地址来访问内存中的数据。当计算机需要随机访问数组中的某个元素时，它会首先通过下面的寻址公式，计算出该元素存储的内存地址。1a[i]_address = base_address + i * data_type_size 其中data_type_size表示每个元素的大小。我们这个例子数组中存储的是int类型数组，所以date_type_size就为4个字节。这里特别纠正一个“错误”。面试中经常会问到数组和链表的区别，很多人都会回答，“链表适合插入、删除、时间复杂度O(1);数组适合查找，查找时间复杂度为O(1)”。实际上，这种表述是不准确的。数组是适合查找操作，但查找时间复杂度不是O(1)。即便是排序好的数组，你用二分查找时间复杂度才是O(logn)。所以正确的表述是数组支持随机访问，根据下标随机访问的时间复杂度为O(1)。 低效的插入和删除前面提到，数组为了保证内存数组连续性，会导致插入、删除两个操作会比较低效。 插入操作假设数组长度为n，现在，需要把一个数据插入到数组第k个位置。为了把第k个位置腾出来，给新来的数据，我们需要把k～n这部分元素都顺序往后挪一下，那时间复杂度是多少呢？如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。如果数组中的数据是有序的，我们在某个位置插入一个新元素，就必须按照刚才的方法搬移k之后的数组。但是如果数组中的数据没有任何规律，数组只是存储集合。在这种情况下，如果把数据插入第k个位置，为了避免大规模搬移，直接将第k个位置的数据插入到数组最后，把新的元素放在第k个位置。如下图所示 利用这个技巧，在特定场景下，在第k个位置插入元素的复杂度就降到了O(1) 删除操作跟插入操作类似，如果删除第k个位置的数据，为了内存连续性，也需要搬移数据，不然会出现空洞，内存就不连续了。和插入类似，如果删除末尾的数据，时间复杂度O(1);如果删除开头的数据，则最坏情况时间复杂度为O(n);平均情况时间复杂度也为O(n)。实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？我们继续来看例子。数组 a[10] 中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。 为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。如果你了解 JVM，你会发现，这不就是 JVM 标记清除垃圾回收算法的核心思想吗？没错，数据结构和算法的魅力就在于此，很多时候我们并不是要去死记硬背某个数据结构或者算法，而是要学习它背后的思想和处理技巧，这些东西才是最有价值的。 容器能否代替数组？针对数组类型，很多语言提供了容器类，比如java中的ArrayList。在项目开发中，什么时候适合用数组，什么时候适合用容器呢？因为我是java开发，几乎每天都在用ArrayList。个人觉得ArrayList的最大优势就是可以把很多数组操作的细节封装起来。比如插入、删除、动态扩容。 数组本身在定义时需要预先指定大小，因为需要分配连续的内存空间。乳沟我们申请了大小为10的数组，当第11个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的数据复制过去，然后在插入新的数据。 如果使用ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList已经帮我们实现好了。每次空间不够时，他都会自动扩容为1.5倍大小。 不过，这里需要注意一点，因为扩容需要内存申请和数据搬移，比较耗时。所以，如果事先能去定需要存储数据的大小，最后在创建ArrayList的时候事先指定数据大小。 作为高级语言编程者，是不是数组就无用武之地了？当然不是，一下情况可以用数组。 Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 Object[][] array；而用容器的话则需要这样定义：ArrayList array。 我总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。 为什么大多数编程语言数组下标要从0开始而不是1从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式：1a[k]_address = base_address + k * type_size 但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为：1a[k]_address = base_address + (k-1)*type_size 对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。 数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。 不过我认为，上面解释得再多其实都算不上压倒性的证明，说数组起始编号非 0 开始不可。所以我觉得最主要的原因可能是历史原因。 C 语言设计者用 0 开始计数数组下标，之后的 Java、JavaScript 等高级语言都效仿了 C 语言，或者说，为了在一定程度上减少 C 语言程序员学习 Java 的学习成本，因此继续沿用了从 0 开始计数的习惯。实际上，很多语言中数组也并不是从 0 开始计数的，比如 Matlab。甚至还有一些语言支持负数下标，比如 Python。]]></content>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[策略模式]]></title>
    <url>%2F2019%2F01%2F26%2F%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[第一版鸭子游戏我们设计了一个鸭子游戏项目，游戏中会出现各种鸭子，为此Joe实现了一个超类Duck，让鸭子子类都继承这个类，如下图所示。 加鸭子会飞的需求使用继承过了几天主管找到Joe说希望有的鸭子能飞，Joe说so easy，只需在超类里增加一个fly()方法就ok了。但是过了几天主管对外演示时发现，橡皮鸭子在屏幕上飞来飞去，非常尴尬。主管打电话开始喷Joe:你可以去看看boss直聘了。Joe忽略了一件事，并非所有鸭子都会飞，他在超类里加上新的行为，会使得不该拥有这个行为的子类也拥有了这个行为。他体会了一件事：当涉及维护时为了复用目的使用继承结局并不完美。 Joe：那么橡皮鸭子类可以重写这个fly方法里边什么也不做，不就可以了吗？主管：那么所有不能飞的子类都得重写一遍fly，然后如果加入木鸭(不会飞不会叫)，也得把quack()方法重写。Joe意思到继承可能无法解决这个问题： 使用接口Joe：那么可以把fly()和quack()方法从超类里拆出来，只有能飞/能叫的鸭子子类实现这个接口主管：这真是个超级糟糕的设计，如果你认为重写几个方法很差劲，但是48个子类都稍微修改下飞行的行为呢？Joe：我去还真是，虽然Flayable和Quackable能解决一部分问题（不再有会飞的橡皮鸭），但是却造成代码无法复用啊，每个会飞的鸭子子类都得实现一个fly()方法哪怕他们的飞的行为是一样的，如果每个fly方法里增加打印日志得需要改48个子类。那咋办呀？ 设计原则一主管：把问题归零把，下面给你介绍一个设计原则：找出应用中可能需要变化的地方，把他们独立出来，不要在和哪些不变的代码混在一起了，把会变化的代码封装起来，好让其他部分不受影响，代码变化引起的bug变少，系统更加有弹性 下面是这个设计原则的另一种思考方式：“把会变化的部分取出并封装起来，以便以后可以轻易的改动或者扩展此部分，而不影响不需要变化的部分“主管：Joe是时候把鸭子的行为从Duck里行为拆出来了！Joe：ok，那么Duck里的行为除了fly和quack有问题外，其他行为一切还算正常，现在要分开变化部分和不变部分了，我打算建立两组类看图 设计原则二那么如何设计那组实现飞行和呱呱叫的的行为的类呢？我们希望一切能有弹性，毕竟，正是一开始鸭子行为没有弹性，才让我们走上现在这条路。我们还想能够“指定”行为到鸭子的实例。比方说，我们想要产生一个新的绿头鸭的实例，并指定特定的“类型”的飞行行为给它。干脆让鸭子的行为可以动态地改变好了。换句话说，我们应该在鸭子类中包含设定行为的方法，这样就可以在“运行时”动态地“改变”绿头鸭的飞行行为。有了这些目标要实现，接着看看第二个设计原则： 我们利用接口代表每个行为，比方说，FlyBehavior和QuackBehavior接口。所以这次鸭子类不回负责实现Fly与Quack接口，反而是由我们制造一组其他类专门实现FlyBehavior与QuackBehavior,这些就称为“行为”类。 这样的设计，可以让飞行和呱呱叫的动作被其他的对象复用，因为这些行为已经与鸭子类无关了。而我们可以新增一些行为，不会影响到既有的行为类，也不会影响“使用”到飞行行为的鸭子类。这么一来，有了继承的“复用”的好处，却没有继承所带来的包袱。 整合鸭子的行为关键在于，鸭子现在会将飞行和呱呱叫“委托”(delegate)别人处理，而不是定义在Duck类(或子类)内的呱呱叫和飞行方法。 1234567891011public class Duck &#123; //每只鸭子都会引用实现QuackBehavior接口的对象。 QuackBehavior quackBehavior; // 还有更多 ....... //鸭子对象不亲自处理呱呱叫的行为，而是委托给quackBehavior引用的对象 public void performQuack()&#123; quckBehavior.quack(); &#125;&#125; UML图 设计原则三“有一个”可能比“是一个”更好。“有一个”关系相当有趣：每一鸭子都有一个FlyBehavior和一个QuackBehavior，好将飞行和呱呱叫委托给它们代为处理。当你将两个类结合起来使用，如同本例一般，这就是组合(composition)。这种做法和“继承”不同的地方在于，鸭子的行为不是继承来的，而是和适当的行为对像“组合来的”。这是一个很重要的技巧。其实是使用了我们的第三个设计原则：]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态代理]]></title>
    <url>%2F2015%2F01%2F31%2F%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86%2F</url>
    <content type="text"><![CDATA[java静态代理和动态代理 代理的优点 职责清晰 真实的角色就是实现实际的业务逻辑，不用关心其他非本职责的事务，通过后期的代理完成一件完成事务，附带的结果就是编程简洁清晰。 代理对象可以在客户端和目标对象之间起到中介的作用，这样起到了中介的作用和保护了目标对象的作用。 高扩展性，可以在代理方法前后增加额外的处理逻辑。 被代理的对象一个接口123public interface Subject &#123; void doSomething();&#125; 它的实现类1234567public class SubjectImpl implements Subject &#123; @Override public void doSomething() &#123; System.out.println( "call doSomething()" ); &#125;&#125; 静态代理静态代理是由程序员创建或工具生成代理类的源码，再编译代理类。也就是在程序运行前就已经存在代理类的字节码文件，代理类和委托类的关系在运行前就确定了。缺点是被代理对象和代理紧耦合在一起。而且代理类都是针对被代理类创建的。12345678910public class SubjectProxy implements Subject &#123; Subject subject = new SubjectImpl(); @Override public void doSomething() &#123; System.out.println("before"); //调用目标对象之前可以做相关操作 subject.doSomething(); System.out.println("after");//调用目标对象之后可以做相关操作 &#125;&#125; 动态代理动态代理在运行阶段才指定被代理的对象，通过实现InvocationHandler接口，调用具体的方法。123456789101112131415161718public class ProxyHandler implements InvocationHandler &#123; private Object tar; public Object bind(Object tar) &#123; this.tar = tar; return Proxy.newProxyInstance(tar.getClass().getClassLoader(), tar.getClass().getInterfaces(), this); &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object result = null; System.out.println("before"); //调用目标对象之前可以做相关操作 result = method.invoke(tar, args); System.out.println("after");//调用目标对象之后可以做相关操作 return result; &#125;&#125; 运行12345678public class Test &#123; public static void main(String[] args) &#123; ProxyHandler proxy = new ProxyHandler(); //绑定该类实现的所有接口 Subject sub = (Subject) proxy.bind(new SubjectImpl()); sub.doSomething(); &#125;&#125;]]></content>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
</search>
