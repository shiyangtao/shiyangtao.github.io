<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">
<script>
    (function () {
        if ('') {
            if (prompt('请输入文章密码') !== '') {
                alert('密码错误！');
                if (history.length === 1) {
                    location.replace("https://shiyangtao.github.io"); // 这里替换成你的首页
                } else {
                    history.back();
                }
            }
        }
    })();
</script>








<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="shiyangtao's blog" type="application/atom+xml">






<meta name="description" content="minikube 搭建https://kubernetes.io/zh/docs/tasks/tools/install-minikube/ 1sudo minikube start --vm-driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --image-repository=&amp;apos;registry.cn-">
<meta property="og:type" content="article">
<meta property="og:title" content="k8s_command">
<meta property="og:url" content="http://shiyangtao.github.io/2020/02/21/k8s-command/index.html">
<meta property="og:site_name" content="shiyangtao&#39;s blog">
<meta property="og:description" content="minikube 搭建https://kubernetes.io/zh/docs/tasks/tools/install-minikube/ 1sudo minikube start --vm-driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --image-repository=&amp;apos;registry.cn-">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200222120521800.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200222121522402.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200225192309030.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200226080846682.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200226080929620.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200226093239685.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200229152908546.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200229195512419.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200229233523745.png">
<meta property="og:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200229233458219.png">
<meta property="og:updated_time" content="2020-04-10T13:37:19.895Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="k8s_command">
<meta name="twitter:description" content="minikube 搭建https://kubernetes.io/zh/docs/tasks/tools/install-minikube/ 1sudo minikube start --vm-driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --image-repository=&amp;apos;registry.cn-">
<meta name="twitter:image" content="http://shiyangtao.github.io/2020/02/21/k8s-command/images/image-20200222120521800.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://shiyangtao.github.io/2020/02/21/k8s-command/">


  <script>
  (function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice")
  daovoice('init', {
      app_id: "242df597"
    });
  daovoice('update');
  </script>




  <title>k8s_command | shiyangtao's blog</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">shiyangtao's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://shiyangtao.github.io/2020/02/21/k8s-command/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sytao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="shiyangtao's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">k8s_command</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-21T11:44:32+08:00">
                2020-02-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  23.6k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  104
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="minikube-搭建"><a href="#minikube-搭建" class="headerlink" title="minikube 搭建"></a>minikube 搭建</h2><p><a href="https://kubernetes.io/zh/docs/tasks/tools/install-minikube/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tasks/tools/install-minikube/</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo minikube start --vm-driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --image-repository=&apos;registry.cn-hangzhou.aliyuncs.com/google_containers&apos;</span><br></pre></td></tr></table></figure>
<h3 id="外网访问dashboard"><a href="#外网访问dashboard" class="headerlink" title="外网访问dashboard"></a>外网访问dashboard</h3><p><a href="https://blog.haohtml.com/archives/19201" target="_blank" rel="noopener">https://blog.haohtml.com/archives/19201</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup kubectl proxy --address=&apos;0.0.0.0&apos; --port=8001 --accept-hosts=&apos;^*$&apos; &amp;</span><br></pre></td></tr></table></figure>
<p>外网访问dashboard</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://&lt;ip&gt;:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://47.92.136.48:8001/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/</span><br></pre></td></tr></table></figure>
<p>headless</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it busybox -- nslookup busybox-1.default-subdomain.default.svc.cluster.local</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it busybox -- nslookup default-subdomain.default.svc.cluster.local</span><br></pre></td></tr></table></figure>
<p>外网访问内部servcie服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://&lt;ip&gt;:8001/api/v1/namespaces/default/services/SERVICE-NAME:PORT-NAME/proxy/</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http://47.92.136.48:8001/api/v1/namespaces/default/services/hostnames:default/proxy/</span><br><span class="line">http://47.92.136.48:8001/api/v1/namespaces/default/services/coffee-svc:http/proxy/</span><br><span class="line">http://47.92.136.48:8001/api/v1/namespaces/default/services/tea-svc:http/proxy/</span><br></pre></td></tr></table></figure>
<p>ingress</p>
<h2 id="搭建kubeadm"><a href="#搭建kubeadm" class="headerlink" title="搭建kubeadm"></a>搭建kubeadm</h2><h3 id="安装-kubeadm-和-Docker"><a href="#安装-kubeadm-和-Docker" class="headerlink" title="安装 kubeadm 和 Docker"></a>安装 kubeadm 和 Docker</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line">deb http://apt.kubernetes.io/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line">apt-get update</span><br><span class="line">apt-get install -y docker.io kubeadm</span><br><span class="line"></span><br><span class="line"># 安装指定版本</span><br><span class="line">sudo apt-get install kubeadm=1.14.3-00 kubectl=1.14.3-00 kubelet=1.14.3-00</span><br></pre></td></tr></table></figure>
<h3 id="部署-Kubernetes-的-Master-节点"><a href="#部署-Kubernetes-的-Master-节点" class="headerlink" title="部署 Kubernetes 的 Master 节点"></a>部署 Kubernetes 的 Master 节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cat kubeadm.yaml</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">controllerManager:</span><br><span class="line">    extraArgs:</span><br><span class="line">        horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot;</span><br><span class="line">        horizontal-pod-autoscaler-sync-period: &quot;10s&quot;</span><br><span class="line">        node-monitor-grace-period: &quot;10s&quot;</span><br><span class="line">apiServer:</span><br><span class="line">    extraArgs:</span><br><span class="line">        runtime-config: &quot;api/all=true&quot;</span><br><span class="line">kubernetesVersion: &quot;stable-1.14&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm  init --config kubeadm.yaml --ignore-preflight-errors=NumCPU</span><br></pre></td></tr></table></figure>
<p>就可以完成 Kubernetes Master 的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令：这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 172.26.87.130:6443 --token 29kgba.dytvremded8eicnv \</span><br><span class="line">   --discovery-token-ca-cert-hash sha256:83a982aaefeb3935dcbcce7da005de080a7bd43086ecf26503605bd71b01f6cc</span><br></pre></td></tr></table></figure>
<h3 id="配置kubectl-授权信息"><a href="#配置kubectl-授权信息" class="headerlink" title="配置kubectl 授权信息"></a>配置kubectl 授权信息</h3><p>而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mkdir -p $HOME/.kube</span><br><span class="line">sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line"></span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    1d        v1.11.1</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe node master</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">Conditions:</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Ready   False ... KubeletNotReady  runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods -n kube-system</span><br><span class="line"></span><br><span class="line">NAME               READY   STATUS   RESTARTS  AGE</span><br><span class="line">coredns-78fcdf6894-j9s52     0/1    Pending  0     1h</span><br><span class="line">coredns-78fcdf6894-jm4wf     0/1    Pending  0     1h</span><br><span class="line">etcd-master           1/1    Running  0     2s</span><br><span class="line">kube-apiserver-master      1/1    Running  0     1s</span><br><span class="line">kube-controller-manager-master  0/1    Pending  0     1s</span><br><span class="line">kube-proxy-xbd47         1/1    NodeLost  0     1h</span><br><span class="line">kube-scheduler-master      1/1    Running  0     1s</span><br></pre></td></tr></table></figure>
<p>可以看到，CoreDNS、kube-controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。</p>
<h3 id="部署网络插件"><a href="#部署网络插件" class="headerlink" title="部署网络插件"></a>部署网络插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl apply -f &quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d &apos;\n&apos;)&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods -n kube-system</span><br><span class="line"></span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE</span><br><span class="line">coredns-78fcdf6894-j9s52         1/1       Running   0          1d</span><br><span class="line">coredns-78fcdf6894-jm4wf         1/1       Running   0          1d</span><br><span class="line">etcd-master                      1/1       Running   0          9s</span><br><span class="line">kube-apiserver-master            1/1       Running   0          9s</span><br><span class="line">kube-controller-manager-master   1/1       Running   0          9s</span><br><span class="line">kube-proxy-xbd47                 1/1       Running   0          1d</span><br><span class="line">kube-scheduler-master            1/1       Running   0          9s</span><br><span class="line">weave-net-cmk27                  2/2       Running   0          19s</span><br></pre></td></tr></table></figure>
<h3 id="部署-Kubernetes-的-Worker-节点"><a href="#部署-Kubernetes-的-Worker-节点" class="headerlink" title="部署 Kubernetes 的 Worker 节点"></a>部署 Kubernetes 的 Worker 节点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 172.26.87.130:6443 --token 29kgba.dytvremded8eicnv \</span><br><span class="line">   --discovery-token-ca-cert-hash sha256:83a982aaefeb3935dcbcce7da005de080a7bd43086ecf26503605bd71b01f6cc</span><br></pre></td></tr></table></figure>
<h3 id="通过-Taint-Toleration-调整-Master-执行-Pod-的策略"><a href="#通过-Taint-Toleration-调整-Master-执行-Pod-的策略" class="headerlink" title="通过 Taint/Toleration 调整 Master 执行 Pod 的策略"></a>通过 Taint/Toleration 调整 Master 执行 Pod 的策略</h3><p>打污点(taint)指令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl taint nodes node1 foo=bar:NoSchedule</span><br></pre></td></tr></table></figure>
<p>pod容忍(tolerations)这个污点指令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  tolerations:</span><br><span class="line">  - key: &quot;foo&quot;</span><br><span class="line">    operator: &quot;Equal&quot;</span><br><span class="line">    value: &quot;bar&quot;</span><br><span class="line">    effect: &quot;NoSchedule&quot;</span><br></pre></td></tr></table></figure>
<p>我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。</p>
<p>查看master默认污点</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe node master</span><br><span class="line"></span><br><span class="line">Name:               master</span><br><span class="line">Roles:              master</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br></pre></td></tr></table></figure>
<p>删除master默认污点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl taint nodes --all node-role.kubernetes.io/master-</span><br><span class="line">node.kubernetes.io/not-ready</span><br></pre></td></tr></table></figure>
<p>或者Pod容忍这个污点</p>
<p>可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  tolerations:</span><br><span class="line">  - key: &quot;foo&quot;</span><br><span class="line">    operator: &quot;Exists&quot;</span><br><span class="line">    effect: &quot;NoSchedule&quot;</span><br></pre></td></tr></table></figure>
<h3 id="部署-Dashboard-可视化插件"><a href="#部署-Dashboard-可视化插件" class="headerlink" title="部署 Dashboard 可视化插件"></a>部署 Dashboard 可视化插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml</span><br></pre></td></tr></table></figure>
<h3 id="部署容器存储插件"><a href="#部署容器存储插件" class="headerlink" title="部署容器存储插件"></a>部署容器存储插件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml</span><br></pre></td></tr></table></figure>
<h2 id="创建容器化应用"><a href="#创建容器化应用" class="headerlink" title="创建容器化应用"></a>创建容器化应用</h2><h3 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure>
<h3 id="替换"><a href="#替换" class="headerlink" title="替换"></a>替换</h3><p>修改yaml文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...    </span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.8 #这里被从1.7.9修改为1.8</span><br><span class="line">        ports:</span><br><span class="line">      - containerPort: 80</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl replace -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure>
<h3 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br><span class="line"></span><br><span class="line"># 修改nginx-deployment.yaml的内容</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure>
<h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f nginx-deployment.yaml</span><br></pre></td></tr></table></figure>
<h2 id="Pod属性"><a href="#Pod属性" class="headerlink" title="Pod属性"></a>Pod属性</h2><h3 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.8</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: nginx-vol</span><br><span class="line">      volumes:</span><br><span class="line">      - name: nginx-vol</span><br><span class="line">        emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure>
<h4 id="emptyDir"><a href="#emptyDir" class="headerlink" title="emptyDir"></a>emptyDir</h4><p>它其实就等同于我们之前讲过的 Docker 的隐式 Volume 参数，即：不显式声明宿主机目录的 Volume。所以，Kubernetes 也会在宿主机上创建一个临时目录，这个目录将来就会被绑定挂载到容器所声明的 Volume 目录上。<strong><em>备注：不难看到，Kubernetes 的 emptyDir 类型，只是把 Kubernetes 创建的临时目录作为 Volume 的宿主机目录，交给了 Docker。这么做的原因，是 Kubernetes 不想依赖 Docker 自己创建的那个 _data 目录。</em></strong></p>
<h4 id="hostPath"><a href="#hostPath" class="headerlink" title="hostPath"></a>hostPath</h4><p>当然，Kubernetes 也提供了显式的 Volume 定义，它叫做 hostPath。比如下面的这个 YAML 文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...   </span><br><span class="line">   volumes:</span><br><span class="line">     - name: nginx-vol</span><br><span class="line">       hostPath: </span><br><span class="line">         path: /var/data</span><br></pre></td></tr></table></figure>
<p>进入容器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec -it nginx-deployment-5c678cfb6d-lg9lw -- /bin/bash</span><br><span class="line"># ls /usr/share/nginx/html</span><br></pre></td></tr></table></figure>
<h4 id="Projected-Volume"><a href="#Projected-Volume" class="headerlink" title="Projected Volume"></a>Projected Volume</h4><p>在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是被 Kubernetes“投射”（Project）进入容器当中的。这正是 Projected Volume 的含义。到目前为止，Kubernetes 支持的 Projected Volume </p>
<p>一共有四种：</p>
<ul>
<li>Secret；</li>
<li>ConfigMap；</li>
<li>Downward API；</li>
<li>ServiceAccountToken。</li>
</ul>
<h5 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h5><p>它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-projected-volume </span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: test-secret-volume</span><br><span class="line">    image: busybox</span><br><span class="line">    args:</span><br><span class="line">    - sleep</span><br><span class="line">    - &quot;86400&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: mysql-cred</span><br><span class="line">      mountPath: &quot;/projected-volume&quot;</span><br><span class="line">      readOnly: true</span><br><span class="line">  volumes:</span><br><span class="line">  - name: mysql-cred</span><br><span class="line">    projected:</span><br><span class="line">      sources:</span><br><span class="line">      - secret:</span><br><span class="line">          name: user</span><br><span class="line">      - secret:</span><br><span class="line">          name: pass</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat ./username.txt</span><br><span class="line">admin</span><br><span class="line">$ cat ./password.txt</span><br><span class="line">c1oudc0w!</span><br><span class="line"></span><br><span class="line">$ kubectl create secret generic user --from-file=./username.txt</span><br><span class="line">$ kubectl create secret generic pass --from-file=./password.txt</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get secrets</span><br><span class="line">NAME           TYPE                                DATA      AGE</span><br><span class="line">user          Opaque                                1         51s</span><br><span class="line">pass          Opaque                                1         51s</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-projected-volume.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec -it test-projected-volume -- /bin/sh</span><br><span class="line">$ ls /projected-volume/</span><br><span class="line">user</span><br><span class="line">pass</span><br><span class="line">$ cat /projected-volume/user</span><br><span class="line">root</span><br><span class="line">$ cat /projected-volume/pass</span><br><span class="line">1f2d1e2e67df</span><br></pre></td></tr></table></figure>
<h5 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h5><p>与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># .properties文件的内容</span><br><span class="line">$ cat example/ui.properties</span><br><span class="line">color.good=purple</span><br><span class="line">color.bad=yellow</span><br><span class="line">allow.textmode=true</span><br><span class="line">how.nice.to.look=fairlyNice</span><br><span class="line"></span><br><span class="line"># 从.properties文件创建ConfigMap</span><br><span class="line">$ kubectl create configmap ui-config --from-file=example/ui.properties</span><br><span class="line"></span><br><span class="line"># 查看这个ConfigMap里保存的信息(data)</span><br><span class="line">$ kubectl get configmaps ui-config -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  ui.properties: |</span><br><span class="line">    color.good=purple</span><br><span class="line">    color.bad=yellow</span><br><span class="line">    allow.textmode=true</span><br><span class="line">    how.nice.to.look=fairlyNice</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: ui-config</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<h5 id="Downward-API"><a href="#Downward-API" class="headerlink" title="Downward API"></a>Downward API</h5><p>它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: test-downwardapi-volume</span><br><span class="line">  labels:</span><br><span class="line">    zone: us-est-coast</span><br><span class="line">    cluster: test-cluster1</span><br><span class="line">    rack: rack-22</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: client-container</span><br><span class="line">      image: k8s.gcr.io/busybox</span><br><span class="line">      command: [&quot;sh&quot;, &quot;-c&quot;]</span><br><span class="line">      args:</span><br><span class="line">      - while true; do</span><br><span class="line">          if [[ -e /etc/podinfo/labels ]]; then</span><br><span class="line">            echo -en &apos;\n\n&apos;; cat /etc/podinfo/labels; fi;</span><br><span class="line">          sleep 5;</span><br><span class="line">        done;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - name: podinfo</span><br><span class="line">          mountPath: /etc/podinfo</span><br><span class="line">          readOnly: false</span><br><span class="line">  volumes:</span><br><span class="line">    - name: podinfo</span><br><span class="line">      projected:</span><br><span class="line">        sources:</span><br><span class="line">        - downwardAPI:</span><br><span class="line">            items:</span><br><span class="line">              - path: &quot;labels&quot;</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.labels</span><br></pre></td></tr></table></figure>
<p>在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。</p>
<p>通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。</p>
<p>而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f dapi-volume.yaml</span><br><span class="line">$ kubectl logs test-downwardapi-volume</span><br><span class="line">cluster=&quot;test-cluster1&quot;</span><br><span class="line">rack=&quot;rack-22&quot;</span><br><span class="line">zone=&quot;us-est-coast&quot;</span><br></pre></td></tr></table></figure>
<p>目前Downward API支持的字段更加丰富了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1. 使用fieldRef可以声明使用:</span><br><span class="line">spec.nodeName - 宿主机名字</span><br><span class="line">status.hostIP - 宿主机IP</span><br><span class="line">metadata.name - Pod的名字</span><br><span class="line">metadata.namespace - Pod的Namespace</span><br><span class="line">status.podIP - Pod的IP</span><br><span class="line">spec.serviceAccountName - Pod的Service Account的名字</span><br><span class="line">metadata.uid - Pod的UID</span><br><span class="line">metadata.labels[&apos;&lt;KEY&gt;&apos;] - 指定&lt;KEY&gt;的Label值</span><br><span class="line">metadata.annotations[&apos;&lt;KEY&gt;&apos;] - 指定&lt;KEY&gt;的Annotation值</span><br><span class="line">metadata.labels - Pod的所有Label</span><br><span class="line">metadata.annotations - Pod的所有Annotation</span><br><span class="line"></span><br><span class="line">2. 使用resourceFieldRef可以声明使用:</span><br><span class="line">容器的CPU limit</span><br><span class="line">容器的CPU request</span><br><span class="line">容器的memory limit</span><br><span class="line">容器的memory request</span><br></pre></td></tr></table></figure>
<h5 id="Service-Account"><a href="#Service-Account" class="headerlink" title="Service Account"></a>Service Account</h5><p>default Service Account</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lw</span><br><span class="line">Containers:</span><br><span class="line">...</span><br><span class="line">  Mounts:</span><br><span class="line">    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)</span><br><span class="line">Volumes:</span><br><span class="line">  default-token-s8rbq:</span><br><span class="line">  Type:       Secret (a volume populated by a Secret)</span><br><span class="line">  SecretName:  default-token-s8rbq</span><br><span class="line">  Optional:    false</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /var/run/secrets/kubernetes.io/serviceaccount </span><br><span class="line">ca.crt namespace  token</span><br></pre></td></tr></table></figure>
<h3 id="NodeSelector"><a href="#NodeSelector" class="headerlink" title="NodeSelector"></a>NodeSelector</h3><p>是一个供用户将 Pod 与 Node 进行绑定的字段</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line"> nodeSelector:</span><br><span class="line">   disktype: ssd</span><br></pre></td></tr></table></figure>
<h3 id="NodeName"><a href="#NodeName" class="headerlink" title="NodeName"></a>NodeName</h3><p>一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  nodeName: k8s.node1 #指定调度节点为k8s.node1</span><br></pre></td></tr></table></figure>
<h3 id="HostAliases"><a href="#HostAliases" class="headerlink" title="HostAliases"></a>HostAliases</h3><p>定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">  hostAliases:</span><br><span class="line">  - ip: &quot;10.1.2.3&quot;</span><br><span class="line">    hostnames:</span><br><span class="line">    - &quot;foo.remote&quot;</span><br><span class="line">    - &quot;bar.remote&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">cat /etc/hosts</span><br><span class="line"># Kubernetes-managed hosts file.</span><br><span class="line">127.0.0.1 localhost</span><br><span class="line">...</span><br><span class="line">10.244.135.10 hostaliases-pod</span><br><span class="line">10.1.2.3 foo.remote</span><br><span class="line">10.1.2.3 bar.remote</span><br></pre></td></tr></table></figure>
<h3 id="shareProcessNamespace-true"><a href="#shareProcessNamespace-true" class="headerlink" title="shareProcessNamespace=true"></a>shareProcessNamespace=true</h3><p> Pod 里的容器要共享 PID Namespace。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  shareProcessNamespace: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">  - name: shell</span><br><span class="line">    image: busybox</span><br><span class="line">    stdin: true</span><br><span class="line">    tty: true</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f nginx.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl attach -it nginx -c shell</span><br><span class="line">/ # ps ax</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:00 /pause</span><br><span class="line">    8 root      0:00 nginx: master process nginx -g daemon off;</span><br><span class="line">   14 101       0:00 nginx: worker process</span><br><span class="line">   15 root      0:00 sh</span><br><span class="line">   21 root      0:00 ps ax</span><br></pre></td></tr></table></figure>
<h3 id="共享宿主机的Linux-namespace"><a href="#共享宿主机的Linux-namespace" class="headerlink" title="共享宿主机的Linux namespace"></a>共享宿主机的Linux namespace</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  hostIPC: true</span><br><span class="line">  hostPID: true</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">  - name: shell</span><br><span class="line">    image: busybox</span><br><span class="line">    stdin: true</span><br><span class="line">    tty: true</span><br></pre></td></tr></table></figure>
<h3 id="Probe"><a href="#Probe" class="headerlink" title="Probe"></a>Probe</h3><p>这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。</p>
<p>####livenessProbe </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    test: liveness</span><br><span class="line">  name: test-liveness-exec</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: liveness</span><br><span class="line">    image: busybox</span><br><span class="line">    args:</span><br><span class="line">    - /bin/sh</span><br><span class="line">    - -c</span><br><span class="line">    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600</span><br><span class="line">    livenessProbe:</span><br><span class="line">      exec:</span><br><span class="line">        command:</span><br><span class="line">        - cat</span><br><span class="line">        - /tmp/healthy</span><br><span class="line">      initialDelaySeconds: 5</span><br><span class="line">      periodSeconds: 5</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-liveness-exec.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod</span><br><span class="line">NAME                READY     STATUS    RESTARTS   AGE</span><br><span class="line">test-liveness-exec   1/1       Running   0          10s</span><br></pre></td></tr></table></figure>
<p>可以看到，由于已经通过了健康检查，这个 Pod 就进入了 Running 状态。</p>
<p>而 30 s 之后，我们再查看一下 Pod 的 Events：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod test-liveness-exec</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你会发现，这个 Pod 在 Events 报告了一个异常：</span><br><span class="line">FirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message</span><br><span class="line">--------- --------    -----   ----            -------------           --------    ------      -------</span><br><span class="line">2s        2s      1   &#123;kubelet worker0&#125;   spec.containers&#123;liveness&#125;   Warning     Unhealthy   Liveness probe failed: cat: can&apos;t open &apos;/tmp/healthy&apos;: No such file or directory</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod test-liveness-exec</span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE</span><br><span class="line">liveness-exec   1/1       Running   1          1m</span><br></pre></td></tr></table></figure>
<p>这时我们发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态。这是为什么呢？其实，如果你注意到 RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了。在这个过程中，Pod 保持 Running 状态不变。</p>
<p>除了在容器中执行命令外，livenessProbe 也可以定义为发起 HTTP 或者 TCP 请求的方式，定义格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">livenessProbe:</span><br><span class="line">     httpGet:</span><br><span class="line">       path: /healthz</span><br><span class="line">       port: 8080</span><br><span class="line">       httpHeaders:</span><br><span class="line">       - name: X-Custom-Header</span><br><span class="line">         value: Awesome</span><br><span class="line">       initialDelaySeconds: 3</span><br><span class="line">       periodSeconds: 3</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    livenessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 15</span><br><span class="line">      periodSeconds: 20</span><br></pre></td></tr></table></figure>
<h4 id="readinessProbe"><a href="#readinessProbe" class="headerlink" title="readinessProbe"></a>readinessProbe</h4><p>虽然它的用法与 livenessProbe 类似，但作用却大不一样。readinessProbe 检查结果的成功与否，决定的这个 Pod 是不是能被通过 Service 的方式访问到，而并不影响 Pod 的生命周期。</p>
<h3 id="restartPolicy"><a href="#restartPolicy" class="headerlink" title="restartPolicy"></a>restartPolicy</h3><p>它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。</p>
<p>但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。但是Deployment可以跑到别的节点上去。</p>
<p>而作为用户，你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况：</p>
<ul>
<li>Always：在任何情况下，只要容器不在运行状态，就自动重启容器；</li>
<li>OnFailure: 只在容器 异常时才自动重启容器；</li>
<li>Never: 从来不重启容器。</li>
</ul>
<p>实际上，你根本不需要死记硬背这些对应关系，只要记住如下两个基本的设计原理即可</p>
<ol>
<li>只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。</li>
<li>对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数，比如</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod test-liveness-exec</span><br><span class="line">NAME           READY     STATUS    RESTARTS   AGE</span><br><span class="line">liveness-exec   0/1       Running   1          1m</span><br></pre></td></tr></table></figure>
<h3 id="PodPreset"><a href="#PodPreset" class="headerlink" title="PodPreset"></a>PodPreset</h3><p>举个例子，现在开发人员编写了如下一个 pod.yaml 文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: website</span><br><span class="line">  labels:</span><br><span class="line">    app: website</span><br><span class="line">    role: frontend</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: website</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>
<p>这个时候，运维人员就可以定义一个 PodPreset 对象。在这个对象中，凡是他想在开发人员编写的 Pod 里追加的字段，都可以预先定义好。比如这个 preset.yaml:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: settings.k8s.io/v1alpha1</span><br><span class="line">kind: PodPreset</span><br><span class="line">metadata:</span><br><span class="line">  name: allow-database</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      role: frontend</span><br><span class="line">  env:</span><br><span class="line">    - name: DB_PORT</span><br><span class="line">      value: &quot;6379&quot;</span><br><span class="line">  volumeMounts:</span><br><span class="line">    - mountPath: /cache</span><br><span class="line">      name: cache-volume</span><br><span class="line">  volumes:</span><br><span class="line">    - name: cache-volume</span><br><span class="line">      emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f preset.yaml</span><br><span class="line">$ kubectl create -f pod.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod website -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: website</span><br><span class="line">  labels:</span><br><span class="line">    app: website</span><br><span class="line">    role: frontend</span><br><span class="line">  annotations:</span><br><span class="line">    podpreset.admission.kubernetes.io/podpreset-allow-database: &quot;resource version&quot;</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: website</span><br><span class="line">      image: nginx</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: /cache</span><br><span class="line">          name: cache-volume</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">      env:</span><br><span class="line">        - name: DB_PORT</span><br><span class="line">          value: &quot;6379&quot;</span><br><span class="line">  volumes:</span><br><span class="line">    - name: cache-volume</span><br><span class="line">      emptyDir: &#123;&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Container-属性"><a href="#Container-属性" class="headerlink" title="Container 属性"></a>Container 属性</h2><h3 id="ImagePullPolicy"><a href="#ImagePullPolicy" class="headerlink" title="ImagePullPolicy"></a>ImagePullPolicy</h3><p>它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。</p>
<p>ImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。而如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。</p>
<h3 id="Lifecycle"><a href="#Lifecycle" class="headerlink" title="Lifecycle"></a>Lifecycle</h3><p>它定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: lifecycle-demo</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: lifecycle-demo-container</span><br><span class="line">    image: nginx</span><br><span class="line">    lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;]</span><br><span class="line">      preStop:</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]</span><br></pre></td></tr></table></figure>
<h2 id="作业副本与水平扩展"><a href="#作业副本与水平扩展" class="headerlink" title="作业副本与水平扩展"></a>作业副本与水平扩展</h2><h3 id="ReplicaSet-结构"><a href="#ReplicaSet-结构" class="headerlink" title="ReplicaSet 结构"></a>ReplicaSet 结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: ReplicaSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-set</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br></pre></td></tr></table></figure>
<h3 id="Deployment结构"><a href="#Deployment结构" class="headerlink" title="Deployment结构"></a>Deployment结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure>
<h3 id="Deployment和ReplicaSet关系"><a href="#Deployment和ReplicaSet关系" class="headerlink" title="Deployment和ReplicaSet关系"></a>Deployment和ReplicaSet关系</h3><p><img src="/2020/02/21/k8s-command/images/image-20200222120521800.png" alt="image-20200222120521800"></p>
<h3 id="部署deployment"><a href="#部署deployment" class="headerlink" title="部署deployment"></a>部署deployment</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f nginx-deployment.yaml --record</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get deployments</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   3         0         0            0           1s</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout status deployment/nginx-deployment</span><br><span class="line">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">deployment.apps/nginx-deployment successfully rolled out</span><br></pre></td></tr></table></figure>
<h3 id="查看replicaSet状态"><a href="#查看replicaSet状态" class="headerlink" title="查看replicaSet状态"></a>查看replicaSet状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-3167673210   3         3         3       20s</span><br></pre></td></tr></table></figure>
<h3 id="edit-deployment"><a href="#edit-deployment" class="headerlink" title="edit deployment"></a>edit deployment</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit deployment/nginx-deployment</span><br><span class="line">... </span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.9.1 # 1.7.9 -&gt; 1.9.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">...</span><br><span class="line">deployment.extensions/nginx-deployment edited</span><br></pre></td></tr></table></figure>
<p>查看deployment 状态变化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout status deployment/nginx-deployment</span><br><span class="line">Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">deployment.extensions/nginx-deployment successfully rolled out</span><br></pre></td></tr></table></figure>
<p>也可以查看deployment的event</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe deployment nginx-deployment</span><br><span class="line">...</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age   From                   Message</span><br><span class="line">  ----    ------             ----  ----                   -------</span><br><span class="line">...</span><br><span class="line">  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 1</span><br><span class="line">  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 2</span><br><span class="line">  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 2</span><br><span class="line">  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 1</span><br><span class="line">  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 3</span><br><span class="line">  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 0</span><br></pre></td></tr></table></figure>
<p>在这个“滚动更新”过程完成之后，你可以查看一下新、旧两个 ReplicaSet 的最终状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-1764197365   3         3         3       6s</span><br><span class="line">nginx-deployment-3167673210   0         0         0       30s</span><br></pre></td></tr></table></figure>
<h3 id="scale"><a href="#scale" class="headerlink" title="scale"></a>scale</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale deployment nginx-deployment --replicas=4</span><br><span class="line">deployment.apps/nginx-deployment scaled</span><br></pre></td></tr></table></figure>
<h3 id="RollingUpdateStrategy"><a href="#RollingUpdateStrategy" class="headerlink" title="RollingUpdateStrategy"></a>RollingUpdateStrategy</h3><p>为了进一步保证服务的连续性，Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">...</span><br><span class="line">  strategy:</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 1</span><br></pre></td></tr></table></figure>
<p>maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。同时，这两个配置还可以用前面我们介绍的百分比形式来表示。</p>
<p><img src="/2020/02/21/k8s-command/images/image-20200222121522402.png" alt="image-20200222121522402"></p>
<p>kubectl set image 的指令，直接修改 nginx-deployment 所使用的镜像。这个命令的好处就是，你可以不用像 kubectl edit 那样需要打开编辑器。不过这一次，我把这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91</span><br><span class="line">deployment.extensions/nginx-deployment image updated</span><br></pre></td></tr></table></figure>
<p>这时来检查下replicaSet的状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME                          DESIRED   CURRENT   READY   AGE</span><br><span class="line">nginx-deployment-1764197365   2         2         2       24s</span><br><span class="line">nginx-deployment-3167673210   0         0         0       35s</span><br><span class="line">nginx-deployment-2156724341   2         2         0       7s</span><br></pre></td></tr></table></figure>
<h3 id="回滚到上一个版本"><a href="#回滚到上一个版本" class="headerlink" title="回滚到上一个版本"></a>回滚到上一个版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout undo deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment</span><br></pre></td></tr></table></figure>
<h3 id="回滚到指定版本"><a href="#回滚到指定版本" class="headerlink" title="回滚到指定版本"></a>回滚到指定版本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout history deployment/nginx-deployment</span><br><span class="line">deployments &quot;nginx-deployment&quot;</span><br><span class="line">REVISION    CHANGE-CAUSE</span><br><span class="line">1           kubectl create -f nginx-deployment.yaml --record</span><br><span class="line">2           kubectl edit deployment/nginx-deployment</span><br><span class="line">3           kubectl set image deployment/nginx-deployment nginx=nginx:1.91</span><br></pre></td></tr></table></figure>
<p>当然，你还可以通过这个 kubectl rollout history 指令，看到每个版本对应的 Deployment 的 API 对象的细节，具体命令如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout history deployment/nginx-deployment --revision=2</span><br></pre></td></tr></table></figure>
<p>回滚到指定版本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout undo deployment/nginx-deployment --to-revision=2</span><br><span class="line">deployment.extensions/nginx-deployment</span><br></pre></td></tr></table></figure>
<h3 id="多次修改deployment只生成一个rs"><a href="#多次修改deployment只生成一个rs" class="headerlink" title="多次修改deployment只生成一个rs"></a>多次修改deployment只生成一个rs</h3><p>你可能已经想到了一个问题：我们对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象，是不是有些多余，甚至浪费资源呢？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout pause deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment paused</span><br></pre></td></tr></table></figure>
<p>这个 kubectl rollout pause 的作用，是让这个 Deployment 进入了一个“暂停”状态。</p>
<p>所以接下来，你就可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。</p>
<p>由于此时 Deployment 正处于“暂停”状态，所以我们对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>而等到我们对 Deployment 修改操作都完成之后，只需要再执行一条 kubectl rollout resume 指令，就可以把这个 Deployment“恢复”回来，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout resume deployment/nginx-deployment</span><br><span class="line">deployment.extensions/nginx-deployment resumed</span><br></pre></td></tr></table></figure>
<p>查看rs状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get rs</span><br><span class="line">NAME               DESIRED   CURRENT   READY     AGE</span><br><span class="line">nginx-1764197365   0         0         0         2m</span><br><span class="line">nginx-3196763511   3         3         3         28s</span><br></pre></td></tr></table></figure>
<h3 id="控制这些“历史”ReplicaSet-的数量呢？"><a href="#控制这些“历史”ReplicaSet-的数量呢？" class="headerlink" title="控制这些“历史”ReplicaSet 的数量呢？"></a>控制这些“历史”ReplicaSet 的数量呢？</h3><p>很简单，Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了。</p>
<h2 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h2><h3 id="拓扑状态"><a href="#拓扑状态" class="headerlink" title="拓扑状态"></a>拓扑状态</h3><p>svc.yaml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    name: web</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br></pre></td></tr></table></figure>
<p>当你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">&lt;pod-name&gt;.&lt;svc-name&gt;.&lt;namespace&gt;.svc.cluster.local</span><br></pre></td></tr></table></figure>
<p>statefulset.yaml</p>
<p>这个 YAML 文件，和我们在前面文章中用到的 nginx-deployment 的唯一区别，就是多了一个 serviceName=nginx 字段。这个字段的作用，就是告诉 StatefulSet 控制器，在执行控制循环（Control Loop）的时候，请使用 nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.9.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br></pre></td></tr></table></figure>
<p>所以，当你通过 kubectl create 创建了上面这个 Service 和 StatefulSet 之后，就会看到如下两个对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f svc.yaml</span><br><span class="line">$ kubectl get service nginx</span><br><span class="line">NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">nginx     ClusterIP    None         &lt;none&gt;        80/TCP    10s</span><br><span class="line"></span><br><span class="line">$ kubectl create -f statefulset.yaml</span><br><span class="line">$ kubectl get statefulset web</span><br><span class="line">NAME      DESIRED   CURRENT   AGE</span><br><span class="line">web       2         1         19s</span><br></pre></td></tr></table></figure>
<p>这时候，如果你手比较快的话，还可以通过 kubectl 的 -w 参数，即：Watch 功能，实时查看 StatefulSet 创建两个有状态实例的过程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods -w -l app=nginx</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE</span><br><span class="line">web-0     0/1       Pending   0          0s</span><br><span class="line">web-0     0/1       Pending   0         0s</span><br><span class="line">web-0     0/1       ContainerCreating   0         0s</span><br><span class="line">web-0     1/1       Running   0         19s</span><br><span class="line">web-1     0/1       Pending   0         0s</span><br><span class="line">web-1     0/1       Pending   0         0s</span><br><span class="line">web-1     0/1       ContainerCreating   0         0s</span><br><span class="line">web-1     1/1       Running   0         20s</span><br></pre></td></tr></table></figure>
<p>我们使用 kubectl exec 命令进入到容器中查看它们的 hostname：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec web-0 -- sh -c &apos;hostname&apos;</span><br><span class="line">web-0</span><br><span class="line">$ kubectl exec web-1 -- sh -c &apos;hostname&apos;</span><br><span class="line">web-1</span><br></pre></td></tr></table></figure>
<p>通过这条命令，我们启动了一个一次性的 Pod，因为–rm 意味着 Pod 退出后就会被删除掉。然后，在这个 Pod 的容器里面，我们尝试用 nslookup 命令，解析一下 Pod 对应的 Headless Service：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh</span><br><span class="line">$ nslookup web-0.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-0.nginx</span><br><span class="line">Address 1: 10.244.1.7</span><br><span class="line"></span><br><span class="line">$ nslookup web-1.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-1.nginx</span><br><span class="line">Address 1: 10.244.2.7</span><br></pre></td></tr></table></figure>
<p>删掉pod看看会出现什么情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete pod -l app=nginx</span><br><span class="line">pod &quot;web-0&quot; deleted</span><br><span class="line">pod &quot;web-1&quot; deleted</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pod -w -l app=nginx</span><br><span class="line">NAME      READY     STATUS              RESTARTS   AGE</span><br><span class="line">web-0     0/1       ContainerCreating   0          0s</span><br><span class="line">NAME      READY     STATUS    RESTARTS   AGE</span><br><span class="line">web-0     1/1       Running   0          2s</span><br><span class="line">web-1     0/1       Pending   0         0s</span><br><span class="line">web-1     0/1       ContainerCreating   0         0s</span><br><span class="line">web-1     1/1       Running   0         32s</span><br></pre></td></tr></table></figure>
<p>可以看到，当我们把这两个 Pod 删除之后，Kubernetes 会按照原先编号的顺序，创建出了两个新的 Pod。并且，Kubernetes 依然为它们分配了与原来相同的“网络身份”：web-0.nginx 和 web-1.nginx。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh </span><br><span class="line">$ nslookup web-0.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-0.nginx</span><br><span class="line">Address 1: 10.244.1.8</span><br><span class="line"></span><br><span class="line">$ nslookup web-1.nginx</span><br><span class="line">Server:    10.0.0.10</span><br><span class="line">Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      web-1.nginx</span><br><span class="line">Address 1: 10.244.2.8</span><br></pre></td></tr></table></figure>
<p>不过，相信你也已经注意到了，尽管 web-0.nginx 这条记录本身不会变，但它解析到的 Pod 的 IP 地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式，而绝不应该直接访问这些 Pod 的 IP 地址。</p>
<h3 id="存储状态"><a href="#存储状态" class="headerlink" title="存储状态"></a>存储状态</h3><h4 id="PVC"><a href="#PVC" class="headerlink" title="PVC"></a>PVC</h4><p>如果你还是不太理解 PVC 的话，可以先记住这样一个结论：PVC 其实就是一种特殊的 Volume。只不过一个 PVC 具体是什么类型的 Volume，要在跟某个 PV 绑定之后才知道。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure>
<h4 id="在Pod中声明这个PVC"><a href="#在Pod中声明这个PVC" class="headerlink" title="在Pod中声明这个PVC"></a>在Pod中声明这个PVC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: pv-container</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: &quot;http-server&quot;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: pv-storage</span><br><span class="line">  volumes:</span><br><span class="line">    - name: pv-storage</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">        claimName: pv-claim</span><br></pre></td></tr></table></figure>
<h4 id="PV"><a href="#PV" class="headerlink" title="PV"></a>PV</h4><p>可以看到，在这个 Pod 的 Volumes 定义中，我们只需要声明它的类型是 persistentVolumeClaim，然后指定 PVC 的名字，而完全不必关心 Volume 本身的定义。</p>
<p>这时候，只要我们创建这个 PVC 对象，Kubernetes 就会自动为它绑定一个符合条件的 Volume。可是，这些符合条件的 Volume 又是从哪里来的呢？</p>
<p>答案是，它们来自于由运维人员维护的 PV（Persistent Volume）对象。接下来，我们一起看一个常见的 PV 对象的 YAML 文件。</p>
<p>pv.yaml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-volume</span><br><span class="line">  labels:</span><br><span class="line">    type: local</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  rbd:</span><br><span class="line">    monitors:</span><br><span class="line">    - &apos;rook-ceph-mon-a-698fc4d55f-q4x8d&apos;</span><br><span class="line">    - &apos;rook-ceph-mon-b-7cf9c7bcb7-wjld6&apos;</span><br><span class="line">    - &apos;rook-ceph-mon-c-5fb4dfb454-gnrll&apos;</span><br><span class="line">    pool: kube</span><br><span class="line">    image: foo</span><br><span class="line">    fsType: ext4</span><br><span class="line">    readOnly: true</span><br><span class="line">    user: admin</span><br><span class="line">    keyring: /etc/ceph/keyring</span><br></pre></td></tr></table></figure>
<p>–注monitors –修改为 kubectl get pods -n rook-ceph 查看 rook-ceph-mon- 开头的Pod id </p>
<p>所以，Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。</p>
<h4 id="StatefulSet-集成PVC"><a href="#StatefulSet-集成PVC" class="headerlink" title="StatefulSet 集成PVC"></a>StatefulSet 集成PVC</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.9.1</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: www</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: www</span><br><span class="line">    spec:</span><br><span class="line">      accessModes:</span><br><span class="line">      - ReadWriteOnce</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 1Gi</span><br></pre></td></tr></table></figure>
<p>create</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f pv.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f statefulset.yaml</span><br><span class="line">$ kubectl get pvc -l app=nginx</span><br><span class="line">NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE</span><br><span class="line">www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s</span><br><span class="line">www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ for i in 0 1; do kubectl exec web-$i -- sh -c &apos;echo hello $(hostname) &gt; /usr/share/nginx/html/index.html&apos;; done</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done</span><br><span class="line">hello web-0</span><br><span class="line">hello web-1</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 删除pod</span><br><span class="line">$ kubectl delete pod -l app=nginx</span><br><span class="line">pod &quot;web-0&quot; deleted</span><br><span class="line">pod &quot;web-1&quot; deleted</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 在被重新创建出来的Pod容器里访问http://localhost</span><br><span class="line">$ kubectl exec -it web-0 -- curl localhost</span><br><span class="line">hello web-0</span><br></pre></td></tr></table></figure>
<h2 id="daemonSet"><a href="#daemonSet" class="headerlink" title="daemonSet"></a>daemonSet</h2><h3 id="滚动更新"><a href="#滚动更新" class="headerlink" title="滚动更新"></a>滚动更新</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl patch statefulset mysql --type=&apos;json&apos; -p=&apos;[&#123;&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/template/spec/containers/0/image&quot;, &quot;value&quot;:&quot;mysql:5.7.23&quot;&#125;]&apos;</span><br><span class="line">statefulset.apps/mysql patched</span><br></pre></td></tr></table></figure>
<h3 id="灰度发布"><a href="#灰度发布" class="headerlink" title="灰度发布"></a>灰度发布</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl patch statefulset mysql -p &apos;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;type&quot;:&quot;RollingUpdate&quot;,&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:2&#125;&#125;&#125;&#125;&apos;</span><br><span class="line">statefulset.apps/mysql patched</span><br></pre></td></tr></table></figure>
<p>这样，我就指定了当 Pod 模板发生变化的时候，比如 MySQL 镜像更新到 5.7.23，那么只有序号大于或者等于 2 的 Pod 会被更新到这个版本。并且，如果你删除或者重启了序号小于 2 的 Pod，等它再次启动后，也会保持原先的 5.7.2 版本，绝不会被升级到 5.7.23 版本。</p>
<h3 id="deamonSet结构"><a href="#deamonSet结构" class="headerlink" title="deamonSet结构"></a>deamonSet结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: fluentd-elasticsearch</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: fluentd-logging</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: fluentd-elasticsearch</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: fluentd-elasticsearch</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node-role.kubernetes.io/master</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      containers:</span><br><span class="line">      - name: fluentd-elasticsearch</span><br><span class="line">        image: k8s.gcr.io/fluentd-elasticsearch:1.20</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            memory: 200Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 200Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: varlog</span><br><span class="line">          mountPath: /var/log</span><br><span class="line">        - name: varlibdockercontainers</span><br><span class="line">          mountPath: /var/lib/docker/containers</span><br><span class="line">          readOnly: true</span><br><span class="line">      terminationGracePeriodSeconds: 30</span><br><span class="line">      volumes:</span><br><span class="line">      - name: varlog</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/log</span><br><span class="line">      - name: varlibdockercontainers</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /var/lib/docker/containers</span><br></pre></td></tr></table></figure>
<h3 id="nodeAffinity"><a href="#nodeAffinity" class="headerlink" title="nodeAffinity"></a>nodeAffinity</h3><p>在 Kubernetes 项目里，nodeSelector 其实已经是一个将要被废弃的字段了。因为，现在有了一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: with-node-affinity</span><br><span class="line">spec:</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: metadata.name</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">            - node-geektime</span><br></pre></td></tr></table></figure>
<h3 id="Toleration"><a href="#Toleration" class="headerlink" title="Toleration"></a>Toleration</h3><p>DaemonSet 的“过人之处”，其实就是依靠 Toleration 实现的。假如当前 DaemonSet 管理的，是一个网络插件的 Agent Pod，那么你就必须在这个 DaemonSet 的 YAML 文件里，给它的 Pod 模板加上一个能够“容忍”node.kubernetes.io/network-unavailable“污点”的 Toleration。正如下面这个例子所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">...</span><br><span class="line">template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: network-plugin-agent</span><br><span class="line">    spec:</span><br><span class="line">      tolerations:</span><br><span class="line">      - key: node.kubernetes.io/network-unavailable</span><br><span class="line">        operator: Exists</span><br><span class="line">        effect: NoSchedule</span><br></pre></td></tr></table></figure>
<p>daemonSet 如何确保能在在mater上部署</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">tolerations:</span><br><span class="line">- key: node-role.kubernetes.io/master</span><br><span class="line">  effect: NoSchedule</span><br></pre></td></tr></table></figure>
<h3 id="创建deamonSet"><a href="#创建deamonSet" class="headerlink" title="创建deamonSet"></a>创建deamonSet</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f fluentd-elasticsearch.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pod -n kube-system -l name=fluentd-elasticsearch</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">fluentd-elasticsearch-dqfv9   1/1       Running   0          53m</span><br><span class="line">fluentd-elasticsearch-pf9z5   1/1       Running   0          53m</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get ds -n kube-system fluentd-elasticsearch</span><br><span class="line">NAME                    DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE</span><br><span class="line">fluentd-elasticsearch   2         2         2         2            2           &lt;none&gt;          1h</span><br></pre></td></tr></table></figure>
<h3 id="回滚daemonSet"><a href="#回滚daemonSet" class="headerlink" title="回滚daemonSet"></a>回滚daemonSet</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system</span><br><span class="line">daemonsets &quot;fluentd-elasticsearch&quot;</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br></pre></td></tr></table></figure>
<p>滚动更新</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record -n=kube-system</span><br></pre></td></tr></table></figure>
<p>查看滚动更新状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system</span><br><span class="line">Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 0 out of 2 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 0 out of 2 new pods have been updated...</span><br><span class="line">Waiting for daemon set &quot;fluentd-elasticsearch&quot; rollout to finish: 1 of 2 updated pods are available...</span><br><span class="line">daemon set &quot;fluentd-elasticsearch&quot; successfully rolled out</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system</span><br><span class="line">daemonsets &quot;fluentd-elasticsearch&quot;</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         &lt;none&gt;</span><br><span class="line">2         kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --namespace=kube-system --record=true</span><br></pre></td></tr></table></figure>
<h3 id="ControllerRevision对象"><a href="#ControllerRevision对象" class="headerlink" title="ControllerRevision对象"></a>ControllerRevision对象</h3><p>而我在前面的文章中讲解 Deployment 对象的时候，曾经提到过，Deployment 管理这些版本，靠的是“一个版本对应一个 ReplicaSet 对象”。</p>
<p>可是，DaemonSet 控制器操作的直接就是 Pod，不可能有 ReplicaSet 这样的对象参与其中。那么，它的这些版本又是如何维护的呢？所谓，一切皆对象！在 Kubernetes 项目中，任何你觉得需要记录下来的状态，都可以被用 API 对象的方式实现。当然，“版本”也不例外。Kubernetes v1.7 之后添加了一个 API 对象，名叫 ControllerRevision</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get controllerrevision -n kube-system -l name=fluentd-elasticsearch</span><br><span class="line">NAME                               CONTROLLER                             REVISION   AGE</span><br><span class="line">fluentd-elasticsearch-64dc6799c9   daemonset.apps/fluentd-elasticsearch   2          1h</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 -n kube-system</span><br><span class="line">Name:         fluentd-elasticsearch-64dc6799c9</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Labels:       controller-revision-hash=2087235575</span><br><span class="line">              name=fluentd-elasticsearch</span><br><span class="line">Annotations:  deprecated.daemonset.template.generation=2</span><br><span class="line">              kubernetes.io/change-cause=kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record=true --namespace=kube-system</span><br><span class="line">API Version:  apps/v1</span><br><span class="line">Data:</span><br><span class="line">  Spec:</span><br><span class="line">    Template:</span><br><span class="line">      $ Patch:  replace</span><br><span class="line">      Metadata:</span><br><span class="line">        Creation Timestamp:  &lt;nil&gt;</span><br><span class="line">        Labels:</span><br><span class="line">          Name:  fluentd-elasticsearch</span><br><span class="line">      Spec:</span><br><span class="line">        Containers:</span><br><span class="line">          Image:              k8s.gcr.io/fluentd-elasticsearch:v2.2.0</span><br><span class="line">          Image Pull Policy:  IfNotPresent</span><br><span class="line">          Name:               fluentd-elasticsearch</span><br><span class="line">...</span><br><span class="line">Revision:                  2</span><br><span class="line">Events:                    &lt;none&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision=1 -n kube-system</span><br><span class="line">daemonset.extensions/fluentd-elasticsearch rolled back</span><br></pre></td></tr></table></figure>
<p>执行完这次回滚完成后，你会发现，DaemonSet 的 Revision 并不会从 Revision=2 退回到 1，而是会增加成 Revision=3。这是因为，一个新的 ControllerRevision 被创建了出来。</p>
<h2 id="Job"><a href="#Job" class="headerlink" title="Job"></a>Job</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: pi</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: pi</span><br><span class="line">        image: resouer/ubuntu-bc </span><br><span class="line">        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo &apos;scale=10000; 4*a(1)&apos; | bc -l &quot;]</span><br><span class="line">      restartPolicy: Never</span><br><span class="line">  backoffLimit: 4</span><br></pre></td></tr></table></figure>
<p>job里的restartPolicy只有两种选择：</p>
<p>restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod</p>
<p>restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">spec:</span><br><span class="line"> backoffLimit: 5 #最多失败几次就不在创建Pod默认是6</span><br><span class="line"> activeDeadlineSeconds: 100 #最长运行时间</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spec: </span><br><span class="line">	parallelism: 2 #同一时间最多有几个Pod执行 </span><br><span class="line">	completions: 4 #总共执行多少个Pod</span><br></pre></td></tr></table></figure>
<h2 id="CronJob"><a href="#CronJob" class="headerlink" title="CronJob"></a>CronJob</h2><h3 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: hello</span><br><span class="line">spec:</span><br><span class="line">  schedule: &quot;*/1 * * * *&quot;</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: hello</span><br><span class="line">            image: busybox</span><br><span class="line">            args:</span><br><span class="line">            - /bin/sh</span><br><span class="line">            - -c</span><br><span class="line">            - date; echo Hello from the Kubernetes cluster</span><br><span class="line">          restartPolicy: OnFailure</span><br></pre></td></tr></table></figure>
<p>这个 Cron 表达式里 <em>/1 中的 </em> 表示从 0 开始，/ 表示“每”，1 表示偏移量。</p>
<p>所以，它的意思就是：从 0 开始，每 1 个时间单位执行一次。那么，时间单位又是什么呢？Cron 表达式中的五个部分分别代表：</p>
<p>分钟、小时、日、月、星期。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f ./cronjob.yaml</span><br><span class="line">cronjob &quot;hello&quot; created</span><br><span class="line"></span><br><span class="line"># 一分钟后</span><br><span class="line">$ kubectl get jobs</span><br><span class="line">NAME               DESIRED   SUCCESSFUL   AGE</span><br><span class="line">hello-4111706356   1         1         2s</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get cronjob hello</span><br><span class="line">NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE</span><br><span class="line">hello     */1 * * * *   False     0         Thu, 6 Sep 2018 14:34:00 -070</span><br></pre></td></tr></table></figure>
<h3 id="spec-concurrencyPolicy"><a href="#spec-concurrencyPolicy" class="headerlink" title="spec.concurrencyPolicy"></a>spec.concurrencyPolicy</h3><p>需要注意的是，由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如：</p>
<p>concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在；</p>
<p>concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过；</p>
<p>concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。</p>
<h3 id="spec-startingDeadlineSeconds"><a href="#spec-startingDeadlineSeconds" class="headerlink" title="spec.startingDeadlineSeconds"></a>spec.startingDeadlineSeconds</h3><p>而如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job。这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。</p>
<h2 id="Initializer（Dynamic-Admission-Control）"><a href="#Initializer（Dynamic-Admission-Control）" class="headerlink" title="Initializer（Dynamic Admission Control）"></a>Initializer（Dynamic Admission Control）</h2><h3 id="istio架构图"><a href="#istio架构图" class="headerlink" title="istio架构图"></a>istio架构图</h3><p><img src="/2020/02/21/k8s-command/images/image-20200225192309030.png" alt="image-20200225192309030" style="zoom: 33%;"></p>
<p><strong>Istio 最根本的组件，是运行在每一个应用 Pod 里的 Envoy 容器</strong></p>
<p>这个 Envoy 项目是 Lyft 公司推出的一个高性能 C++ 网络代理，也是 Lyft 公司对 Istio 项目的唯一贡献。</p>
<p>而 Istio 项目，则把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。我们知道，Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。</p>
<p>这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。</p>
<h3 id="Dynamic-Admission-Control"><a href="#Dynamic-Admission-Control" class="headerlink" title="Dynamic Admission Control"></a>Dynamic Admission Control</h3><p>Istio 项目明明需要在每个 Pod 里安装一个 Envoy 容器，又怎么能做到“无感”的呢？</p>
<p>实际上，Istio 项目使用的，是 Kubernetes 中的一个非常重要的功能，叫作 Dynamic Admission Control。</p>
<p>在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，总有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。所以，Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>举个例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp-container</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo Hello Kubernetes! &amp;&amp; sleep 3600&apos;]</span><br></pre></td></tr></table></figure>
<p>Istio 项目要做的，就是在这个 Pod YAML 被提交给 Kubernetes 之后，在它对应的 API 对象里自动加上 Envoy 容器的配置，使这个对象变成如下所示的样子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: myapp-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: myapp-container</span><br><span class="line">    image: busybox</span><br><span class="line">    command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo Hello Kubernetes! &amp;&amp; sleep 3600&apos;]</span><br><span class="line">  - name: envoy</span><br><span class="line">    image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1</span><br><span class="line">    command: [&quot;/usr/local/bin/envoy&quot;]</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>如何做到的呢</p>
<p>首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。这个 ConfigMap（名叫：envoy-initializer）的定义如下所示：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: envoy-initializer</span><br><span class="line">data:</span><br><span class="line">  config: |</span><br><span class="line">    containers:</span><br><span class="line">      - name: envoy</span><br><span class="line">        image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1</span><br><span class="line">        command: [&quot;/usr/local/bin/envoy&quot;]</span><br><span class="line">        args:</span><br><span class="line">          - &quot;--concurrency 4&quot;</span><br><span class="line">          - &quot;--config-path /etc/envoy/envoy.json&quot;</span><br><span class="line">          - &quot;--mode serve&quot;</span><br><span class="line">        ports:</span><br><span class="line">          - containerPort: 80</span><br><span class="line">            protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: &quot;1000m&quot;</span><br><span class="line">            memory: &quot;512Mi&quot;</span><br><span class="line">          requests:</span><br><span class="line">            cpu: &quot;100m&quot;</span><br><span class="line">            memory: &quot;64Mi&quot;</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: envoy-conf</span><br><span class="line">            mountPath: /etc/envoy</span><br><span class="line">    volumes:</span><br><span class="line">      - name: envoy-conf</span><br><span class="line">        configMap:</span><br><span class="line">          name: envoy</span><br></pre></td></tr></table></figure>
<p>接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。这个 Pod 的定义非常简单，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: envoy-initializer</span><br><span class="line">  name: envoy-initializer</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: envoy-initializer</span><br><span class="line">      image: envoy-initializer:0.0.1</span><br><span class="line">      imagePullPolicy: Always</span><br></pre></td></tr></table></figure>
<p>这个pod会做什么工作呢</p>
<ul>
<li>如果这个 Pod 里面已经添加过 Envoy 容器，那么就“放过”这个 Pod，进入下一个检查周期。</li>
<li>而如果还没有添加过 Envoy 容器的话，它就要进行 Initialize 操作了，即：修改该 Pod 的 API 对象。<br>Initializer 的代码拿到config-map里的data数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求。<br>这样，一个用户提交的 Pod 对象里，就会被自动加上 Envoy 容器相关的字段。</li>
</ul>
<h3 id="InitializerConfiguration"><a href="#InitializerConfiguration" class="headerlink" title="InitializerConfiguration"></a>InitializerConfiguration</h3><p>当然，Kubernetes 还允许你通过配置，来指定要对什么样的资源进行这个 Initialize 操作，比如下面这个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: admissionregistration.k8s.io/v1alpha1</span><br><span class="line">kind: InitializerConfiguration</span><br><span class="line">metadata:</span><br><span class="line">  name: envoy-config</span><br><span class="line">initializers:</span><br><span class="line">  // 这个名字必须至少包括两个 &quot;.&quot;</span><br><span class="line">  - name: envoy.initializer.kubernetes.io</span><br><span class="line">    rules:</span><br><span class="line">      - apiGroups:</span><br><span class="line">          - &quot;&quot; // 前面说过， &quot;&quot;就是core API Group的意思</span><br><span class="line">        apiVersions:</span><br><span class="line">          - v1</span><br><span class="line">        resources:</span><br><span class="line">          - pods</span><br></pre></td></tr></table></figure>
<p>这个配置，就意味着 Kubernetes 要对所有的 Pod 进行这个 Initialize 操作，并且，我们指定了负责这个操作的 Initializer，名叫：envoy-initializer。</p>
<p>而一旦这个 InitializerConfiguration 被创建，Kubernetes 就会把这个 Initializer 的名字，加在所有新创建的 Pod 的 Metadata 上，格式如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  initializers:</span><br><span class="line">    pending:</span><br><span class="line">      - name: envoy.initializer.kubernetes.io</span><br><span class="line">  name: myapp-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: myapp</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>此外，除了上面的配置方法，你还可以在具体的 Pod 的 Annotation 里添加一个如下所示的字段，从而声明要使用某个 Initializer：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata</span><br><span class="line">  annotations:</span><br><span class="line">    &quot;initializer.kubernetes.io/envoy&quot;: &quot;true&quot;</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>在这个 Pod 里，我们添加了一个 Annotation，写明： initializer.kubernetes.io/envoy=true。这样，就会使用到我们前面所定义的 envoy-initializer 了。</p>
<h2 id="API对象"><a href="#API对象" class="headerlink" title="API对象"></a>API对象</h2><h3 id="API对象结构"><a href="#API对象结构" class="headerlink" title="API对象结构"></a>API对象结构</h3><p><img src="/2020/02/21/k8s-command/images/image-20200226080846682.png" alt="image-20200226080846682"></p>
<h3 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h3><p><img src="/2020/02/21/k8s-command/images/image-20200226080929620.png" alt="image-20200226080929620"></p>
<h3 id="自定义API资源"><a href="#自定义API资源" class="headerlink" title="自定义API资源"></a>自定义API资源</h3><h4 id="CR（Custom-Resource）"><a href="#CR（Custom-Resource）" class="headerlink" title="CR（Custom Resource）"></a>CR（Custom Resource）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: samplecrd.k8s.io/v1</span><br><span class="line">kind: Network</span><br><span class="line">metadata:</span><br><span class="line">  name: example-network</span><br><span class="line">spec:</span><br><span class="line">  cidr: &quot;192.168.0.0/16&quot;</span><br><span class="line">  gateway: &quot;192.168.0.1&quot;</span><br></pre></td></tr></table></figure>
<h4 id="CRD（Custom-Resource-Definition）"><a href="#CRD（Custom-Resource-Definition）" class="headerlink" title="CRD（Custom Resource Definition）"></a>CRD（Custom Resource Definition）</h4><p>这就是一个 Network API 资源类型的 API 部分的宏观定义了。这就等同于告诉了计算机：“兔子是哺乳动物”。所以这时候，Kubernetes 就能够认识和处理所有声明了 API 类型是“samplecrd.k8s.io/v1/network”的 YAML 文件了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apiextensions.k8s.io/v1beta1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: networks.samplecrd.k8s.io</span><br><span class="line">spec:</span><br><span class="line">  group: samplecrd.k8s.io</span><br><span class="line">  version: v1</span><br><span class="line">  names:</span><br><span class="line">    kind: Network</span><br><span class="line">    plural: networks</span><br><span class="line">  scope: Namespaced</span><br></pre></td></tr></table></figure>
<p>接下来，我还需要让 Kubernetes“认识”这种 YAML 文件里描述的“网络”部分，比如“cidr”（网段），“gateway”（网关）这些字段的含义。这就相当于我要告诉计算机：“兔子有长耳朵和三瓣嘴”。</p>
<h4 id="编写代码"><a href="#编写代码" class="headerlink" title="编写代码"></a>编写代码</h4><h5 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ tree $GOPATH/src/github.com/&lt;your-name&gt;/k8s-controller-custom-resource</span><br><span class="line">.</span><br><span class="line">├── controller.go</span><br><span class="line">├── crd</span><br><span class="line">│   └── network.yaml</span><br><span class="line">├── example</span><br><span class="line">│   └── example-network.yaml</span><br><span class="line">├── main.go</span><br><span class="line">└── pkg</span><br><span class="line">    └── apis</span><br><span class="line">        └── samplecrd</span><br><span class="line">            ├── register.go</span><br><span class="line">            └── v1</span><br><span class="line">                ├── doc.go</span><br><span class="line">                ├── register.go</span><br><span class="line">                └── types.go</span><br></pre></td></tr></table></figure>
<p>其中，pkg/apis/samplecrd 就是 API 组的名字，v1 是版本，而 v1 下面的 types.go 文件里，则定义了 Network 对象的完整描述。</p>
<h5 id="pkg-apis-samplecrd-register-go"><a href="#pkg-apis-samplecrd-register-go" class="headerlink" title="pkg/apis/samplecrd/register.go"></a>pkg/apis/samplecrd/register.go</h5><p>register.go,用来放置后面要用到的全局变量。这个文件的内容如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">package samplecrd</span><br><span class="line"></span><br><span class="line">const (</span><br><span class="line"> GroupName = &quot;samplecrd.k8s.io&quot;</span><br><span class="line"> Version   = &quot;v1&quot;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h5 id="pkg-apis-samplecrd-v1-doc-go"><a href="#pkg-apis-samplecrd-v1-doc-go" class="headerlink" title="pkg/apis/samplecrd/v1/doc.go"></a>pkg/apis/samplecrd/v1/doc.go</h5><p>doc.go 文件（Golang 的文档源文件）。这个文件里的内容如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// +k8s:deepcopy-gen=package</span><br><span class="line"></span><br><span class="line">// +groupName=samplecrd.k8s.io</span><br><span class="line">package v1</span><br></pre></td></tr></table></figure>
<p>其中，+k8s:deepcopy-gen=package 意思是，请为整个 v1 包里的所有类型定义自动生成 DeepCopy 方法；而+groupName=samplecrd.k8s.io，则定义了这个包对应的 API 组的名字。可以看到，这些定义在 doc.go 文件的注释，起到的是全局的代码生成控制的作用，所以也被称为 Global Tags。</p>
<h5 id="pkg-apis-samplecrd-v1-types-go"><a href="#pkg-apis-samplecrd-v1-types-go" class="headerlink" title="pkg/apis/samplecrd/v1/types.go"></a>pkg/apis/samplecrd/v1/types.go</h5><p>接下来，我需要添加 types.go 文件。顾名思义，它的作用就是定义一个 Network 类型到底有哪些字段（比如，spec 字段里的内容）。这个文件的主要内容如下所示：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> v1</span><br><span class="line">...</span><br><span class="line"><span class="comment">// +genclient</span></span><br><span class="line"><span class="comment">// +genclient:noStatus</span></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Network describes a Network resource</span></span><br><span class="line"><span class="keyword">type</span> Network <span class="keyword">struct</span> &#123;</span><br><span class="line"> <span class="comment">// TypeMeta is the metadata for the resource, like kind and apiversion</span></span><br><span class="line"> metav1.TypeMeta <span class="string">`json:",inline"`</span></span><br><span class="line"> <span class="comment">// ObjectMeta contains the metadata for the particular object, including</span></span><br><span class="line"> <span class="comment">// things like...</span></span><br><span class="line"> <span class="comment">//  - name</span></span><br><span class="line"> <span class="comment">//  - namespace</span></span><br><span class="line"> <span class="comment">//  - self link</span></span><br><span class="line"> <span class="comment">//  - labels</span></span><br><span class="line"> <span class="comment">//  - ... etc ...</span></span><br><span class="line"> metav1.ObjectMeta <span class="string">`json:"metadata,omitempty"`</span></span><br><span class="line"> </span><br><span class="line"> Spec networkspec <span class="string">`json:"spec"`</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// networkspec is the spec for a Network resource</span></span><br><span class="line"><span class="keyword">type</span> networkspec <span class="keyword">struct</span> &#123;</span><br><span class="line"> Cidr    <span class="keyword">string</span> <span class="string">`json:"cidr"`</span></span><br><span class="line"> Gateway <span class="keyword">string</span> <span class="string">`json:"gateway"`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// NetworkList is a list of Network resources</span></span><br><span class="line"><span class="keyword">type</span> NetworkList <span class="keyword">struct</span> &#123;</span><br><span class="line"> metav1.TypeMeta <span class="string">`json:",inline"`</span></span><br><span class="line"> metav1.ListMeta <span class="string">`json:"metadata"`</span></span><br><span class="line"> </span><br><span class="line"> Items []Network <span class="string">`json:"items"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>+genclient 的意思是：请为下面这个 API 资源类型生成对应的 Client 代码,需要注意的是，+genclient 只需要写在 Network 类型上，而不用写在 NetworkList 上。因为 NetworkList 只是一个返回值类型，Network 才是“主类型”。</p>
<p>+genclient:noStatus 的意思是：这个 API 资源类型定义里，没有 Status 字段。否则，生成的 Client 就会自动带上 UpdateStatus 方法。</p>
<p>+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object的注释。它的意思是，请在生成 DeepCopy 的时候，实现 Kubernetes 提供的 runtime.Object 接口。否则，在某些版本的 Kubernetes 里，你的这个类型定义会出现编译错误。这是一个固定的操作，记住即可。</p>
<h5 id="pkg-apis-samplecrd-v1-register-go"><a href="#pkg-apis-samplecrd-v1-register-go" class="headerlink" title="pkg/apis/samplecrd/v1/register.go"></a>pkg/apis/samplecrd/v1/register.go</h5><p>在前面对 APIServer 工作原理的讲解中，我已经提到，“registry”的作用就是注册一个类型（Type）给 APIServer。其中，Network 资源类型在服务器端的注册的工作，APIServer 会自动帮我们完成。但与之对应的，我们还需要让客户端也能“知道”Network 资源类型的定义。这就需要我们在项目里添加一个 register.go 文件。它最主要的功能，就是定义了如下所示的 addKnownTypes() 方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">package v1</span><br><span class="line">...</span><br><span class="line">// addKnownTypes adds our types to the API scheme by registering</span><br><span class="line">// Network and NetworkList</span><br><span class="line">func addKnownTypes(scheme *runtime.Scheme) error &#123;</span><br><span class="line"> scheme.AddKnownTypes(</span><br><span class="line">  SchemeGroupVersion,</span><br><span class="line">  &amp;Network&#123;&#125;,</span><br><span class="line">  &amp;NetworkList&#123;&#125;,</span><br><span class="line"> )</span><br><span class="line"> </span><br><span class="line"> // register the type in the scheme</span><br><span class="line"> metav1.AddToGroupVersion(scheme, SchemeGroupVersion)</span><br><span class="line"> return nil</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>有了这个方法，Kubernetes 就能够在后面生成客户端的时候，“知道”Network 以及 NetworkList 类型的定义了。</p>
<h5 id="自动生成代码"><a href="#自动生成代码" class="headerlink" title="自动生成代码"></a>自动生成代码</h5><p>接下来，我就要使用 Kubernetes 提供的代码生成工具，为上面定义的 Network 资源类型自动生成 clientset、informer 和 lister。</p>
<p>这个代码生成工具名叫k8s.io/code-generator，使用方法如下所示：</p>
<ol>
<li>设置环境变量,把以下环境变量写到/etc/environment</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/environment</span><br><span class="line"></span><br><span class="line">GOPATH=/root/go</span><br><span class="line"># 代码生成的工作目录，也就是我们的项目路径</span><br><span class="line">ROOT_PACKAGE=&quot;github.com/shiyangtao/k8s-controller-custom-resource&quot;</span><br><span class="line"># API Group</span><br><span class="line">CUSTOM_RESOURCE_NAME=&quot;samplecrd&quot;</span><br><span class="line"># API Version</span><br><span class="line">CUSTOM_RESOURCE_VERSION=&quot;v1&quot;</span><br><span class="line"></span><br><span class="line">source /etc/environment</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>安装k8s.io/code-generator</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ go get -u k8s.io/code-generator/...</span><br><span class="line"></span><br><span class="line"># 如果go get有问题的话换一种方式</span><br><span class="line">mkdir $GOPATH/src/k8s.io</span><br><span class="line">cd $GOPATH/src/k8s.io</span><br><span class="line">git clone https://github.com/kubernetes/code-generator.git</span><br><span class="line">rm -rf text/.git</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>下载代码 </li>
</ol>
<p>代码路径 $GOPATH/src/github.com/shiyangtao/k8s-controller-custom-resource</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p $GOPATH/src/github.com/shiyangtao/</span><br><span class="line">cd $GOPATH/src/github.com/shiyangtao/</span><br><span class="line">git clone https://github.com/resouer/k8s-controller-custom-resource.git</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>生成代码</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cd $GOPATH/src/k8s.io/code-generator</span><br><span class="line"># 执行代码自动生成，其中pkg/client是生成目标目录，pkg/apis是类型定义目录</span><br><span class="line">$ ./generate-groups.sh all &quot;$ROOT_PACKAGE/pkg/client&quot; &quot;$ROOT_PACKAGE/pkg/apis&quot; &quot;$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION&quot;</span><br></pre></td></tr></table></figure>
<p><strong><em>如果有报错试着执行下 这段代码go get -u k8s.io/apimachinery </em></strong></p>
<p>代码生成工作完成之后，我们再查看一下这个项目的目录结构：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ tree</span><br><span class="line">.</span><br><span class="line">├── controller.go</span><br><span class="line">├── crd</span><br><span class="line">│   └── network.yaml</span><br><span class="line">├── example</span><br><span class="line">│   └── example-network.yaml</span><br><span class="line">├── main.go</span><br><span class="line">└── pkg</span><br><span class="line">    ├── apis</span><br><span class="line">    │   └── samplecrd</span><br><span class="line">    │       ├── constants.go</span><br><span class="line">    │       └── v1</span><br><span class="line">    │           ├── doc.go</span><br><span class="line">    │           ├── register.go</span><br><span class="line">    │           ├── types.go</span><br><span class="line">    │           └── zz_generated.deepcopy.go</span><br><span class="line">    └── client</span><br><span class="line">        ├── clientset</span><br><span class="line">        ├── informers</span><br><span class="line">        └── listers</span><br></pre></td></tr></table></figure>
<p>其中，pkg/apis/samplecrd/v1 下面的 zz_generated.deepcopy.go 文件，就是自动生成的 DeepCopy 代码文件。</p>
<p>而整个 client 目录，以及下面的三个包（clientset、informers、 listers），都是 Kubernetes 为 Network 类型生成的客户端库，这些库会在后面编写自定义控制器的时候用到。</p>
<p>而有了这些内容，现在你就可以在 Kubernetes 集群里创建一个 Network 类型的 API 对象了。我们不妨一起来实验一下。首先，使用 network.yaml 文件，在 Kubernetes 中创建 Network 对象的 CRD（Custom Resource Definition）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl apply -f crd/network.yaml</span><br><span class="line">customresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created</span><br></pre></td></tr></table></figure>
<p>然后，我们就可以创建一个 Network 对象了，这里用到的是 example-network.yaml：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl apply -f example/example-network.yaml </span><br><span class="line">network.samplecrd.k8s.io/example-network created</span><br></pre></td></tr></table></figure>
<p>通过这个操作，你就在 Kubernetes 集群里创建了一个 Network 对象。它的 API 资源路径是samplecrd.k8s.io/v1/networks。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get network</span><br><span class="line">NAME              AGE</span><br><span class="line">example-network   8s</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe network example-network</span><br><span class="line">Name:         example-network</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">...API Version:  samplecrd.k8s.io/v1</span><br><span class="line">Kind:         Network</span><br><span class="line">Metadata:</span><br><span class="line">  ...</span><br><span class="line">  Generation:          1</span><br><span class="line">  Resource Version:    468239</span><br><span class="line">  ...</span><br><span class="line">Spec:</span><br><span class="line">  Cidr:     192.168.0.0/16</span><br><span class="line">  Gateway:  192.168.0.1</span><br></pre></td></tr></table></figure>
<h3 id="自定义控制器"><a href="#自定义控制器" class="headerlink" title="自定义控制器"></a>自定义控制器</h3><h4 id="编写main函数"><a href="#编写main函数" class="headerlink" title="编写main函数"></a>编写main函数</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)</span><br><span class="line">  ...</span><br><span class="line">  kubeClient, err := kubernetes.NewForConfig(cfg)</span><br><span class="line">  ...</span><br><span class="line">  networkClient, err := clientset.NewForConfig(cfg)</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  networkInformerFactory := informers.NewSharedInformerFactory(networkClient, ...)</span><br><span class="line">  </span><br><span class="line">  controller := NewController(kubeClient, networkClient,</span><br><span class="line">  networkInformerFactory.Samplecrd().V1().Networks())</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">go</span> networkInformerFactory.Start(stopCh)</span><br><span class="line"> </span><br><span class="line">  <span class="keyword">if</span> err = controller.Run(<span class="number">2</span>, stopCh); err != <span class="literal">nil</span> &#123;</span><br><span class="line">    glog.Fatalf(<span class="string">"Error running controller: %s"</span>, err.Error())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一步：main 函数根据我提供的 Master 配置（APIServer 的地址端口和 kubeconfig 的路径），创建一个 Kubernetes 的 client（kubeClient）和 Network 对象的 client（networkClient）。但是，如果我没有提供 Master 配置呢？这时，main 函数会直接使用一种名叫 InClusterConfig 的方式来创建这个 client。这个方式，会假设你的自定义控制器是以 Pod 的方式运行在 Kubernetes 集群里的。</p>
<p>第二步：main 函数为 Network 对象创建一个叫作 InformerFactory（即：networkInformerFactory）的工厂，并使用它生成一个 Network 对象的 Informer，传递给控制器。</p>
<p>第三步：main 函数启动上述的 Informer，然后执行 controller.Run，启动自定义控制器。至此，main 函数就结束了</p>
<p><img src="/2020/02/21/k8s-command/images/image-20200226093239685.png" alt="image-20200226093239685"></p>
<h4 id="编写自定义控制器的定义"><a href="#编写自定义控制器的定义" class="headerlink" title="编写自定义控制器的定义"></a>编写自定义控制器的定义</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewController</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">  kubeclientset kubernetes.Interface,</span></span></span><br><span class="line"><span class="function"><span class="params">  networkclientset clientset.Interface,</span></span></span><br><span class="line"><span class="function"><span class="params">  networkInformer informers.NetworkInformer)</span> *<span class="title">Controller</span></span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  controller := &amp;Controller&#123;</span><br><span class="line">    kubeclientset:    kubeclientset,</span><br><span class="line">    networkclientset: networkclientset,</span><br><span class="line">    networksLister:   networkInformer.Lister(),</span><br><span class="line">    networksSynced:   networkInformer.Informer().HasSynced,</span><br><span class="line">    workqueue:        workqueue.NewNamedRateLimitingQueue(...,  <span class="string">"Networks"</span>),</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">    AddFunc: controller.enqueueNetwork,</span><br><span class="line">    UpdateFunc: <span class="function"><span class="keyword">func</span><span class="params">(old, <span class="built_in">new</span> <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">      oldNetwork := old.(*samplecrdv1.Network)</span><br><span class="line">      newNetwork := <span class="built_in">new</span>.(*samplecrdv1.Network)</span><br><span class="line">      <span class="keyword">if</span> oldNetwork.ResourceVersion == newNetwork.ResourceVersion &#123;</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line">      controller.enqueueNetwork(<span class="built_in">new</span>)</span><br><span class="line">    &#125;,</span><br><span class="line">    DeleteFunc: controller.enqueueNetworkForDelete,</span><br><span class="line"> <span class="keyword">return</span> controller</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="编写控制器里的业务逻辑"><a href="#编写控制器里的业务逻辑" class="headerlink" title="编写控制器里的业务逻辑"></a>编写控制器里的业务逻辑</h4><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">Run</span><span class="params">(threadiness <span class="keyword">int</span>, stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> ...</span><br><span class="line">  <span class="keyword">if</span> ok := cache.WaitForCacheSync(stopCh, c.networksSynced); !ok &#123;</span><br><span class="line">    <span class="keyword">return</span> fmt.Errorf(<span class="string">"failed to wait for caches to sync"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; threadiness; i++ &#123;</span><br><span class="line">    <span class="keyword">go</span> wait.Until(c.runWorker, time.Second, stopCh)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">runWorker</span><span class="params">()</span></span> &#123;</span><br><span class="line">  <span class="keyword">for</span> c.processNextWorkItem() &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">processNextWorkItem</span><span class="params">()</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">  obj, shutdown := c.workqueue.Get()</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  err := <span class="function"><span class="keyword">func</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">if</span> err := c.syncHandler(key); err != <span class="literal">nil</span> &#123;</span><br><span class="line">     <span class="keyword">return</span> fmt.Errorf(<span class="string">"error syncing '%s': %s"</span>, key, err.Error())</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    c.workqueue.Forget(obj)</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">  &#125;(obj)</span><br><span class="line">  </span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">syncHandler</span><span class="params">(key <span class="keyword">string</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"></span><br><span class="line">  namespace, name, err := cache.SplitMetaNamespaceKey(key)</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  network, err := c.networksLister.Networks(namespace).Get(name)</span><br><span class="line">  <span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> errors.IsNotFound(err) &#123;</span><br><span class="line">      glog.Warningf(<span class="string">"Network does not exist in local cache: %s/%s, will delete it from Neutron ..."</span>,</span><br><span class="line">      namespace, name)</span><br><span class="line">      </span><br><span class="line">      glog.Warningf(<span class="string">"Network: %s/%s does not exist in local cache, will delete it from Neutron ..."</span>,</span><br><span class="line">    namespace, name)</span><br><span class="line">    </span><br><span class="line">     <span class="comment">// FIX ME: call Neutron API to delete this network by name.</span></span><br><span class="line">     <span class="comment">//</span></span><br><span class="line">     <span class="comment">// neutron.Delete(namespace, name)</span></span><br><span class="line">     </span><br><span class="line">     <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">  &#125;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> err</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  glog.Infof(<span class="string">"[Neutron] Try to process network: %#v ..."</span>, network)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// FIX ME: Do diff().</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// actualNetwork, exists := neutron.Get(namespace, name)</span></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="comment">// if !exists &#123;</span></span><br><span class="line">  <span class="comment">//   neutron.Create(namespace, name)</span></span><br><span class="line">  <span class="comment">// &#125; else if !reflect.DeepEqual(actualNetwork, network) &#123;</span></span><br><span class="line">  <span class="comment">//   neutron.Update(namespace, name)</span></span><br><span class="line">  <span class="comment">// &#125;</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="执行控制器"><a href="#执行控制器" class="headerlink" title="执行控制器"></a>执行控制器</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ cd $GOPATH/src/github.com/shiyangtao/k8s-controller-custom-resource</span><br><span class="line"></span><br><span class="line">### Skip this part if you don&apos;t want to build</span><br><span class="line"># Install dependency</span><br><span class="line">$ go get github.com/tools/godep</span><br><span class="line">$ godep restore</span><br><span class="line"># Build</span><br><span class="line">$ go build -o samplecrd-controller .</span><br><span class="line"></span><br><span class="line">$ ./samplecrd-controller -kubeconfig=$HOME/.kube/config -alsologtostderr=true</span><br><span class="line">I0915 12:50:29.051349   27159 controller.go:84] Setting up event handlers</span><br><span class="line">I0915 12:50:29.051615   27159 controller.go:113] Starting Network control loop</span><br><span class="line">I0915 12:50:29.051630   27159 controller.go:116] Waiting for informer caches to sync</span><br><span class="line">E0915 12:50:29.066745   27159 reflector.go:134] github.com/resouer/k8s-controller-custom-resource/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>接下来可以打开另外一个命令行进行以下操作,观察下以上控制台的变化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">修改网段</span><br><span class="line">example/example-network.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl apply -f example/example-network.yaml </span><br><span class="line">network.samplecrd.k8s.io/example-network configured</span><br><span class="line"></span><br><span class="line">删除</span><br><span class="line"></span><br><span class="line">$ kubectl delete -f example/example-network.yaml</span><br></pre></td></tr></table></figure>
<p>所谓的 Informer，就是一个自带缓存和索引机制，可以触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。</p>
<p>Informer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。Reflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。</p>
<p>而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同。</p>
<p>在实际应用中，除了控制循环之外的所有代码，实际上都是 Kubernetes 为你自动生成的，即：pkg/client/{informers, listers, clientset}里的内容。</p>
<h2 id="RBAC"><a href="#RBAC" class="headerlink" title="RBAC"></a>RBAC</h2><h3 id="Role"><a href="#Role" class="headerlink" title="Role"></a>Role</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">  name: example-role</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure>
<p>类似的，Role 对象的 rules 字段也可以进一步细化。比如，你可以只针对某一个具体的对象进行权限设置，如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;configmaps&quot;]</span><br><span class="line">  resourceNames: [&quot;my-config&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;]</span><br></pre></td></tr></table></figure>
<p>这个例子就表示，这条规则的“被作用者”，只对名叫“my-config”的 ConfigMap 对象，有进行 GET 操作的权限。</p>
<h3 id="RoleBinding"><a href="#RoleBinding" class="headerlink" title="RoleBinding"></a>RoleBinding</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-rolebinding</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: example-user</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: example-role</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>
<p>需要再次提醒的是，Role 和 RoleBinding 对象都是 Namespaced 对象（NamespacedObject），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。</p>
<h3 id="ClusterRole"><a href="#ClusterRole" class="headerlink" title="ClusterRole"></a>ClusterRole</h3><p>对于非 Namespaced（Non-namespaced）对象（比如：Node），或者，某一个 Role 想要作用于所有的 Namespace 的时候，我们又该如何去做授权呢？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-clusterrole</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure>
<p>查看k8s内置clusterRole</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get clusterrole</span><br></pre></td></tr></table></figure>
<h3 id="ClusterRoleBinding"><a href="#ClusterRoleBinding" class="headerlink" title="ClusterRoleBinding"></a>ClusterRoleBinding</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-clusterrolebinding</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: example-user</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: example-clusterrole</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>
<h3 id="User"><a href="#User" class="headerlink" title="User"></a>User</h3><p>可以看到，这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 User，即 Kubernetes 里的用户。这个用户的名字是 example-user。</p>
<p>可是，在 Kubernetes 中，其实并没有一个叫作“User”的 API 对象。而且，我们在前面和部署使用 Kubernetes 的流程里，既不需要 User，也没有创建过 User。</p>
<p>这个 User 到底是从哪里来的呢？</p>
<p>实际上，Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。或者，你也可以直接给 APIServer 指定一个用户名、密码文件。那么 Kubernetes 的授权系统，就能够从这个文件里找到对应的“用户”了。当然，在大多数私有的使用环境中，我们只要使用 Kubernetes 提供的内置“用户”，就足够了。</p>
<p>在大多数时候，我们其实都不太使用“用户”这个功能，而是直接使用 Kubernetes 里的“内置用户”。</p>
<p><strong><em>ServiceAccount其实就是内置用户</em></strong></p>
<p>定义一个ServiceAccount</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">  name: example-sa</span><br></pre></td></tr></table></figure>
<p>然后，我们通过编写 RoleBinding 的 YAML 文件，来为这个 ServiceAccount 分配权限：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-rolebinding</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: example-sa</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: example-role</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f svc-account.yaml</span><br><span class="line">$ kubectl create -f role-binding.yaml</span><br><span class="line">$ kubectl create -f role.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get sa -n mynamespace -o yaml</span><br><span class="line">- apiVersion: v1</span><br><span class="line">  kind: ServiceAccount</span><br><span class="line">  metadata:</span><br><span class="line">    creationTimestamp: 2018-09-08T12:59:17Z</span><br><span class="line">    name: example-sa</span><br><span class="line">    namespace: mynamespace</span><br><span class="line">    resourceVersion: &quot;409327&quot;</span><br><span class="line">    ...</span><br><span class="line">  secrets:</span><br><span class="line">  - name: example-sa-token-vmfg6</span><br></pre></td></tr></table></figure>
<p>可以看到，Kubernetes 会为一个 ServiceAccount 自动创建并分配一个 Secret 对象，即：上述 ServiceAcount 定义里最下面的 secrets 字段。这个 Secret，就是这个 ServiceAccount 对应的、用来跟 APIServer 进行交互的授权文件，我们一般称它为：Token。Token 文件的内容一般是证书或者密码，它以一个 Secret 对象的方式保存在 Etcd 当中。</p>
<p>这时候，用户的 Pod，就可以声明使用这个 ServiceAccount 了，比如下面这个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  namespace: mynamespace</span><br><span class="line">  name: sa-token-test</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx:1.7.9</span><br><span class="line">  serviceAccountName: example-sa</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f pod.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl describe pod sa-token-test -n mynamespace</span><br><span class="line">Name:               sa-token-test</span><br><span class="line">Namespace:          mynamespace</span><br><span class="line">...</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    ...</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from example-sa-token-vmfg6 (ro)</span><br></pre></td></tr></table></figure>
<p>等这个 Pod 运行起来之后，我们就可以看到，该 ServiceAccount 的 token，也就是一个 Secret 对象，被 Kubernetes 自动挂载到了容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录下.</p>
<p>这时候，我们可以通过 kubectl exec 查看到这个目录里的文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl exec -it sa-token-test -n mynamespace -- /bin/bash</span><br><span class="line">root@sa-token-test:/# ls /var/run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">ca.crt namespace  token</span><br></pre></td></tr></table></figure>
<h4 id="defaultServiceAccount"><a href="#defaultServiceAccount" class="headerlink" title="defaultServiceAccount"></a>defaultServiceAccount</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$kubectl describe sa default</span><br><span class="line">Name:                default</span><br><span class="line">Namespace:           default</span><br><span class="line">Labels:              &lt;none&gt;</span><br><span class="line">Annotations:         &lt;none&gt;</span><br><span class="line">Image pull secrets:  &lt;none&gt;</span><br><span class="line">Mountable secrets:   default-token-s8rbq</span><br><span class="line">Tokens:              default-token-s8rbq</span><br><span class="line">Events:              &lt;none&gt;</span><br><span class="line"></span><br><span class="line">$ kubectl get secret</span><br><span class="line">NAME                  TYPE                                  DATA      AGE</span><br><span class="line">kubernetes.io/service-account-token   3         82d</span><br><span class="line"></span><br><span class="line">$ kubectl describe secret default-token-s8rbq</span><br><span class="line">Name:         default-token-s8rbq</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  kubernetes.io/service-account.name=default</span><br><span class="line">              kubernetes.io/service-account.uid=ffcb12b2-917f-11e8-abde-42010aa80002</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">ca.crt:     1025 bytes</span><br><span class="line">namespace:  7 bytes</span><br><span class="line">token:      &lt;TOKEN数据&gt;</span><br></pre></td></tr></table></figure>
<p>可以看到，Kubernetes 会自动为默认 ServiceAccount 创建并绑定一个特殊的 Secret：它的类型是kubernetes.io/service-account-token；它的 Annotation 字段，声明了kubernetes.io/service-account.name=default，即这个 Secret 会跟同一 Namespace 下名叫 default 的 ServiceAccount 进行绑定。</p>
<h3 id="Group"><a href="#Group" class="headerlink" title="Group"></a>Group</h3><p>如果你为 Kubernetes 配置了外部认证服务的话，这个“用户组”的概念就会由外部认证服务提供。</p>
<p>而对于 Kubernetes 的内置“用户”ServiceAccount 来说，上述“用户组”的概念也同样适用。</p>
<p>实际上，一个 ServiceAccount，在 Kubernetes 里对应的“用户”的名字是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">system:serviceaccount:&lt;Namespace名字&gt;:&lt;ServiceAccount名字&gt;</span><br></pre></td></tr></table></figure>
<p>而它对应的内置“用户组”的名字，就是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">system:serviceaccounts:&lt;Namespace名字&gt;</span><br></pre></td></tr></table></figure>
<p>比如，现在我们可以在 RoleBinding 里定义如下的 subjects：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: system:serviceaccounts:mynamespace</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>
<p>这就意味着这个 Role 的权限规则，作用于 mynamespace 里的所有 ServiceAccount。这就用到了“用户组”的概念。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: system:serviceaccounts</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>
<p>就意味着这个 Role 的权限规则，作用于整个系统里的所有 ServiceAccount。</p>
<h4 id="所有Namespace-下的默认-ServiceAccount，绑定一个只读权限的-Role。"><a href="#所有Namespace-下的默认-ServiceAccount，绑定一个只读权限的-Role。" class="headerlink" title="所有Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。"></a>所有Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">name: readonly-all-default</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">	name: system:serviceaccount:default</span><br><span class="line">roleRef:</span><br><span class="line">kind: ClusterRole</span><br><span class="line">name: view</span><br><span class="line">apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>
<p>最后，值得一提的是，在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。</p>
<p>一般来说，这些系统 ClusterRole，是绑定给 Kubernetes 系统组件对应的 ServiceAccount 使用的。比如，其中一个名叫 system:kube-scheduler 的 ClusterRole，定义的权限规则是 kube-scheduler（Kubernetes 的调度器组件）运行所需要的必要权限。你可以通过如下指令查看这些权限的列表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe clusterrole system:kube-scheduler</span><br><span class="line">Name:         system:kube-scheduler</span><br><span class="line">...</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources                    Non-Resource URLs Resource Names    Verbs</span><br><span class="line">  ---------                    -----------------  --------------    -----</span><br><span class="line">...</span><br><span class="line">  services                     []                 []                [get list watch]</span><br><span class="line">  replicasets.apps             []                 []                [get list watch]</span><br><span class="line">  statefulsets.apps            []                 []                [get list watch]</span><br><span class="line">  replicasets.extensions       []                 []                [get list watch]</span><br><span class="line">  poddisruptionbudgets.policy  []                 []                [get list watch]</span><br><span class="line">  pods/status                  []                 []                [patch update]</span><br></pre></td></tr></table></figure>
<p>这个 system:kube-scheduler 的 ClusterRole，就会被绑定给 kube-system Namesapce 下名叫 kube-scheduler 的 ServiceAccount，它正是 Kubernetes 调度器的 Pod 声明使用的 ServiceAccount。</p>
<p>除此之外，Kubernetes 还提供了四个预先定义好的 ClusterRole 来供用户直接使用：</p>
<ul>
<li>cluster-admin；</li>
<li>admin；</li>
<li>edit；</li>
<li>view。</li>
</ul>
<p>通过它们的名字，你应该能大致猜出它们都定义了哪些权限。比如，这个名叫 view 的 ClusterRole，就规定了被作用者只有 Kubernetes API 的只读权限。而我还要提醒你的是，上面这个 cluster-admin 角色，对应的是整个 Kubernetes 项目中的最高权限（verbs=*），如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe clusterrole cluster-admin -n kube-system</span><br><span class="line">Name:         cluster-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=true</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources  Non-Resource URLs Resource Names  Verbs</span><br><span class="line">  ---------  -----------------  --------------  -----</span><br><span class="line">  *.*        []                 []              [*]</span><br><span class="line">             [*]                []              [*]</span><br></pre></td></tr></table></figure>
<h2 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h2><p>可能你已经感觉到，在 Kubernetes 中，管理“有状态应用”是一个比较复杂的过程，尤其是编写 Pod 模板的时候，总有一种“在 YAML 文件里编程序”的感觉，让人很不舒服。</p>
<p>而在 Kubernetes 生态中，还有一个相对更加灵活和编程友好的管理“有状态应用”的解决方案，它就是：Operator。</p>
<p>接下来，我就以 Etcd Operator 为例，来为你讲解一下 Operator 的工作原理和编写方法。Operator的原理就是自定义控制器。</p>
<h3 id="构建步骤"><a href="#构建步骤" class="headerlink" title="构建步骤"></a>构建步骤</h3><h4 id="第一步，将这个-Operator-的代码-Clone-到本地："><a href="#第一步，将这个-Operator-的代码-Clone-到本地：" class="headerlink" title="第一步，将这个 Operator 的代码 Clone 到本地："></a>第一步，将这个 Operator 的代码 Clone 到本地：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/coreos/etcd-operator</span><br></pre></td></tr></table></figure>
<h4 id="第二步，将这个-Etcd-Operator-部署在-Kubernetes-集群里。"><a href="#第二步，将这个-Etcd-Operator-部署在-Kubernetes-集群里。" class="headerlink" title="第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。"></a>第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ example/rbac/create_role.sh</span><br></pre></td></tr></table></figure>
<p>这个脚本的作用，就是为 Etcd Operator 创建 RBAC 规则。这是因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象。</p>
<h4 id="第三步，创建Etcd-Operator"><a href="#第三步，创建Etcd-Operator" class="headerlink" title="第三步，创建Etcd Operator"></a>第三步，创建Etcd Operator</h4><p>而 Etcd Operator 本身，其实就是一个 Deployment，它的 YAML 文件如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: extensions/v1beta1 高版本改成 apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: etcd-operator</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      name: etcd-operator</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        name: etcd-operator</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: etcd-operator</span><br><span class="line">        image: quay.io/coreos/etcd-operator:v0.9.2</span><br><span class="line">        command:</span><br><span class="line">        - etcd-operator</span><br><span class="line">        env:</span><br><span class="line">        - name: MY_POD_NAMESPACE</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line">        - name: MY_POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f example/deployment.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                              READY     STATUS      RESTARTS   AGE</span><br><span class="line">etcd-operator-649dbdb5cb-bzfzp    1/1       Running     0          20s</span><br><span class="line"></span><br><span class="line">$ kubectl get crd</span><br><span class="line">NAME                                    CREATED AT</span><br><span class="line">etcdclusters.etcd.database.coreos.com   2018-09-18T11:42:55Z</span><br></pre></td></tr></table></figure>
<p>这个 CRD 名叫etcdclusters.etcd.database.coreos.com 。你可以通过 kubectl describe 命令看到它的细节，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl describe crd  etcdclusters.etcd.database.coreos.com</span><br><span class="line">...</span><br><span class="line">Group:   etcd.database.coreos.com</span><br><span class="line">  Names:</span><br><span class="line">    Kind:       EtcdCluster</span><br><span class="line">    List Kind:  EtcdClusterList</span><br><span class="line">    Plural:     etcdclusters</span><br><span class="line">    Short Names:</span><br><span class="line">      etcd</span><br><span class="line">    Singular:  etcdcluster</span><br><span class="line">  Scope:       Namespaced</span><br><span class="line">  Version:     v1beta2</span><br><span class="line">  </span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>所以说，通过上述两步操作，你实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型。而 Etcd Operator 本身，就是这个自定义资源类型对应的自定义控制器。</p>
<h4 id="第四步，创建Etcd-集群"><a href="#第四步，创建Etcd-集群" class="headerlink" title="第四步，创建Etcd 集群"></a>第四步，创建Etcd 集群</h4><p>不难想到，这个文件里定义的，正是 EtcdCluster 这个 CRD 的一个具体实例，也就是一个 Custom Resource（CR）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: &quot;etcd.database.coreos.com/v1beta2&quot;</span><br><span class="line">kind: &quot;EtcdCluster&quot;</span><br><span class="line">metadata:</span><br><span class="line">  name: &quot;example-etcd-cluster&quot;</span><br><span class="line">spec:</span><br><span class="line">  size: 3</span><br><span class="line">  version: &quot;3.2.13&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f example/example-etcd-cluster.yaml</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl get pods</span><br><span class="line">NAME                            READY     STATUS    RESTARTS   AGE</span><br><span class="line">example-etcd-cluster-dp8nqtjznc   1/1       Running     0          1m</span><br><span class="line">example-etcd-cluster-mbzlg6sd56   1/1       Running     0          2m</span><br><span class="line">example-etcd-cluster-v6v6s6stxd   1/1       Running     0          2m</span><br></pre></td></tr></table></figure>
<p>而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作了。看到这里，相信你应该已经对 Operator 有了一个初步的认知：</p>
<p><strong>Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。</strong></p>
<h3 id="Etcd-Operator原理分析"><a href="#Etcd-Operator原理分析" class="headerlink" title="Etcd Operator原理分析"></a>Etcd Operator原理分析</h3><h4 id="Etcd静态集群构建"><a href="#Etcd静态集群构建" class="headerlink" title="Etcd静态集群构建"></a>Etcd静态集群构建</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \</span><br><span class="line">  --listen-peer-urls http://10.0.1.10:2380 \</span><br><span class="line">...</span><br><span class="line">  --initial-cluster-token etcd-cluster-1 \</span><br><span class="line">  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \</span><br><span class="line">  --initial-cluster-state new</span><br><span class="line">  </span><br><span class="line">$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \</span><br><span class="line">  --listen-peer-urls http://10.0.1.11:2380 \</span><br><span class="line">...</span><br><span class="line">  --initial-cluster-token etcd-cluster-1 \</span><br><span class="line">  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \</span><br><span class="line">  --initial-cluster-state new</span><br><span class="line">  </span><br><span class="line">$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \</span><br><span class="line">  --listen-peer-urls http://10.0.1.12:2380 \</span><br><span class="line">...</span><br><span class="line">  --initial-cluster-token etcd-cluster-1 \</span><br><span class="line">  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \</span><br><span class="line">  --initial-cluster-state new</span><br></pre></td></tr></table></figure>
<p>其中，这些节点启动参数里的–initial-cluster 参数，非常值得你关注。它的含义，正是当前节点启动时集群的拓扑结构。说得更详细一点，就是当前这个节点启动时，需要跟哪些节点通信来组成集群。</p>
<p>此外，一个 Etcd 集群还需要用–initial-cluster-token 字段，来声明一个该集群独一无二的 Token 名字。</p>
<p>像上述这样为每一个 Ectd 节点配置好它对应的启动参数之后把它们启动起来，一个 Etcd 集群就可以自动组建起来了。</p>
<h4 id="EtcdCluster-CRD"><a href="#EtcdCluster-CRD" class="headerlink" title="EtcdCluster CRD"></a>EtcdCluster CRD</h4><p>types.go的定义</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">// +genclient</span><br><span class="line">// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object</span><br><span class="line"></span><br><span class="line">type EtcdCluster struct &#123;</span><br><span class="line">  metav1.TypeMeta   `json:&quot;,inline&quot;`</span><br><span class="line">  metav1.ObjectMeta `json:&quot;metadata,omitempty&quot;`</span><br><span class="line">  Spec              ClusterSpec   `json:&quot;spec&quot;`</span><br><span class="line">  Status            ClusterStatus `json:&quot;status&quot;`</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">type ClusterSpec struct &#123;</span><br><span class="line"> // Size is the expected size of the etcd cluster.</span><br><span class="line"> // The etcd-operator will eventually make the size of the running</span><br><span class="line"> // cluster equal to the expected size.</span><br><span class="line"> // The vaild range of the size is from 1 to 7.</span><br><span class="line"> Size int `json:&quot;size&quot;`</span><br><span class="line"> ... </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，EtcdCluster 是一个有 Status 字段的 CRD。在这里，我们可以不必关心 ClusterSpec 里的其他字段，只关注 Size（即：Etcd 集群的大小）字段即可。Size 字段的存在，就意味着将来如果我们想要调整集群大小的话，应该直接修改 YAML 文件里 size 的值，并执行 kubectl apply -f。</p>
<p><strong>这样，Operator 就会帮我们完成 Etcd 节点的增删操作。这种“scale”能力，也是 Etcd Operator 自动化运维 Etcd 集群需要实现的主要功能。</strong></p>
<p>而为了能够支持这个功能，我们就不再像前面那样在–initial-cluster 参数里把拓扑结构固定死</p>
<h4 id="Etcd-Operator-scale原理"><a href="#Etcd-Operator-scale原理" class="headerlink" title="Etcd Operator scale原理"></a>Etcd Operator scale原理</h4><p>首先，Etcd Operator 会创建一个“种子节点”；</p>
<p>然后，Etcd Operator 会不断创建新的 Etcd 节点，然后将它们逐一加入到这个集群当中，直到集群的节点数等于 size。</p>
<p>而这两种节点的不同之处，就在于一个名叫–initial-cluster-state 的启动参数：</p>
<ul>
<li><p>当这个参数值设为 new 时，就代表了该节点是种子节点。而我们前面提到过，种子节点还必须通过–initial-cluster-token 声明一个独一无二的 Token。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> etcd</span><br><span class="line">  --data-dir=/var/etcd/data</span><br><span class="line">  --name=infra0</span><br><span class="line">  --initial-advertise-peer-urls=http://10.0.1.10:2380</span><br><span class="line">  --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">  --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">  --advertise-client-urls=http://10.0.1.10:2379</span><br><span class="line">  --initial-cluster=infra0=http://10.0.1.10:2380</span><br><span class="line">  --initial-cluster-state=new</span><br><span class="line">  --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p>而如果这个参数值设为 existing，那就是说明这个节点是一个普通节点，Etcd Operator 需要把它加入到已有集群里。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 第一步：通过 Etcd 命令行添加一个新成员：</span><br><span class="line"><span class="meta">$</span> etcdctl member add infra1 http://10.0.1.11:2380</span><br><span class="line"><span class="meta">#</span> 第二步：为这个成员节点生成对应的启动参数，并启动它：</span><br><span class="line"><span class="meta">$</span> etcd</span><br><span class="line">    --data-dir=/var/etcd/data</span><br><span class="line">    --name=infra1</span><br><span class="line">    --initial-advertise-peer-urls=http://10.0.1.11:2380</span><br><span class="line">    --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">    --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">    --advertise-client-urls=http://10.0.1.11:2379</span><br><span class="line">    --initial-cluster=infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380</span><br><span class="line">    --initial-cluster-state=existing</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Etcd-Operator-Informer"><a href="#Etcd-Operator-Informer" class="headerlink" title="Etcd Operator Informer"></a>Etcd Operator Informer</h4><p>跟所有的自定义控制器一样，Etcd Operator 的启动流程也是围绕着 Informer 展开的，如下所示：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">Start</span><span class="params">()</span> <span class="title">error</span></span> &#123;</span><br><span class="line"> <span class="keyword">for</span> &#123;</span><br><span class="line">  err := c.initResource()</span><br><span class="line">  ...</span><br><span class="line">  time.Sleep(initRetryWaitTime)</span><br><span class="line"> &#125;</span><br><span class="line"> c.run()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(c *Controller)</span> <span class="title">run</span><span class="params">()</span></span> &#123;</span><br><span class="line"> ...</span><br><span class="line"> </span><br><span class="line"> _, informer := cache.NewIndexerInformer(source, &amp;api.EtcdCluster&#123;&#125;, <span class="number">0</span>, cache.ResourceEventHandlerFuncs&#123;</span><br><span class="line">  AddFunc:    c.onAddEtcdClus,</span><br><span class="line">  UpdateFunc: c.onUpdateEtcdClus,</span><br><span class="line">  DeleteFunc: c.onDeleteEtcdClus,</span><br><span class="line"> &#125;, cache.Indexers&#123;&#125;)</span><br><span class="line"> </span><br><span class="line"> ctx := context.TODO()</span><br><span class="line"> <span class="comment">// <span class="doctag">TODO:</span> use workqueue to avoid blocking</span></span><br><span class="line"> informer.Run(ctx.Done())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，Etcd Operator 启动要做的第一件事（ c.initResource），是创建 EtcdCluster 对象所需要的 CRD，即：前面提到的etcdclusters.etcd.database.coreos.com。这样 Kubernetes 就能够“认识”EtcdCluster 这个自定义 API 资源了。</p>
<p>而接下来，Etcd Operator 会定义一个 EtcdCluster 对象的 Informer。不过，需要注意的是，由于 Etcd Operator 的完成时间相对较早，所以它里面有些代码的编写方式会跟我们之前讲解的最新的编写方式不太一样。在具体实践的时候，你还是应该以我讲解的模板为主。</p>
<p>比如，在上面的代码最后，你会看到有这样一句注释：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// TODO: use workqueue to avoid blocking</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>具体来讲，我们在控制循环里执行的业务逻辑，往往是比较耗时间的。比如，创建一个真实的 Etcd 集群。而 Informer 的 WATCH 机制对 API 对象变化的响应，则非常迅速。所以，控制器里的业务逻辑就很可能会拖慢 Informer 的执行周期，甚至可能 Block 它。而要协调这样两个快、慢任务的一个典型解决方法，就是引入一个工作队列。</p>
<p>由于 Etcd Operator 里没有工作队列，那么在它的 EventHandler 部分，就不会有什么入队操作，而直接就是每种事件对应的具体的业务逻辑了。</p>
<p><img src="/2020/02/21/k8s-command/images/image-20200229152908546.png" alt="image-20200229152908546"></p>
<p>可以看到，Etcd Operator 的特殊之处在于，它为每一个 EtcdCluster 对象，都启动了一个控制循环，“并发”地响应这些对象的变化。显然，这种做法不仅可以简化 Etcd Operator 的代码实现，还有助于提高它的响应速度。</p>
<h4 id="启动Bootstrap"><a href="#启动Bootstrap" class="headerlink" title="启动Bootstrap"></a>启动Bootstrap</h4><p>其中，第一个工作只在该 Cluster 对象第一次被创建的时候才会执行。这个工作，就是我们前面提到过的 Bootstrap，即：创建一个单节点的种子集群。</p>
<p>由于种子集群只有一个节点，所以这一步直接就会生成一个 Etcd 的 Pod 对象。这个 Pod 里有一个 InitContainer，负责检查 Pod 的 DNS 记录是否正常。如果检查通过，用户容器也就是 Etcd 容器就会启动起来。</p>
<p>启动命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/usr/local/bin/etcd</span><br><span class="line">  --data-dir=/var/etcd/data</span><br><span class="line">  --name=example-etcd-cluster-mbzlg6sd56</span><br><span class="line">  --initial-advertise-peer-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">  --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">  --advertise-client-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2379</span><br><span class="line">  --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --initial-cluster-state=new</span><br><span class="line">  --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8</span><br></pre></td></tr></table></figure>
<p>可以看到，在这些启动参数（比如：initial-cluster）里，Etcd Operator 只会使用 Pod 的 DNS 记录，而不是它的 IP 地址。</p>
<p>这当然是因为，在 Operator 生成上述启动命令的时候，Etcd 的 Pod 还没有被创建出来，它的 IP 地址自然也无从谈起。</p>
<p>这也就意味着，每个 Cluster 对象，都会事先创建一个与该 EtcdCluster 同名的 Headless Service。这样，Etcd Operator 在接下来的所有创建 Pod 的步骤里，就都可以使用 Pod 的 DNS 记录来代替它的 IP 地址了。</p>
<h4 id="启动该集群所对应的控制循环"><a href="#启动该集群所对应的控制循环" class="headerlink" title="启动该集群所对应的控制循环"></a>启动该集群所对应的控制循环</h4><p>Cluster 对象的第二个工作，则是启动该集群所对应的控制循环。</p>
<p>这个控制循环每隔一定时间，就会执行一次下面的 Diff 流程。</p>
<p>首先，控制循环要获取到所有正在运行的、属于这个 Cluster 的 Pod 数量，也就是该 Etcd 集群的“实际状态”。</p>
<p>而这个 Etcd 集群的“期望状态”，正是用户在 EtcdCluster 对象里定义的 size。</p>
<p>如果实际的 Pod 数量不够，那么控制循环就会执行一个添加成员节点的操作（即：上述流程图中的 addOneMember 方法）；反之，就执行删除成员节点的操作（即：上述流程图中的 removeOneMember 方法）</p>
<p>以 addOneMember 方法为例，它执行的流程如下所示：</p>
<ol>
<li>生成一个新节点的 Pod 的名字，比如：example-etcd-cluster-v6v6s6stxd；</li>
<li>调用 Etcd Client，执行前面提到过的 etcdctl member add example-etcd-cluster-v6v6s6stxd 命令；</li>
<li>使用这个 Pod 名字，和已经存在的所有节点列表，组合成一个新的 initial-cluster 字段的值；</li>
<li>使用这个 initial-cluster 的值，生成这个 Pod 里 Etcd 容器的启动命令。如下所示：</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">/usr/local/bin/etcd</span><br><span class="line">  --data-dir=/var/etcd/data</span><br><span class="line">  --name=example-etcd-cluster-v6v6s6stxd</span><br><span class="line">  --initial-advertise-peer-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --listen-peer-urls=http://0.0.0.0:2380</span><br><span class="line">  --listen-client-urls=http://0.0.0.0:2379</span><br><span class="line">  --advertise-client-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2379</span><br><span class="line">  --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380,example-etcd-cluster-v6v6s6stxd=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380</span><br><span class="line">  --initial-cluster-state=existing</span><br></pre></td></tr></table></figure>
<h3 id="对比StatefulSet-可能有的两个疑问"><a href="#对比StatefulSet-可能有的两个疑问" class="headerlink" title="对比StatefulSet  可能有的两个疑问"></a>对比StatefulSet  可能有的两个疑问</h3><h4 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h4><p>在 StatefulSet 里，它为 Pod 创建的名字是带编号的，这样就把整个集群的拓扑状态固定了下来（比如：一个三节点的集群一定是由名叫 web-0、web-1 和 web-2 的三个 Pod 组成）。可是，在 Etcd Operator 里，为什么我们使用随机名字就可以了呢？</p>
<p>答：这是因为，Etcd Operator 在每次添加 Etcd 节点的时候，都会先执行 etcdctl member add ；每次删除节点的时候，则会执行 etcdctl member remove 。这些操作，其实就会更新 Etcd 内部维护的拓扑信息，所以 Etcd Operator 无需在集群外部通过编号来固定这个拓扑关系。</p>
<h4 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h4><p>为什么我没有在 EtcdCluster 对象里声明 Persistent Volume？难道，我们不担心节点宕机之后 Etcd 的数据会丢失吗？</p>
<p>答：在有了 Operator 机制之后，上述 Etcd 的备份操作，是由一个单独的 Etcd Backup Operator 负责完成的。</p>
<p>创建和使用这个 Operator 的流程，如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span> 首先，创建etcd-backup-operator</span><br><span class="line"><span class="meta">$</span> kubectl create -f example/etcd-backup-operator/deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 确认etcd-backup-operator已经在正常运行</span><br><span class="line"><span class="meta">$</span> kubectl get pod</span><br><span class="line">NAME                                    READY     STATUS    RESTARTS   AGE</span><br><span class="line">etcd-backup-operator-1102130733-hhgt7   1/1       Running   0          3s</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 可以看到，Backup Operator会创建一个叫etcdbackups的CRD</span><br><span class="line"><span class="meta">$</span> kubectl get crd</span><br><span class="line">NAME                                    KIND</span><br><span class="line">etcdbackups.etcd.database.coreos.com    CustomResourceDefinition.v1beta1.apiextensions.k8s.io</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 我们这里要使用AWS S3来存储备份，需要将S3的授权信息配置在文件里</span><br><span class="line"><span class="meta">$</span> cat $AWS_DIR/credentials</span><br><span class="line">[default]</span><br><span class="line">aws_access_key_id = XXX</span><br><span class="line">aws_secret_access_key = XXX</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> cat $AWS_DIR/config</span><br><span class="line">[default]</span><br><span class="line">region = &lt;region&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 然后，将上述授权信息制作成一个Secret</span><br><span class="line"><span class="meta">$</span> kubectl create secret generic aws --from-file=$AWS_DIR/credentials --from-file=$AWS_DIR/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 使用上述S3的访问信息，创建一个EtcdBackup对象</span><br><span class="line"><span class="meta">$</span> sed -e 's|&lt;full-s3-path&gt;|mybucket/etcd.backup|g' \</span><br><span class="line">    -e 's|&lt;aws-secret&gt;|aws|g' \</span><br><span class="line">    -e 's|&lt;etcd-cluster-endpoints&gt;|"http://example-etcd-cluster-client:2379"|g' \</span><br><span class="line">    example/etcd-backup-operator/backup_cr.yaml \</span><br><span class="line">    | kubectl create -f -</span><br></pre></td></tr></table></figure>
<p>需要注意的是，每当你创建一个 EtcdBackup 对象（backup_cr.yaml），就相当于为它所指定的 Etcd 集群做了一次备份。EtcdBackup 对象的 etcdEndpoints 字段，会指定它要备份的 Etcd 集群的访问地址。所以，在实际的环境里，我建议你把最后这个备份操作，编写成一个 Kubernetes 的 CronJob 以便定时运行。</p>
<p>而当 Etcd 集群发生了故障之后，你就可以通过创建一个 EtcdRestore 对象来完成恢复操作。当然，这就意味着你也需要事先启动 Etcd Restore Operator。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建etcd-restore-operator</span><br><span class="line"><span class="meta">$</span> kubectl create -f example/etcd-restore-operator/deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 确认它已经正常运行</span><br><span class="line"><span class="meta">$</span> kubectl get pods</span><br><span class="line">NAME                                     READY     STATUS    RESTARTS   AGE</span><br><span class="line">etcd-restore-operator-4203122180-npn3g   1/1       Running   0          7s</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建一个EtcdRestore对象，来帮助Etcd Operator恢复数据，记得替换模板里的S3的访问信息</span><br><span class="line"><span class="meta">$</span> sed -e 's|&lt;full-s3-path&gt;|mybucket/etcd.backup|g' \</span><br><span class="line">    -e 's|&lt;aws-secret&gt;|aws|g' \</span><br><span class="line">    example/etcd-restore-operator/restore_cr.yaml \</span><br><span class="line">    | kubectl create -f -</span><br></pre></td></tr></table></figure>
<p>上面例子里的 EtcdRestore 对象（restore_cr.yaml），会指定它要恢复的 Etcd 集群的名字和备份数据所在的 S3 存储的访问信息。而当一个 EtcdRestore 对象成功创建后，Etcd Restore Operator 就会通过上述信息，恢复出一个全新的 Etcd 集群。然后，Etcd Operator 会把这个新集群直接接管过来，从而重新进入可用的状态。EtcdBackup 和 EtcdRestore 这两个 Operator 的工作原理，与 Etcd Operator 的实现方式非常类似。</p>
<h2 id="再谈PVC、PV、StorageClass"><a href="#再谈PVC、PV、StorageClass" class="headerlink" title="再谈PVC、PV、StorageClass"></a>再谈PVC、PV、StorageClass</h2><h3 id="PV-1"><a href="#PV-1" class="headerlink" title="PV"></a>PV</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  nfs:</span><br><span class="line">    server: 10.244.1.4</span><br><span class="line">    path: &quot;/&quot;</span><br></pre></td></tr></table></figure>
<h3 id="PVC-1"><a href="#PVC-1" class="headerlink" title="PVC"></a>PVC</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure>
<h3 id="在Pod中使用PVC"><a href="#在Pod中使用PVC" class="headerlink" title="在Pod中使用PVC"></a>在Pod中使用PVC</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    role: web-frontend</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: web</span><br><span class="line">    image: nginx</span><br><span class="line">    ports:</span><br><span class="line">      - name: web</span><br><span class="line">        containerPort: 80</span><br><span class="line">    volumeMounts:</span><br><span class="line">        - name: nfs</span><br><span class="line">          mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">  volumes:</span><br><span class="line">  - name: nfs</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: nfs</span><br></pre></td></tr></table></figure>
<p>可以看到，Pod 需要做的，就是在 volumes 字段里声明自己要使用的 PVC 名字。接下来，等这个 Pod 创建之后，kubelet 就会把这个 PVC 所对应的 PV，也就是一个 NFS 类型的 Volume，挂载在这个 Pod 容器内的目录上。</p>
<h3 id="Volume-Controller"><a href="#Volume-Controller" class="headerlink" title="Volume Controller"></a>Volume Controller</h3><p>在 Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器,这个 Volume Controller 维护着多个控制循环。</p>
<h4 id="PersistentVolumeController"><a href="#PersistentVolumeController" class="headerlink" title="PersistentVolumeController"></a>PersistentVolumeController</h4><p>其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。它的名字叫作 PersistentVolumeController。</p>
<p>PersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态，从而结束“单身”之旅。</p>
<p>而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。</p>
<h4 id="AttachDetachController"><a href="#AttachDetachController" class="headerlink" title="AttachDetachController"></a>AttachDetachController</h4><p>其中，“第一阶段”的 Attach（以及 Dettach）操作，是由 Volume Controller 负责维护的，这个控制循环的名字叫作：AttachDetachController。而它的作用，就是不断地检查每一个 Pod 对应的 PV，和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作。</p>
<p>需要注意，作为一个 Kubernetes 内置的控制器，Volume Controller 自然是 kube-controller-manager 的一部分。所以，AttachDetachController 也一定是运行在 Master 节点上的。当然，Attach 操作只需要调用公有云或者具体存储项目的 API，并不需要在具体的宿主机上执行操作，所以这个设计没有任何问题。</p>
<h3 id="VolumeManagerReconciler"><a href="#VolumeManagerReconciler" class="headerlink" title="VolumeManagerReconciler"></a>VolumeManagerReconciler</h3><p>而“第二阶段”的 Mount（以及 Unmount）操作，必须发生在 Pod 对应的宿主机上，所以它必须是 kubelet 组件的一部分。这个控制循环的名字，叫作：VolumeManagerReconciler，它运行起来之后，是一个独立于 kubelet 主循环的 Goroutine。通过这样将 Volume 的处理同 kubelet 的主循环解耦，Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环，进而导致 Pod 的创建效率大幅下降的问题。实际上，kubelet 的一个主要设计原则，就是它的主控制循环绝对不可以被 block。这个思想，我在后续的讲述容器运行时的时候还会提到。</p>
<h3 id="PV对象是如何变成容器里的一个持久化存储的呢？"><a href="#PV对象是如何变成容器里的一个持久化存储的呢？" class="headerlink" title="PV对象是如何变成容器里的一个持久化存储的呢？"></a>PV对象是如何变成容器里的一个持久化存储的呢？</h3><p>我在前面讲解容器基础的时候，已经为你详细剖析了容器 Volume 的挂载机制。</p>
<p><strong>用一句话总结，所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。</strong></p>
<p><strong>而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”。</strong>即：这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。</p>
<p><strong>显然，我们前面使用的 hostPath 和 emptyDir 类型的 Volume 并不具备这个特征</strong>：它们既有可能被 kubelet 清理掉，也不能被“迁移”到其他节点上。</p>
<p><strong>所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务</strong>，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。</p>
<p><strong>而 Kubernetes 需要做的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载时使用。而所谓“持久化”，指的是容器在这个目录里写入的文件，都会保存在远程存储中，从而使得这个目录具备了“持久性”。</strong></p>
<p>当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;</span><br></pre></td></tr></table></figure>
<p>这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”。</p>
<h4 id="“第一阶段”（Attach）"><a href="#“第一阶段”（Attach）" class="headerlink" title="“第一阶段”（Attach）"></a>“第一阶段”（Attach）</h4><p>接下来，kubelet 要做的操作就取决于你的 Volume 类型了。</p>
<ul>
<li><p>如果你的 Volume 类型是远程块存储，即提供远程磁盘服务。</p>
<p>比如 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么 kubelet 就需要先调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。<br>这相当于执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> gcloud compute instances attach-disk &lt;虚拟机名字&gt; --disk &lt;远程磁盘名字&gt;</span><br></pre></td></tr></table></figure>
<p>这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach。</p>
</li>
<li><p>而如果你的 Volume 类型是远程文件存储（比如 NFS）的话，kubelet 的处理过程就会更简单一些。因为在这种情况下，kubelet 可以跳过“第一阶段”（Attach）的操作，这是因为一般来说，远程文件存储并没有一个“存储设备”需要挂载在宿主机上。</p>
</li>
</ul>
<h4 id="“第二阶段”（Mount）"><a href="#“第二阶段”（Mount）" class="headerlink" title="“第二阶段”（Mount）"></a>“第二阶段”（Mount）</h4><ul>
<li><p>如果你的 Volume 类型是远程块存储，即提供远程磁盘服务。</p>
<p>Attach 阶段完成后，为了能够使用这个远程磁盘，kubelet 还要进行第二个操作，即：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上。不难理解，这个挂载点，正是我在前面反复提到的 Volume 的宿主机目录。所以，这一步相当于执行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 通过lsblk命令获取磁盘设备ID</span><br><span class="line">$ sudo lsblk</span><br><span class="line"># 格式化成ext4格式</span><br><span class="line">$ sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/&lt;磁盘设备ID&gt;</span><br><span class="line"># 挂载到挂载点</span><br><span class="line">$ sudo mkdir -p /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;</span><br></pre></td></tr></table></figure>
<p>这个<strong>将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。</strong></p>
<p>Mount 阶段完成后，这个 Volume 的宿主机目录就是一个“持久化”的目录了，容器在它里面写入的内容，会保存在 Google Cloud 的远程磁盘中。</p>
</li>
<li><p>而如果你的 Volume 类型是远程文件存储（比如 NFS）的话。</p>
<p>kubelet 会直接从“第二阶段”（Mount）开始准备宿主机上的 Volume 目录。</p>
<p>kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mount -t nfs &lt;NFS服务器地址&gt;:/ /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>而经过了“两阶段处理”，我们就得到了一个“持久化”的 Volume 宿主机目录。所以，接下来，kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -v /var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;:/&lt;容器内的目标目录&gt; 我的镜像 ...</span><br></pre></td></tr></table></figure>
<h3 id="Dynamic-Provisioning"><a href="#Dynamic-Provisioning" class="headerlink" title="Dynamic Provisioning"></a>Dynamic Provisioning</h3><p>前面人工管理 PV 的方式就叫作 Static Provisioning。</p>
<p>Kubernetes 为我们提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。</p>
<h4 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h4><p>具体地说，StorageClass 对象会定义如下两个部分内容：</p>
<p>第一，PV 的属性。比如，存储类型、Volume 的大小等等。</p>
<p>第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。</p>
<p>有了这样两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 StorageClass 声明的存储插件，创建出需要的 PV。</p>
<p>举个例子,假如我们的 Volume 的类型是 GCE 的 Persistent Disk 的话</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: block-service</span><br><span class="line">provisioner: kubernetes.io/gce-pd</span><br><span class="line">parameters:</span><br><span class="line">  type: pd-ssd</span><br></pre></td></tr></table></figure>
<p>这个 StorageClass 的 provisioner 字段的值是：kubernetes.io/gce-pd，这正是 Kubernetes 内置的 GCE PD 存储插件的名字。</p>
<p>在举个例子，使用Rook存储服务的话</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: ceph.rook.io/v1beta1</span><br><span class="line">kind: Pool</span><br><span class="line">metadata:</span><br><span class="line">  name: replicapool</span><br><span class="line">  namespace: rook-ceph</span><br><span class="line">spec:</span><br><span class="line">  replicated:</span><br><span class="line">    size: 3</span><br><span class="line">---</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: block-service</span><br><span class="line">provisioner: ceph.rook.io/block</span><br><span class="line">parameters:</span><br><span class="line">  pool: replicapool</span><br><span class="line">  #The value of &quot;clusterNamespace&quot; MUST be the same as the one in which your rook cluster exist</span><br><span class="line">  clusterNamespace: rook-ceph</span><br></pre></td></tr></table></figure>
<p>有了 StorageClass 的 YAML 文件之后，运维人员就可以在 Kubernetes 里创建这个 StorageClass 了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f sc.yaml</span><br></pre></td></tr></table></figure>
<p><strong>StorageClass 并不是专门为了 Dynamic Provisioning 而设计的。</strong></p>
<p>比如，在本篇一开始的例子里，我在 PV 和 PVC 里都声明了 storageClassName=manual。而我的集群里，实际上并没有一个名叫 manual 的 StorageClass 对象。这完全没有问题，这个时候 Kubernetes 进行的是 Static Provisioning，但在做绑定决策的时候，它依然会考虑 PV 和 PVC 的 StorageClass 定义。</p>
<h4 id="在Pvc中使用"><a href="#在Pvc中使用" class="headerlink" title="在Pvc中使用"></a>在Pvc中使用</h4><p>这时候，作为应用开发者，我们只需要在 PVC 里指定要使用的 StorageClass 名字即可，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: claim1</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: block-service</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 30Gi</span><br></pre></td></tr></table></figure>
<p>这里，你可能会有疑问，我在之前讲解 StatefulSet 存储状态的例子时，好像并没有声明 StorageClass 啊？<strong>实际上，如果你的集群已经开启了名叫 DefaultStorageClass 的 Admission Plugin，它就会为 PVC 和 PV 自动添加一个默认的 StorageClass；否则，PVC 的 storageClassName 的值就是“”，这也意味着它只能够跟 storageClassName 也是“”的 PV 进行绑定。</strong></p>
<h4 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h4><p><img src="/2020/02/21/k8s-command/images/image-20200229195512419.png" alt="image-20200229195512419" style="zoom:50%;"></p>
<p>从图中我们可以看到，在这个体系中：</p>
<ul>
<li><p>PVC 描述的，是 Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。</p>
</li>
<li><p>PV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。</p>
</li>
<li><p>而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。</p>
</li>
</ul>
<p>当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>容器持久化存储涉及的概念比较多，试着总结一下整体流程。</p>
<p>当用户创建了一个 PVC 之后，为你创建出对应的 PV。</p>
<p>PersistentVolumeController帮PVC和PV配对。</p>
<p>用户提交请求创建pod，Kubernetes发现这个pod声明使用了PVC，找到对应的PV，新创建的PV，还只是一个API 对象，需要经过“两阶段处理”变成宿主机上的“持久化 Volume”才真正有用：<br>第一阶段由运行在master上的AttachDetachController负责，为这个PV完成 Attach 操作，为宿主机挂载远程磁盘；<br>第二阶段是运行在每个节点上kubelet组件的内部，把第一步attach的远程磁盘 mount 到宿主机目录。这个控制循环叫VolumeManagerReconciler，运行在独立的Goroutine，不会阻塞kubelet主循环。</p>
<p>完成这两步，PV对应的“持久化 Volume”就准备好了，POD可以正常启动，将“持久化 Volume”挂载在容器内指定的路径。</p>
<h3 id="Local-Persistent-Volume"><a href="#Local-Persistent-Volume" class="headerlink" title="Local Persistent Volume"></a>Local Persistent Volume</h3><h4 id="出现的意义"><a href="#出现的意义" class="headerlink" title="出现的意义"></a>出现的意义</h4><p>在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。也就是说，用户希望 Kubernetes 能够直接使用宿主机上的本地磁盘目录，而不依赖于远程存储服务，来提供“持久化”的容器 Volume。这样做的好处很明显，由于这个 Volume 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多。这个需求对本地物理服务器部署的私有 Kubernetes 集群来说，非常常见。所以，Kubernetes 在 v1.10 之后，就逐渐依靠 PV、PVC 体系实现了这个特性。这个特性的名字叫作：Local Persistent Volume。</p>
<h4 id="LPV，不就等同于-hostPath-加-NodeAffinity-吗？"><a href="#LPV，不就等同于-hostPath-加-NodeAffinity-吗？" class="headerlink" title="LPV，不就等同于 hostPath 加 NodeAffinity 吗？"></a>LPV，不就等同于 hostPath 加 NodeAffinity 吗？</h4><p>比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了。那么，我只需要再给这个 Pod 加上一个 nodeAffinity=nodeA，不就可以使用这个 Volume 了吗？</p>
<p>事实上，<strong>你绝不应该把一个宿主机上的目录当作 PV 使用。</strong>这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。</p>
<p>所以，一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。这个原则，我们可以称为<strong>“一个 PV 一块盘”</strong>。</p>
<h4 id="适用哪些应用"><a href="#适用哪些应用" class="headerlink" title="适用哪些应用"></a>适用哪些应用</h4><p>它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。</p>
<h4 id="实现难点一"><a href="#实现难点一" class="headerlink" title="实现难点一"></a>实现难点一</h4><p>如何把本地磁盘抽象成 PV</p>
<ul>
<li><p>在公有云上，这个操作等同于给虚拟机额外挂载一个磁盘，比如 GCE 的 Local SSD 类型的磁盘就是一个典型例子。</p>
</li>
<li><p>而在我们部署的私有环境中，你有两种办法来完成这个步骤。</p>
<ul>
<li>第一种，当然就是给你的宿主机挂载并格式化一个可用的本地磁盘，这也是最常规的操作；</li>
<li>第二种，对于实验环境，你其实可以在宿主机上挂载几个 RAM Disk（内存盘）来模拟本地磁盘。</li>
</ul>
</li>
</ul>
<p>我们使用第二种模拟</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">#</span> 在node-1上执行</span><br><span class="line"><span class="meta">$</span> mkdir /mnt/disks</span><br><span class="line"><span class="meta">$</span> for vol in vol1 vol2 vol3; do</span><br><span class="line">    mkdir /mnt/disks/$vol</span><br><span class="line">    mount -t tmpfs $vol /mnt/disks/$vol</span><br><span class="line">done</span><br></pre></td></tr></table></figure>
<p>需要注意的是，如果你希望其他节点也能支持 Local Persistent Volume 的话，那就需要为它们也执行上述操作，并且确保这些磁盘的名字（vol1、vol2 等）都不重复。</p>
<h4 id="实现难点二"><a href="#实现难点二" class="headerlink" title="实现难点二"></a>实现难点二</h4><p>调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？</p>
<p>造成这个问题的原因在于，对于常规的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后，再通过“两阶段处理”来“持久化”这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。</p>
<p>可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。</p>
<p>这个原则，我们可以称为<strong>“在调度的时候考虑 Volume 分布”</strong>。在 Kubernetes 的调度器里，有一个叫作 <strong>VolumeBindingChecker</strong> 的过滤条件专门负责这个事情。在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。</p>
<h4 id="模拟Local-PV"><a href="#模拟Local-PV" class="headerlink" title="模拟Local PV"></a>模拟Local PV</h4><h5 id="PV-2"><a href="#PV-2" class="headerlink" title="PV"></a>PV</h5><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">example-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">5</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> <span class="string">Delete</span></span><br><span class="line"><span class="attr">  storageClassName:</span> <span class="string">local-storage</span></span><br><span class="line"><span class="attr">  local:</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/mnt/disks/vol1</span></span><br><span class="line"><span class="attr">  nodeAffinity:</span></span><br><span class="line"><span class="attr">    required:</span></span><br><span class="line"><span class="attr">      nodeSelectorTerms:</span></span><br><span class="line"><span class="attr">      - matchExpressions:</span></span><br><span class="line"><span class="attr">        - key:</span> <span class="string">kubernetes.io/hostname</span></span><br><span class="line"><span class="attr">          operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">          values:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">node-1</span></span><br></pre></td></tr></table></figure>
<p>可以看到，这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。</p>
<p>当然了，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 node-1 上。</p>
<p>所以，在这个 PV 的定义里，需要有一个 nodeAffinity 字段指定 node-1 这个节点的名字。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。这正是 Kubernetes 实现<strong>“在调度的时候就考虑 Volume 分布”</strong>的主要方法。</p>
<p>接下来，我们就可以使用 kubect create 来创建这个 PV，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f local-pv.yaml </span><br><span class="line">persistentvolume/example-pv created</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY  STATUS      CLAIM             STORAGECLASS    REASON    AGE</span><br><span class="line">example-pv   5Gi        RWO            Delete           Available                     local-storage             16s</span><br></pre></td></tr></table></figure>
<p>而正如我在上一篇文章里所建议的那样，使用 PV 和 PVC 的最佳实践，是你要创建一个 StorageClass 来描述这个 PV，如下所示：</p>
<h5 id="StorageClass-1"><a href="#StorageClass-1" class="headerlink" title="StorageClass"></a>StorageClass</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: local-storage</span><br><span class="line">provisioner: kubernetes.io/no-provisioner</span><br><span class="line">volumeBindingMode: WaitForFirstConsumer</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f local-sc.yaml </span><br><span class="line">storageclass.storage.k8s.io/local-storage created</span><br></pre></td></tr></table></figure>
<p>这个 StorageClass 的名字，叫作 local-storage。需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。也就是说，我们前面创建 PV 的操作，是不可以省略的。</p>
<p>与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：<strong>延迟绑定</strong>[^1]。</p>
<h5 id="PVC-2"><a href="#PVC-2" class="headerlink" title="PVC"></a>PVC</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-local-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br><span class="line">  storageClassName: local-storage</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f local-pvc.yaml </span><br><span class="line">persistentvolumeclaim/example-local-claim created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME                  STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">example-local-claim   Pending                                       local-storage   7s</span><br></pre></td></tr></table></figure>
<p>可以看到，尽管这个时候，Kubernetes 里已经存在了一个可以与 PVC 匹配的 PV，但这个 PVC 依然处于 Pending 状态，也就是等待绑定的状态。</p>
<h5 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: example-pv-pod</span><br><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">    - name: example-pv-storage</span><br><span class="line">      persistentVolumeClaim:</span><br><span class="line">       claimName: example-local-claim</span><br><span class="line">  containers:</span><br><span class="line">    - name: example-pv-container</span><br><span class="line">      image: nginx</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: &quot;http-server&quot;</span><br><span class="line">      volumeMounts:</span><br><span class="line">        - mountPath: &quot;/usr/share/nginx/html&quot;</span><br><span class="line">          name: example-pv-storage</span><br></pre></td></tr></table></figure>
<p>而我们一旦使用 kubectl create 创建这个 Pod，就会发现，我们前面定义的 PVC，会立刻变成 Bound 状态，与前面定义的 PV 绑定在了一起，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl create -f local-pod.yaml </span><br><span class="line">pod/example-pv-pod created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME                  STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS    AGE</span><br><span class="line">example-local-claim   Bound     example-pv   5Gi        RWO            local-storage   6h</span><br></pre></td></tr></table></figure>
<p>也就是说，在我们创建的 Pod 进入调度器之后，“绑定”操作才开始进行。这时候，我们可以尝试在这个 Pod 的 Volume 目录里，创建一个测试文件，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ kubectl exec -it example-pv-pod -- /bin/sh</span><br><span class="line"># cd /usr/share/nginx/html</span><br><span class="line"># touch test.txt</span><br></pre></td></tr></table></figure>
<p>然后，登录到 node-1 这台机器上，查看一下它的 /mnt/disks/vol1 目录下的内容，你就可以看到刚刚创建的这个文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># 在node-1上</span><br><span class="line">$ ls /mnt/disks/vol1</span><br><span class="line">test.txt</span><br></pre></td></tr></table></figure>
<p>而如果你重新创建这个 Pod 的话，就会发现，我们之前创建的测试文件，依然被保存在这个持久化 Volume 当中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f local-pod.yaml </span><br><span class="line"></span><br><span class="line">$ kubectl create -f local-pod.yaml </span><br><span class="line"></span><br><span class="line">$ kubectl exec -it example-pv-pod -- /bin/sh</span><br><span class="line"># ls /usr/share/nginx/html</span><br><span class="line"># touch test.txt</span><br></pre></td></tr></table></figure>
<h5 id="为什么需要延迟绑定"><a href="#为什么需要延迟绑定" class="headerlink" title="为什么需要延迟绑定"></a>为什么需要延迟绑定</h5><p>我们知道，当你提交了 PV 和 PVC 的 YAML 文件之后，Kubernetes 就会根据它们俩的属性，以及它们指定的 StorageClass 来进行绑定。只有绑定成功后，Pod 才能通过声明这个 PVC 来使用对应的 PV。可是，如果你使用的是 Local Persistent Volume 的话，就会发现，这个流程根本行不通。比如，现在你有一个 Pod，它声明使用的 PVC 叫作 pvc-1。并且，我们规定，这个 Pod 只能运行在 node-2 上。而在 Kubernetes 集群中，有两个属性（比如：大小、读写权限）相同的 Local 类型的 PV。其中，第一个 PV 的名字叫作 pv-1，它对应的磁盘所在的节点是 node-1。而第二个 PV 的名字叫作 pv-2，它对应的磁盘所在的节点是 node-2。假设现在，Kubernetes 的 Volume 控制循环里，首先检查到了 pvc-1 和 pv-1 的属性是匹配的，于是就将它们俩绑定在一起。然后，你用 kubectl create 创建了这个 Pod。这时候，问题就出现了。调度器看到，这个 Pod 所声明的 pvc-1 已经绑定了 pv-1，而 pv-1 所在的节点是 node-1，根据“调度器必须在调度的时候考虑 Volume 分布”的原则，这个 Pod 自然会被调度到 node-1 上。可是，我们前面已经规定过，这个 Pod 根本不允许运行在 node-1 上。所以。最后的结果就是，这个 Pod 的调度必然会失败。这就是为什么，在使用 Local Persistent Volume 的时候，我们必须想办法推迟这个“绑定”操作。</p>
<h5 id="延迟到什么时候"><a href="#延迟到什么时候" class="headerlink" title="延迟到什么时候"></a>延迟到什么时候</h5><p>答案是：推迟到调度的时候。</p>
<p>所以说，StorageClass 里的 volumeBindingMode=WaitForFirstConsumer 的含义，就是告诉 Kubernetes 里的 Volume 控制循环（“红娘”）：虽然你已经发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但请不要现在就执行绑定操作（即：设置 PVC 的 VolumeName 字段）。</p>
<p>而要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV 所在的节点位置，来统一决定，这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。</p>
<p>这样，在上面的例子里，由于这个 Pod 不允许运行在 pv-1 所在的节点 node-1，所以它的 PVC 最后会跟 pv-2 绑定，并且 Pod 也会被调度到 node-2 上。所以，通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个绑定结果不会影响 Pod 的正常调度。</p>
<h3 id="编写自己的存储插件"><a href="#编写自己的存储插件" class="headerlink" title="编写自己的存储插件"></a>编写自己的存储插件</h3><h4 id="FlexVolume"><a href="#FlexVolume" class="headerlink" title="FlexVolume"></a>FlexVolume</h4><p>flexVolume插件只负责attach和mount，使用简单</p>
<p>所以，如果场景简单，不需要Dynamic Provisioning，则可以使用flexVolume；</p>
<p><img src="/2020/02/21/k8s-command/images/image-20200229233523745.png" alt="image-20200229233523745" style="zoom:50%;"></p>
<p>可以看到，在上述体系下，无论是 FlexVolume，还是 Kubernetes 内置的其他存储插件，它们实际上担任的角色，仅仅是 Volume 管理中的“Attach 阶段”和“Mount 阶段”的具体执行者。而像 Dynamic Provisioning 这样的功能，就不是存储插件的责任，而是 Kubernetes 本身存储管理功能的一部分。</p>
<h4 id="CSI"><a href="#CSI" class="headerlink" title="CSI"></a>CSI</h4><p>而CSI插件包括了一部分原来kubernetes中存储管理的功能，实现、部署起来比较复杂。</p>
<p>如果场景复杂，需要支持Dynamic Provisioning，则用CSI插件。</p>
<p><img src="/2020/02/21/k8s-command/images/image-20200229233458219.png" alt="image-20200229233458219" style="zoom:50%;"></p>

      
    </div>
    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束感谢您的阅读-------------</div>
    
</div>
      </div>
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>赏块糖吃吧</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechat.jpeg" alt="sytao 微信支付">
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpeg" alt="sytao 支付宝">
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/02/08/linux_note/" rel="next" title="note">
                <i class="fa fa-chevron-left"></i> note
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/29/java程序员的能力要求/" rel="prev" title="java程序员的能力要求">
                java程序员的能力要求 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="lv-container" data-id="city" data-uid="MTAyMC80MjMyOS8xODg3Ng"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpeg" alt="sytao">
            
              <p class="site-author-name" itemprop="name">sytao</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/shiyangtao" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-globe"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/ShiYangtao" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-globe"></i>Twitter</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.jianshu.com/u/0d11cd4d7bc1" target="_blank" title="简书">
                      
                        <i class="fa fa-fw fa-globe"></i>简书</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#minikube-搭建"><span class="nav-number">1.</span> <span class="nav-text">minikube 搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#外网访问dashboard"><span class="nav-number">1.1.</span> <span class="nav-text">外网访问dashboard</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建kubeadm"><span class="nav-number">2.</span> <span class="nav-text">搭建kubeadm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#安装-kubeadm-和-Docker"><span class="nav-number">2.1.</span> <span class="nav-text">安装 kubeadm 和 Docker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署-Kubernetes-的-Master-节点"><span class="nav-number">2.2.</span> <span class="nav-text">部署 Kubernetes 的 Master 节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置kubectl-授权信息"><span class="nav-number">2.3.</span> <span class="nav-text">配置kubectl 授权信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署网络插件"><span class="nav-number">2.4.</span> <span class="nav-text">部署网络插件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署-Kubernetes-的-Worker-节点"><span class="nav-number">2.5.</span> <span class="nav-text">部署 Kubernetes 的 Worker 节点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#通过-Taint-Toleration-调整-Master-执行-Pod-的策略"><span class="nav-number">2.6.</span> <span class="nav-text">通过 Taint/Toleration 调整 Master 执行 Pod 的策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署-Dashboard-可视化插件"><span class="nav-number">2.7.</span> <span class="nav-text">部署 Dashboard 可视化插件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署容器存储插件"><span class="nav-number">2.8.</span> <span class="nav-text">部署容器存储插件</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建容器化应用"><span class="nav-number">3.</span> <span class="nav-text">创建容器化应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建"><span class="nav-number">3.1.</span> <span class="nav-text">创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#替换"><span class="nav-number">3.2.</span> <span class="nav-text">替换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#apply"><span class="nav-number">3.3.</span> <span class="nav-text">apply</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#删除"><span class="nav-number">3.4.</span> <span class="nav-text">删除</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pod属性"><span class="nav-number">4.</span> <span class="nav-text">Pod属性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Volume"><span class="nav-number">4.1.</span> <span class="nav-text">Volume</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#emptyDir"><span class="nav-number">4.1.1.</span> <span class="nav-text">emptyDir</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#hostPath"><span class="nav-number">4.1.2.</span> <span class="nav-text">hostPath</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Projected-Volume"><span class="nav-number">4.1.3.</span> <span class="nav-text">Projected Volume</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Secret"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">Secret</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ConfigMap"><span class="nav-number">4.1.3.2.</span> <span class="nav-text">ConfigMap</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Downward-API"><span class="nav-number">4.1.3.3.</span> <span class="nav-text">Downward API</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Service-Account"><span class="nav-number">4.1.3.4.</span> <span class="nav-text">Service Account</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NodeSelector"><span class="nav-number">4.2.</span> <span class="nav-text">NodeSelector</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#NodeName"><span class="nav-number">4.3.</span> <span class="nav-text">NodeName</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HostAliases"><span class="nav-number">4.4.</span> <span class="nav-text">HostAliases</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#shareProcessNamespace-true"><span class="nav-number">4.5.</span> <span class="nav-text">shareProcessNamespace=true</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#共享宿主机的Linux-namespace"><span class="nav-number">4.6.</span> <span class="nav-text">共享宿主机的Linux namespace</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Probe"><span class="nav-number">4.7.</span> <span class="nav-text">Probe</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#readinessProbe"><span class="nav-number">4.7.1.</span> <span class="nav-text">readinessProbe</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#restartPolicy"><span class="nav-number">4.8.</span> <span class="nav-text">restartPolicy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PodPreset"><span class="nav-number">4.9.</span> <span class="nav-text">PodPreset</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Container-属性"><span class="nav-number">5.</span> <span class="nav-text">Container 属性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ImagePullPolicy"><span class="nav-number">5.1.</span> <span class="nav-text">ImagePullPolicy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lifecycle"><span class="nav-number">5.2.</span> <span class="nav-text">Lifecycle</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#作业副本与水平扩展"><span class="nav-number">6.</span> <span class="nav-text">作业副本与水平扩展</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ReplicaSet-结构"><span class="nav-number">6.1.</span> <span class="nav-text">ReplicaSet 结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deployment结构"><span class="nav-number">6.2.</span> <span class="nav-text">Deployment结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deployment和ReplicaSet关系"><span class="nav-number">6.3.</span> <span class="nav-text">Deployment和ReplicaSet关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#部署deployment"><span class="nav-number">6.4.</span> <span class="nav-text">部署deployment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看replicaSet状态"><span class="nav-number">6.5.</span> <span class="nav-text">查看replicaSet状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#edit-deployment"><span class="nav-number">6.6.</span> <span class="nav-text">edit deployment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scale"><span class="nav-number">6.7.</span> <span class="nav-text">scale</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RollingUpdateStrategy"><span class="nav-number">6.8.</span> <span class="nav-text">RollingUpdateStrategy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回滚到上一个版本"><span class="nav-number">6.9.</span> <span class="nav-text">回滚到上一个版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回滚到指定版本"><span class="nav-number">6.10.</span> <span class="nav-text">回滚到指定版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多次修改deployment只生成一个rs"><span class="nav-number">6.11.</span> <span class="nav-text">多次修改deployment只生成一个rs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#控制这些“历史”ReplicaSet-的数量呢？"><span class="nav-number">6.12.</span> <span class="nav-text">控制这些“历史”ReplicaSet 的数量呢？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#StatefulSet"><span class="nav-number">7.</span> <span class="nav-text">StatefulSet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#拓扑状态"><span class="nav-number">7.1.</span> <span class="nav-text">拓扑状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#存储状态"><span class="nav-number">7.2.</span> <span class="nav-text">存储状态</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PVC"><span class="nav-number">7.2.1.</span> <span class="nav-text">PVC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在Pod中声明这个PVC"><span class="nav-number">7.2.2.</span> <span class="nav-text">在Pod中声明这个PVC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PV"><span class="nav-number">7.2.3.</span> <span class="nav-text">PV</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#StatefulSet-集成PVC"><span class="nav-number">7.2.4.</span> <span class="nav-text">StatefulSet 集成PVC</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#daemonSet"><span class="nav-number">8.</span> <span class="nav-text">daemonSet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#滚动更新"><span class="nav-number">8.1.</span> <span class="nav-text">滚动更新</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#灰度发布"><span class="nav-number">8.2.</span> <span class="nav-text">灰度发布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#deamonSet结构"><span class="nav-number">8.3.</span> <span class="nav-text">deamonSet结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nodeAffinity"><span class="nav-number">8.4.</span> <span class="nav-text">nodeAffinity</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Toleration"><span class="nav-number">8.5.</span> <span class="nav-text">Toleration</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建deamonSet"><span class="nav-number">8.6.</span> <span class="nav-text">创建deamonSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回滚daemonSet"><span class="nav-number">8.7.</span> <span class="nav-text">回滚daemonSet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ControllerRevision对象"><span class="nav-number">8.8.</span> <span class="nav-text">ControllerRevision对象</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Job"><span class="nav-number">9.</span> <span class="nav-text">Job</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CronJob"><span class="nav-number">10.</span> <span class="nav-text">CronJob</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#格式"><span class="nav-number">10.1.</span> <span class="nav-text">格式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spec-concurrencyPolicy"><span class="nav-number">10.2.</span> <span class="nav-text">spec.concurrencyPolicy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spec-startingDeadlineSeconds"><span class="nav-number">10.3.</span> <span class="nav-text">spec.startingDeadlineSeconds</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Initializer（Dynamic-Admission-Control）"><span class="nav-number">11.</span> <span class="nav-text">Initializer（Dynamic Admission Control）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#istio架构图"><span class="nav-number">11.1.</span> <span class="nav-text">istio架构图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-Admission-Control"><span class="nav-number">11.2.</span> <span class="nav-text">Dynamic Admission Control</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原理"><span class="nav-number">11.3.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#InitializerConfiguration"><span class="nav-number">11.4.</span> <span class="nav-text">InitializerConfiguration</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#API对象"><span class="nav-number">12.</span> <span class="nav-text">API对象</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#API对象结构"><span class="nav-number">12.1.</span> <span class="nav-text">API对象结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#构建过程"><span class="nav-number">12.2.</span> <span class="nav-text">构建过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自定义API资源"><span class="nav-number">12.3.</span> <span class="nav-text">自定义API资源</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CR（Custom-Resource）"><span class="nav-number">12.3.1.</span> <span class="nav-text">CR（Custom Resource）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CRD（Custom-Resource-Definition）"><span class="nav-number">12.3.2.</span> <span class="nav-text">CRD（Custom Resource Definition）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#编写代码"><span class="nav-number">12.3.3.</span> <span class="nav-text">编写代码</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#代码结构"><span class="nav-number">12.3.3.1.</span> <span class="nav-text">代码结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pkg-apis-samplecrd-register-go"><span class="nav-number">12.3.3.2.</span> <span class="nav-text">pkg/apis/samplecrd/register.go</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pkg-apis-samplecrd-v1-doc-go"><span class="nav-number">12.3.3.3.</span> <span class="nav-text">pkg/apis/samplecrd/v1/doc.go</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pkg-apis-samplecrd-v1-types-go"><span class="nav-number">12.3.3.4.</span> <span class="nav-text">pkg/apis/samplecrd/v1/types.go</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pkg-apis-samplecrd-v1-register-go"><span class="nav-number">12.3.3.5.</span> <span class="nav-text">pkg/apis/samplecrd/v1/register.go</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#自动生成代码"><span class="nav-number">12.3.3.6.</span> <span class="nav-text">自动生成代码</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#自定义控制器"><span class="nav-number">12.4.</span> <span class="nav-text">自定义控制器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#编写main函数"><span class="nav-number">12.4.1.</span> <span class="nav-text">编写main函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#编写自定义控制器的定义"><span class="nav-number">12.4.2.</span> <span class="nav-text">编写自定义控制器的定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#编写控制器里的业务逻辑"><span class="nav-number">12.4.3.</span> <span class="nav-text">编写控制器里的业务逻辑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#执行控制器"><span class="nav-number">12.4.4.</span> <span class="nav-text">执行控制器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RBAC"><span class="nav-number">13.</span> <span class="nav-text">RBAC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Role"><span class="nav-number">13.1.</span> <span class="nav-text">Role</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RoleBinding"><span class="nav-number">13.2.</span> <span class="nav-text">RoleBinding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ClusterRole"><span class="nav-number">13.3.</span> <span class="nav-text">ClusterRole</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ClusterRoleBinding"><span class="nav-number">13.4.</span> <span class="nav-text">ClusterRoleBinding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#User"><span class="nav-number">13.5.</span> <span class="nav-text">User</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#defaultServiceAccount"><span class="nav-number">13.5.1.</span> <span class="nav-text">defaultServiceAccount</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Group"><span class="nav-number">13.6.</span> <span class="nav-text">Group</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#所有Namespace-下的默认-ServiceAccount，绑定一个只读权限的-Role。"><span class="nav-number">13.6.1.</span> <span class="nav-text">所有Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Operator"><span class="nav-number">14.</span> <span class="nav-text">Operator</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#构建步骤"><span class="nav-number">14.1.</span> <span class="nav-text">构建步骤</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#第一步，将这个-Operator-的代码-Clone-到本地："><span class="nav-number">14.1.1.</span> <span class="nav-text">第一步，将这个 Operator 的代码 Clone 到本地：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第二步，将这个-Etcd-Operator-部署在-Kubernetes-集群里。"><span class="nav-number">14.1.2.</span> <span class="nav-text">第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第三步，创建Etcd-Operator"><span class="nav-number">14.1.3.</span> <span class="nav-text">第三步，创建Etcd Operator</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#第四步，创建Etcd-集群"><span class="nav-number">14.1.4.</span> <span class="nav-text">第四步，创建Etcd 集群</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Etcd-Operator原理分析"><span class="nav-number">14.2.</span> <span class="nav-text">Etcd Operator原理分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Etcd静态集群构建"><span class="nav-number">14.2.1.</span> <span class="nav-text">Etcd静态集群构建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#EtcdCluster-CRD"><span class="nav-number">14.2.2.</span> <span class="nav-text">EtcdCluster CRD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Etcd-Operator-scale原理"><span class="nav-number">14.2.3.</span> <span class="nav-text">Etcd Operator scale原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Etcd-Operator-Informer"><span class="nav-number">14.2.4.</span> <span class="nav-text">Etcd Operator Informer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动Bootstrap"><span class="nav-number">14.2.5.</span> <span class="nav-text">启动Bootstrap</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动该集群所对应的控制循环"><span class="nav-number">14.2.6.</span> <span class="nav-text">启动该集群所对应的控制循环</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对比StatefulSet-可能有的两个疑问"><span class="nav-number">14.3.</span> <span class="nav-text">对比StatefulSet  可能有的两个疑问</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#问题1"><span class="nav-number">14.3.1.</span> <span class="nav-text">问题1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#问题2"><span class="nav-number">14.3.2.</span> <span class="nav-text">问题2</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#再谈PVC、PV、StorageClass"><span class="nav-number">15.</span> <span class="nav-text">再谈PVC、PV、StorageClass</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#PV-1"><span class="nav-number">15.1.</span> <span class="nav-text">PV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PVC-1"><span class="nav-number">15.2.</span> <span class="nav-text">PVC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#在Pod中使用PVC"><span class="nav-number">15.3.</span> <span class="nav-text">在Pod中使用PVC</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Volume-Controller"><span class="nav-number">15.4.</span> <span class="nav-text">Volume Controller</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#PersistentVolumeController"><span class="nav-number">15.4.1.</span> <span class="nav-text">PersistentVolumeController</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AttachDetachController"><span class="nav-number">15.4.2.</span> <span class="nav-text">AttachDetachController</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VolumeManagerReconciler"><span class="nav-number">15.5.</span> <span class="nav-text">VolumeManagerReconciler</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PV对象是如何变成容器里的一个持久化存储的呢？"><span class="nav-number">15.6.</span> <span class="nav-text">PV对象是如何变成容器里的一个持久化存储的呢？</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#“第一阶段”（Attach）"><span class="nav-number">15.6.1.</span> <span class="nav-text">“第一阶段”（Attach）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#“第二阶段”（Mount）"><span class="nav-number">15.6.2.</span> <span class="nav-text">“第二阶段”（Mount）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-Provisioning"><span class="nav-number">15.7.</span> <span class="nav-text">Dynamic Provisioning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#StorageClass"><span class="nav-number">15.7.1.</span> <span class="nav-text">StorageClass</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#在Pvc中使用"><span class="nav-number">15.7.2.</span> <span class="nav-text">在Pvc中使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#原理-1"><span class="nav-number">15.7.3.</span> <span class="nav-text">原理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">15.8.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Local-Persistent-Volume"><span class="nav-number">15.9.</span> <span class="nav-text">Local Persistent Volume</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#出现的意义"><span class="nav-number">15.9.1.</span> <span class="nav-text">出现的意义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LPV，不就等同于-hostPath-加-NodeAffinity-吗？"><span class="nav-number">15.9.2.</span> <span class="nav-text">LPV，不就等同于 hostPath 加 NodeAffinity 吗？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#适用哪些应用"><span class="nav-number">15.9.3.</span> <span class="nav-text">适用哪些应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现难点一"><span class="nav-number">15.9.4.</span> <span class="nav-text">实现难点一</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#实现难点二"><span class="nav-number">15.9.5.</span> <span class="nav-text">实现难点二</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模拟Local-PV"><span class="nav-number">15.9.6.</span> <span class="nav-text">模拟Local PV</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#PV-2"><span class="nav-number">15.9.6.1.</span> <span class="nav-text">PV</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#StorageClass-1"><span class="nav-number">15.9.6.2.</span> <span class="nav-text">StorageClass</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PVC-2"><span class="nav-number">15.9.6.3.</span> <span class="nav-text">PVC</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Pod"><span class="nav-number">15.9.6.4.</span> <span class="nav-text">Pod</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#为什么需要延迟绑定"><span class="nav-number">15.9.6.5.</span> <span class="nav-text">为什么需要延迟绑定</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#延迟到什么时候"><span class="nav-number">15.9.6.6.</span> <span class="nav-text">延迟到什么时候</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编写自己的存储插件"><span class="nav-number">15.10.</span> <span class="nav-text">编写自己的存储插件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FlexVolume"><span class="nav-number">15.10.1.</span> <span class="nav-text">FlexVolume</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CSI"><span class="nav-number">15.10.2.</span> <span class="nav-text">CSI</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sytao</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">75.1k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
